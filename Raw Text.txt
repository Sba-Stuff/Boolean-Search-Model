 Improving Requirements Tracing via Information Retrieval
Jane Huffman Hayes 
Computer Science Department Laboratory for Advanced NetworkingUniversity of Kentucky hayes@cs.uky.edu(corresponding author)
Alex Dekhtyar 
Computer Science DepartmentUniversity of Kentucky dekhtyar@cs.uky.eduJames Osborne 
Computer Science DepartmentUniversity of Kentucky jas@netlab.uky.eduAbstract
This paper presents an approach for improving requirements 
tracing based on framing it as an information retrieval (IR) 

problem.  Specifically, we focus on improving recall and 
precision in order to reduce the number of missed traceability 
links as well as to reduce the number of irrelevant potential 
links that an analyst has to examine when performing 
requirements traci
ng.  S
everal IR 
algorithms were adapted and 
implemented to address this problem.  We evaluated our 
algorithms by comparing their results and performance to those 
of a senior analyst who traced manually as well as with an 

existing requirements tracing tool.  Initial results suggest that 
we can retrieve a significantly higher percentage of the links 
than analysts, even when using existing tools, and do so in much 

less time while achieving com
parable signal-to-noise levels. 
Research 
1. Introduction    There are two primary motivators for performing 
requirements tracing:  ensuring that a new system does 
indeed satisfy all its specified requirements, and 

performing impact analysis on proposed changes.  Both of 
these can be facilitated if a developer builds a detailed 

requirements trace as development proceeds.  The 
adoption of CASE tools such as DOORS [25], RDD-100 

[13], and Rational RequisitePro [21], or initiating process 
improvement initiatives, such as ISO9000 [14], 
Capability Maturity Model [6], or Personal Software 
Process (PSP)/Team Software Process (TSP) [24], can 

spark such discipline in organizations that were formerly 

remiss.  Though our auditing and verification and 
validation experience has shown the old adage you can 

lead a horse to water but you cant make it drink to be 
true in many cases. 
   Requirement tracing is at best a mundane, mind 
numbing activity, as anyone who has spent any time 
performing this activity will tell you.  Even with 
automation support, it is still a time consuming, error 

prone, person-power intensive task.  It has been our 

experience that the tools that do exist to support this 

activity have numerous shortcomings:  they require the 

user to perform interactive searches for potential linking 

requirements or design elements, they require the user to 

assign keywords to all the elements in both document 

levels prior to tracing, they return many potential or 

candidate links that are not correct, they fail to return 

correct links, and they do not provide support for easily 

retracing new versions of documents. 

   Since there are still many times when a requirements 

traceability matrix (RTM) does not exist and there is a 

need to ensure requirement completion and to understand 

change impact, a method for easy after-the-fact 

requirements tracing is needed.  Requirements traces can 

be evaluated by calculating two metrics:  the percentage of 

actual matches that are found (
recall
) and the percentage 
of correct matches as a ratio to the total number of 

candidate links returned (
precision
).  As mentioned above, 
current methods are prone to error and require intense 

effort on the part of the analyst.  Recall, precision, and 

performance values for these methods are not widely 

known or generalized.  This paper presents the results of 

NASA-funded research to improve the state of the art of 

after the fact requirements tracing.  The problem was cast 

in as an Information Retrieval problem, three algorithms 

were developed, an analysis tool was developed, and an 

evaluation study was performed.  Our retrieval with 

thesaurus algorithm provided recall of 85.3% and 

precision of 40.6% in a much shorter period of time than 

analysts performing the same task. 

   Section 2 discusses related work in requirements tracing.  

IR Background on Information Retrieval (IR) is presented 

in Section 3.  We discuss how requirements tracing can be 

represented as an IR problem and the algorithms we used 
Proceedings of the 11th IEEE International Requirements Engineering Conference 
1090-705X/03 $17.00  2003 
IEEE 
Authorized licensed use limited to: QUAID E AZAM UNIVERSITY. Downloaded on November 30,2020 at 08:55:58 UTC from IEEE Xplore.  Restrictions apply.  . 
in our study in Section 4. 
 Section 5 discusses the results 
obtained from evaluation.  Finally, Section 6 presents 
conclusions and areas for future work.
2. Related work 
   In the context of our work, there are two areas of 
interest:  requirements tracing and IR as it has been 

applied to the problem of requirements analysis.  Each 

will be addressed below. 
2.1 Requirements tracing
   We have been tackling the requirements tracing problem 
for many decades.  In 1978, Pierce [18] designed a 

requirements tracing tool, basically a way to build and 

maintain a requirements database, to facilitate 

requirements analysis and system verification and 

validation for a large Navy undersea acoustic sensor 
system.  

   Hayes et al [12] built a front end for a requirements 
tracing tool called the Software Automated Verification 

and Validation and Analysis System (SAVVAS) Front 
End processor (SFEP).  This was written in Pascal and 

interfaced with the SAVVAS requirements tracing tool 
that was based on an Ingres relational database.  SFEP 

allows the extraction of requirement text as well as the 
assignment of requirement keywords through the use of 

specified linkwords such as 
shall
,must
,will, etc.  These 
tools are largely based on keyword matching and 

threshold setting for that matching.  Several years later the 

tools were ported to hypercard technology on Macs, and 

then to Microsoft Access and Visual Basic running on 

PCs.  This work is described by Mundie and Hallsworth in 
[17].  These tools have since been further enhanced and 

are still in use as part of the Independent Verification and 

Validation (IV&V) efforts for the Mission Planning 
system of the Tomahawk Cruise Missile as well as for 
several NASA Code S science projects. 

   Abrahams and Barkley, Ramesh, and Watkins and Neal 
[1, 19, 27] discuss the importance of requirements tracing 

from a developers perspective and explain basic concepts 

such as forward tracing, backward tracing, vertical tracing, 

and horizontal tracing.  Casotto [7] examined run-time 

tracing of the design actvity.  Her approach uses 

requirement cards organized into linear hierarchical stacks 

and supports retracing.  Tsumaki and Morisawa [26] 

discuss requirements tracing using UML.  Specifically 

they look at tracing artifacts such as use-cases, class 

diagrams, and sequence diagrams from the business model 

to the analysis model and to the design model (and back) 

[26].   There have also been significant advances in the area of 

requirements elicitation, analysis, and tracing.  Work has 
largely been based on lexical analysis, such as extraction 
and analysis of phoneme occurrences to categorize and 

analyze requirements and other artifacts [22].  Bohners 

work on software change impact analysis using a graphing 

technique may be useful in performing tracing of changed 

requirements [4].  Anezin and Brouse present advances in 

backward tracing and multimedia requirements tracing in  

[2, 5].Cleland-Huang et al [8] propose an event-based 
traceability technique for supporting impact analysis of 
performance requirements.  Data is propagated 
speculatively into performance models that are then re-

executed to determine impacts from the proposed change.  
Ramesh et al examine reference models for traceability.  
They establish two specific models, a low-end model of 

traceability and a high-end model of traceability for more 

sophisticated users [20].  They found that a typical low 

end user created traceability links to model requirement 

dependencies, to examine how requirements had been 

allocated to system components, to verify that 

requirements had been satisfied, and to assist with change 

control.  A typical high-end user, on the other hand, uses 

traceability for full coverage of the life cycle, includes the 

user and the customer in this process, captures discussion 

issues, decision, and rationale, and captures traces across 

product and process dimensions [20].
2.2 Information retrieval in requirements 
analysisDag et al [9] perform automated similarity analysis of 
textual requirements using IR techniques.  They found this 
to be a promising method that helped identify 
relationships between requirements.  Our work differs 

from theirs in that we examine requirements tracing after 

the fact while they focus on assisting developers who 

must deal with a rapid arrival of new requirements from 

numerous diverse sources. They propose to continuously 

analyze the flow of incoming requirements to increase the 
efficiency of the requirements engineering process [9].   
   Antionol et al [3] applied a probabilistic and a vector 

space IR model in two case studies to trace C++ source 

code onto manual pages and to trace Java code to 
functional requirements.  They examined the effect of 
requiring 100% recall and found that the probabilistic 

model achieves the highest recall values, less than 100 

percent, with a smaller number of documents retrieved 
and then performs better when 100% recall is required. 
3. Information retrieval (IR) 
   IR is a field that studies the problem of finding relevant 

documents in document collections given user queries. 
Proceedings of the 11th IEEE International Requirements Engineering Conference 
1090-705X/03 $17.00  2003 
IEEE 
Authorized licensed use limited to: QUAID E AZAM UNIVERSITY. Downloaded on November 30,2020 at 08:55:58 UTC from IEEE Xplore.  Restrictions apply.  . 
While IR research first appeared in the 1960s, it became a 
separate discipline of Computer Science in the late 1970s. 
The advent of the World Wide Web and the growth of 
data storage capacity of computers, which lead to the 

growth in the number and size of document repositories, 

generated a new wave of IR research in the 1990s. A 
recent introduction to IR is found in  [10]; [11] provides 

an excellent roadmap for developing IR systems from 
scratch. We refer the reader interested in the history and 
development of IR to Sparck Jones, and Willet [23], a 

large collection of seminal and influential papers from the 
field.  

   IR attempts to model individual documents within 

document collections and to model user information 

needs. IR methods determine how 
relevant
 the document 
representation is to the query that represented user 
information need.  
   Among a large variety of methods of IR, keyword-based 

retrieval is arguably the most-studied and often-used 
method. In keyword-based IR, each document in the 
repository is analyzed to determine the (key)words or 

terms that are important for this document and can be used 

to query it. User queries are also analyzed for keywords 

and these keywords are compared with the ones associated 

with each document in the collection in order to determine 

matches. In most keyword-based methods, the 
relevance
of the document to the query is expressed using a 
similarity measure
 that computes how closely the 
representations of a document and a query match. The 

answer to the query is given in the form of a list of 

documents in descending order of their expected 

relevance to the query. 

   The quality of IR methods is measured by how well the 

documents returned match the users expectations. This is 

typically formalized with two metrics: 
 precision
 and 
recall
.Precision
 is computed as the fraction of the 
relevant documents in the list of all documents returned by 

the IR method given a query. 
Recall is the fraction of the 
retrieved relevant documents in the entire set of 
documents, retrieved and omitted, that are relevant to the 

query. We notice here that precision is a quantity that 

usually is relatively easy to measure given the query and 
the list of answers. Measuring recall is a much harder task, 

as it requires knowledge of the entire document collection. 

IR methods are usually designed to work on large 

collections of data: a typical test collection for an IR 
system is around 5-7 GB, whereas industrial-strength 

applications, such as WWW search engines are expected 

to handle data collections that are at least 2-3 orders of 
magnitude greater, 
and
 provide answers within seconds. 
Because of this, performance of the IR methods also plays 
a major role in their evaluation, as sometimes methods 
that give higher precision and recall become impractical 

due to the time it takes for them to deliver the answer.   
   There is a wide array of keyword-based retrieval models 
for document collections. Boolean model is the simplest: a 

representation of a document is a Boolean vector 

identifying the keywords present in the document. 
Vector 
model extends Boolean model by associating with each 
term in the document representation a 
weight
 that signifies 
its assumed importance to the document collection. 

Consider a standard vector retrieval model. Given a 
document 
din the collection, let us denote its 
representation in the vector model as a vector 
d=(w
1,w2,...,wN), 
where 
N is the number of terms in the document 
collections vocabulary and 
wiis the abovementioned 
weight of the 
ith term. This weight is computed as  
,)(iiiidfdtfw=where
 tf
i(d) 
is the 
term frequency
 of the 
ith keyword in 
document 
d and 
idf
i is the 
inverse document frequency of 
the ith term in the document collection. 
Term frequency is 
usually the number of occurrences of the term in the 
document and is usually normalized. Inverse document 
frequency is computed as 
=iidfnidf2log, where 
dfiis the total number of documents containing the 
ith term in 
the document collection and 
n is the size of the document 
collection. Basically, the importance of the term is judged 

by how often this term is found in the document and by 
how discriminating the term is. That is., the less frequent 

the term is in the collection, the more its presence is 
important for the document. A user query is also 
converted into a similar vector 
q=(q
1,,qN) of term 
weights. In this model, given a document vector 
dand a 
query vector 
q, the similarity between them is computed 
as the 
cosine of the angle between vectors d and q in the 
N-dimensional space:
.),cos(),(11221=====NiNiiiNiiiqwqwqdqdsim   Different extensions of the standard vector retrieval 
model exist, based on modifications to the computation of 

term weights in the document and similarity between the 

document and query vectors. There are also extensions of 

the vector model based on the use of additional 
information:
Retrieval based on user feedback.
  After the original list 
of the answers to the user query is compiled, the user is 

asked to specify which of the returned documents were 

relevant and which were not. Using this information it is 

possible to re-weigh the query vector and adjust the 
Proceedings of the 11th IEEE International Requirements Engineering Conference 
1090-705X/03 $17.00  2003 
IEEE 
Authorized licensed use limited to: QUAID E AZAM UNIVERSITY. Downloaded on November 30,2020 at 08:55:58 UTC from IEEE Xplore.  Restrictions apply.  . 
similarity computation in a way that documents similar to 
the ones the user declared relevant will get a higher 
relevance rating, while the documents similar to the ones 
declared irrelevant will drop significantly in their 

relevance rating.
Thesaurusbased retrieval.
  Classical vector model 
compares only the occurrences of individual keywords 

and key phrases. However, in many situations, one needs 

to take into account the presence in the document of the 
keywords synonymous or otherwise related to the query 

keywords. For example, the query car retailer will not 
match the document describing Toyota dealership in 

classical vector retrieval, but it may be very relevant to the 

query. 
Thesauri
 are collections of information about the 
relationship between different terms. Use of thesauri in IR 

allows one to extend classical vector retrieval to account 
for the presence of synonyms, words representing 
subcategories of the query terms, etc. Thesauri can come 

in a variety of different flavors: from very detailed 
descriptions of term hierarchies to ad-hoc lists of synonym 
pairs. The exact way of incorporating the thesaurus into 

the IR method depends of its type. 
3. Automating requirements tracing 
   Among the tasks that must be performed during the 

requirements tracing, the most time-consuming and crucial 

activity is the 
generation of candidate links
. Even with the 
aid of currently available support tools, this is still largely 

an analyst-driven process. Whether performing forward-, 

lateral- or back-tracing, the majority of the time analysts 

spend is devoted to 
the generation of sensible lists of 
candidate matches. 
. It is this portion of the requirements 
tracing process that we are automating.  

   In addressing the problem of automating the 

requirements tracing process, our main objectives were to 

improve the quality of the candidate lists as well as 

decrease the time needed for their generation. We notice 

that IR metrics of recall and precision are appropriate 

characteristics of the quality of candidate lists: recall 

measures the fraction of true matches that had been 

included, while precision measures the signal-to-noise 

ratio. Between these two metrics, we chose recall as our 

most important objective.  This ordering ensures that an 

analyst feels confident that all related links have, indeed, 

been retrieved.  Precision comes next, so that the analyst 

would not have to sift through numerous erroneous 

potential links. We notice however, that without good 

precision, total recall is a meaningless accomplishment. 

For example, in forward tracing, it can be achieved by 

simply including every single lower-level requirement in 

the candidate lists for every higher-level requirement. 
   As is apparent, automatic generation of candidate lists is 
bound to be orders of magnitude faster than their manual 
generation by the analyst, even assisted by currently 

available interactive tools. Automated generation 

drastically reduces the burden on the analyst of two time-

consuming and frustrating activities: keyword assignment 

and interactive search for candidate links. 

   There were six major activities as we performed this 

work: (i) framing the problem in terms of an IR problem, 

(ii) selecting IR algorithms to implement, (iii) preparing 

the input requirement text, (iv) analyzing the output from 
the algorithms, (v) selecting strategies for trimming 
algorithm output, and (vi) comparing algorithm 

performance to human analyst performance.  Each activity 

will be discussed in subsections below except for the 

algorithm evaluation activities that will be deferred until 

Section 5. 
3.1 Requirements tracing as an IR problem 
   We illustrate how requirements tracing can be 
represented as an IR problem using forward tracing as an 

example. Note that the same technique can be applied to 
tracing design descriptions backward to requirement 

specifications, to tracing requirement specifications 
laterally to test specifications, to tracing design elements 

forward to code elements, etc.  The collection of high-

level requirements can be extracted from the high-level 
requirements document. Similarly, the lower-level 
document can be broken into the collection of individual 

lower-level requirements (also called design elements 

here). Each requirement and each design element can be 

treated as a separate document in an IR document 

repository. Generally it should contain all text and 
supplemental information (e.g., tables, graphs if such 

exist) necessary for the requirement/design element to be 
readable and understandable on its own. 

   Now, given the list 
R = (r
1,,rN)ofrequirements and 
the list 
S=(s
1,,sM) of lower-level design elements, a 
requirements trace is a mapping 
tr:R
2S, where every 
design element
str(r)
 satisfies part  or all of requirement 
rand no other design elements  satisfy any parts of 
r.  
   The approach we adopt for this research is to consider 
requirements tracing as an IR problem. Because most 

manual or semi-automatic technologies for requirements 

tracing are keyword-based, keyword-based IR methods 
appear to be a natural extension of this process.  In 

particular, we formalize the requirements tracing problem 
as follows. The universe of documents 
D = RSis the 
union of all individual requirements and design elements. 

Let 
VD = {k
1,k2,,kL} be the vocabulary of 
D,i.e., the list 
of all terms that appear in both higher-level and lower-
level requirements. Each document 
diDis represented as 
the vector of term weights 
di=(w
i,1
,...,wi,L
). Assume the 
Proceedings of the 11th IEEE International Requirements Engineering Conference 
1090-705X/03 $17.00  2003 
IEEE 
Authorized licensed use limited to: QUAID E AZAM UNIVERSITY. Downloaded on November 30,2020 at 08:55:58 UTC from IEEE Xplore.  Restrictions apply.  . 
existence of a similarity measure, 
sim, that, given two 
vectors 
di and 
dj,, quantifies the similarity between them. 
   The process of building candidate link lists for the 

requirements tracing problem is then reduced to the 

procedure of computing the matrix of similarities between 

vectors 
r1,,rN,representations of high-level 
requirements, and vectors 
si,,sM ,representations of 
lower-level requirements. For each high-level requirement 
rithe list 
s1,,s
Miof design elements, such that 
sim(ri,s-
j) ,sorted in the order of descending similarity value, 
serves as the first approximation of the candidate list. This 

list can further be pruned in a variety of different ways: for 

example, by considering only the top five vectors on the 



pruning out all specifications that exhibit smaller 
similarity. 
3.2 Features of the requirements tracing process 
   While IR methods seem to provide a good match for the 
problem of automating the generation of the lists of 

candidate matches, the requirements tracing problem has a 

number of specific features that typical IR settings do not 

have. We briefly address these features here and discuss 

their implications on our attempts to apply IR techniques 

to requirements tracing.
1.Size of the domain.
 IR methods are designed for 
working with large numbers of large documents in the 

presence of large vocabularies. In requirements 

tracing, our domain is a fairly small collection of 

documents: there are on the order of thousands of 

requirements in a large-scale software development 

project, whereas, typical document collections 

number hundreds of thousands to millions of 

documents. An individual requirement is also quite 

short: it often contains just 2-3 sentences. Finally, the 

limited document collection that the requirements 

form has a relatively limited vocabulary. 
Implications of domain size. (i) 
Traditional IR 
methods become robust on large collections of data. 

Their performance on smaller collections can be not 

as good because the influence of individual 

components of the model on the final result grows, 

and, sometimes, coincidental matches outscore the 

true similarities. Therefore, we must be careful in 

evaluation of our IR methods. 
(ii)
 On the other hand, 
because of small domain sizes, we can apply some of 
the more complex IR techniques that are typically 
deemed to be too slow for large data collections.  

This is part of our future work. 
2.Query interdependence.
 It is customary in IR systems 
to consider all queries as being independent. It is a 
reasonable assumption for Internet search engines 
which process thousands of queries each second 

coming from thousands of different users. In the 

requirements tracing problem, though, the queries are 

the higher-level requirements, which are, very often, 

related to each other. 
Implications of query interdependence.
 The result of 
our automated process is the matrix of similarities 

between higher-level and lower-level requirements. 
Knowing that the rows of this matrix may be not 
independent, we can perform secondary analysis on 

this matrix comparing the candidate lists for different 

requirements and the similarity measures.  
3.3 Selection and modification of IR algorithms 
In our initial study we have explored three different IR 

methods: classical vector IR model and two extensions of 
it with simple thesauri constructs.  All algorithms followed 

the same path from data preparation to generation of 
answers. First, individual requirements were extracted 

from higher- and lower-level requirements documents 
using automatic extraction scripts, similar to those found 

in SuperTracePlus and commercial tools. Each 
requirement/design element was stored as a separate file. 

The repository thus generated was used as the input for 
the model-building tool.  
   On the model-building stage the following is done: (i) 

each requirement is parsed and tokenized; (ii) stopwords 

(i.e., words that are not useful for the purposes of 
retrieval, like shall, the or for) are detected and 
removed from the token stream; (iii) the remaining tokens 

are stemmed to ensure that different forms of the same 
word are treated as one term (e.g., information and 

informational); (iv) the vector representation of the 
document is created and stored. As a byproduct, the 

master vocabulary of the repository is constructed. 
   Once the vector models of requirements are built, the 

actual retrieval process proceeds as follows. The list of 

queries, higher-level requirements for the case of forward-

tracing, is processed one-by-one and converted into query 

vectors using the same parse -> remove stopwords -> stem 

sequence. After that, similarity computation is performed 

for each query-design element pair. A list of design 

elements with non-zero similarity is created for each 

query, sorted in the descending order of the similarity 

value. These lists are returned to the analyst. 
   The first method implemented, 
vanilla vector 
retrieval,
has been described in Section 2. As the basis, 
we have used a generic IR system developed by a graduate 

student during an IR course taught by one of the co-

authors. The provided software had been modified to 

work with repositories of requirements and design 
Proceedings of the 11th IEEE International Requirements Engineering Conference 
1090-705X/03 $17.00  2003 
IEEE 
Authorized licensed use limited to: QUAID E AZAM UNIVERSITY. Downloaded on November 30,2020 at 08:55:58 UTC from IEEE Xplore.  Restrictions apply.  . 
elements, but the main computational procedures were 
kept intact. 
For the second method, 
retrieval with key-phrases
,we 
have augmented the traditional vector model, by 

associating a list of technical terms or key-phrases with 

the document repository. When the model-building 

software detected a technical term, it was added to the 

vocabulary and treated as any other term from then on. 

This allowed us to raise the relevance of matches related 

to technical terminology and exclude some coincidental 

matches. For example, our requirements and design 

elements contain the phrases ecs production 

environment
 and ecs archive metadata. In the standard 
vector model, the match on keyword ecs generated a 
false positive, but by qualifying the phrases above as 
vocabulary terms we were able to decrease the relevance 

of this match. On the other hand, because ecs archive 

metadata became a much more discriminating term than 

any of its individual components, the key-phrase match 

between two documents started to carry much more 

weight.We note here, that the generation of the list of 

technical terms is a reasonably simple and straightforward 
task: it is readily found in the definitions or acronyms 
sections of most requirement documents. 
The third method, 
thesaurus retrieval,
 took the idea of 
incorporating technical lingo into the retrieval process one 

step further. To aid vector retrieval we used a simple 

thesaurus
. Each entry of our thesaurus is a triple 
(ki,kj,ij),where 
kiand 
kj are vocabulary terms (either words or key-
phrases) and 
ij[0,1] is a 
perceived similarity coefficient
of 
kiand 
kj. The analyst assigns this coefficient to each 
thesaurus entry  , hence the qualifier perceived.During 
the model-building stage, thesaurus entries are recognized 

and added to the vocabulary as new terms, similar to the 

addition of key-phrases in the previous method. The main 
change in the behavior of this method with respect to the 
other two, however, comes during the query processing 

stage. When computing the similarity between a query 
requirement 
r=(r1,,rN) and a design element 
d=(w
1,,w-
N), the standard cosine computation receives an add-on 
that is generated by matches found via the thesaurus. More 

formally, letting 
Tdenote thethesaurus, the new similarity 
measure, 
simT,used in this method is computed as: 
.)(),(1122),,(1===++=NiNiiiTkkijjiijNiiiTrwrwrwqwqdsimijjiConstruction of a thesaurus for our thesaurus-based 
retrieval method is a reasonably straightforward 
procedure. Most of the information linking technical terms 

is present in data dictionaries and acronym lists that are 
typically found in the appendices of requirements 
documents. The analyst has to assign similarity weights to 

each constructed pair, but the final computation of 

similarity is fairly robust with respect to small fluctuations 

in these weights so the analyst needs only provide the 

ballpark estimate.  If the analyst chose not to assign 

such a value, a default value is assigned.  Using a 

thesaurus entry (corrupted data, missing packets, 1.0), 

we can establish similarity between a requirement and a 

design element which otherwise contain no matching 

terms. 

   All our IR algorithms were implemented in C++ running 

under Linux.  
4. Evaluation    We undertook a multi-faceted evaluation effort to ensure 

that our research objectives had been met.  We built two 

datasets from open source NASA Moderate Resolution 

Imaging Spectroradiometer (MODIS) documents [16] for 

this purpose.  One dataset contains ten high level 

requirements and ten lower level requirements and the 

other contains 19 high level and 50 low-level 

requirements.  The 10x10 dataset was a subset of the 

19x50 dataset.  We then verified the 10x10 trace and the 
19x50 trace.  To accomplish this, we had a senior analyst 
with 20 years of experience examine the traceability 
matrix provided in Table 7-1 of the MODIS Science Data 

Processing Software Requirements Specification Version 

2 document [16].  These high level requirements were 
traced down to the Level 1A (L1A) and Geolocation 

Processing Software Requirements Specification [15].  
Several changes were made based on this review.  For 
example, some links were deleted from the RTM and 

some links were added.  Approximately 90% of the RTM 
remained unchanged though. 

   Second, we ran the vanilla vector retrieval algorithm on 
the 10x10 and 19x50 datasets.   We developed an analysis 
tool to compare the result matrix generated by the query 

tool to the manually verified RTM (see Section 5).  The 

tool, written in C++, computes precision and recall for 
each document and for the whole of the dataset.
We used 
the data analysis tool to examine the results.  We also 
asked two junior analysts with less than 5 years of 

experience to manually trace the 10x10 dataset.  The 
results of these tracing activities are shown in 
Table 1.As can be seen, the analysts tied or outperformed the 
vanilla vector algorithm in overall recall (by 0  20%).  
One analyst outperformed precision by 12.4% while the 

other analyst was outperformed by 2.6%.  The vanilla 

vector algorithm had slightly better recall on the larger 

dataset, but lower precision.  The analysts recorded the 

amount of time it took them to build candidate lists and 
Proceedings of the 11th IEEE International Requirements Engineering Conference 
1090-705X/03 $17.00  2003 
IEEE 
Authorized licensed use limited to: QUAID E AZAM UNIVERSITY. Downloaded on November 30,2020 at 08:55:58 UTC from IEEE Xplore.  Restrictions apply.  . 
perform a relative/similarity assessment of these (the same 
tasks performed by our algorithm).  It took the analysts 65 
minutes and 150 minutes, respectively, to perform the 
trace.  Not surprisingly, our algorithms ran in less than a 

minute. 
Table 1. Results of baselin
e algorithm compared 
to analysts. 
Analyst 
1Analyst 
2Vanilla 
vector 
(10 x 10) 
Vanilla 
vector 

(20 x 50) 
Recall23.0% 42.9%
 23.0% 25.4% 
Precision 15.0% 30.0% 17.6% 11.4% 
Performance 
(min.) 
65 150 seconds Seconds 
   These results are not surprising.  The vanilla vector 
algorithm works well only when the vocabularies of the 
high and low level requirements are close enough to 

generate multiple keyword matches.  In this case, the two 

levels of requirements had been written using very 
different terminology.  This contributed to low recall.  

Low precision can be explained by the fact that 
coincidental matches of common English words often 

obscured technical terminology.      
   Next, we implemented two additional IR algorithms that 

extend the vanilla vector algorithm as discussed in Section 

4.  The key-phrase retrieval algorithm slightly improved 

recall on the 10x10 dataset (see 
Table 3).  However, the 
precision went down.  This is also not surprising:  while 
the introduction of technical terminology allowed us to 
capture some previously undetected matches, more noise 

was also introduced. 

   The retrieval with thesaurus algorithm was tested on the 
19x50 dataset.  In addition, we had a senior analyst at 

Science Applications International Corporation (SAIC) 
trace the same dataset using the latest version of 

SuperTracePlus, the requirements tracing tool discussed 

in [12] and [17] in Section 2.1.  The results are shown in 
Table 2.  We show two sets of recall and precision 
measures.  The average recall and precision metrics 
represent the respective means of precision and recall 
values for the 19 individual requirements.  The overall 

precision and recall are computed as the fractions of the 

total number of correctly found matches to the total 
numbers of supplied answers and correct links, 

respectively.   
Table 2. Results of enhanced algorithms 
compared to SuperTracePlus. 
 SuperTracePlus 
Tool 
Analyst Retrieval with 
thesaurus 
algorithm 
Correct links 41 41 41 
Correct links 
found 
26 18 35Total number 
of candidates 
67 39 86 
Missed 
requirements 
36 4 Average recall 69.
37% 53.30% 
71.69% 
Average 
precision 
56.48% 
53.55% 32.76% 
Overall recall 63.
41% 43.9% 
85.36% 
Overall 
precision 
38.80% 
46.15% 
40.69% 
Performance 
(hours) 
N/A  included in 
analyst 
performance 
9 Seconds for 
algorithm, 
0.33 for 
thesaurus 
building 
   The 19x50 dataset included a number of high-level 
requirements with no matching low-level requirements.  
For the purposes of evaluation, we considered precision 

and recall for these requirements to be 100% if no 

candidates were produced and 0% otherwise.  This is 
accounted for in the average recall and precision measures 

but not in the overall recall and precision because the 
latter measures look only at the total number of correct 

links retrieved. 

   Note that the retrieval with thesaurus algorithm achieved 

recall of over 85% with 40.6% precision on a dataset that 

is only 19x50.  The recall outperforms SuperTracePlus 

by 22% and the analyst by a whopping 42%.  The 
algorithms average recall also outperformed the analyst 

by 18% and the SuperTracePlus tool by 2%.  The 
analyst outperformed the retrieval with thesaurus 
algorithm in precision per requirement by 21% and in 
overall precision by 5%.  The SuperTracePlus tool 

outperformed the algorithm only in precision per 

requirement, by 24%.  It should be noted that it took the 
analyst 13 hours to perform the trace.  Nine of the thirteen 
hours were spent on building a link library (keyword 

assignment, two hours) and tracing and data review 

(examining links and interactively searching for others, 

seven hours).  Our thesaurus was built in less than 20 

minutes by cutting and pasting from the data dictionary 

and other appendices of the document. 
Proceedings of the 11th IEEE International Requirements Engineering Conference 
1090-705X/03 $17.00  2003 
IEEE 
Authorized licensed use limited to: QUAID E AZAM UNIVERSITY. Downloaded on November 30,2020 at 08:55:58 UTC from IEEE Xplore.  Restrictions apply.  . 
   Taking a closer look, we see that our algorithm only 
missed 6 links while SuperTracePlus missed 15 and the 
analyst missed 17.  We found that there were three high 
level requirements that did have low level links for which 

SuperTracePlus found no match, four such 

requirements that were missed by our retrieval with 
thesaurus algorithm, while the analysts count was six (see 

missed requirements row of Table 3).  Two of the 
requirements that SuperTracePlus missed were caught 
by our algorithm.  One of the requirements that our 

algorithm missed was found by SuperTracePlus.  There 
were three requirements that both SuperTracePlus and 

the thesaurus retrieval algorithm missed.  As part of our 

future work, we will examine these three requirements 

carefully to understand how our algorithms might be 

improved.
   The SAIC analyst made a number of observations 
during the tracing activity: 
It was difficult to do some of the tracing because the 
two documents were incomplete.  Section titles would 

have helped.  Also, some requirement text was 

incomplete and ambiguous.  For example, some 

sentences were incomplete sentences, did not have a 

subject, or in one case said "Deleted." 
Not knowing acronyms hindered the linking process. 
The linking of parent and child requirements does not 
take into account the analyst-assigned status.  For 

example, I might have selected a link or several links, 

but selected a status of "Partial." 
   The first observation is accurate.  The requirements 
were extracted from the RTM of the documents and the 

section headings were no
t repeated with 
the requirement 
text.  We also found the second observation to be true as 

we verified the trace and also as we interviewed the two 
analysts who manually traced the 10x10 dataset.  The final 

comment is also valuable.  Basically the analyst is saying 
that (s)he may not have been convinced that a high level 
requirement was satisfied.  But the analyst could not find 

any more links for it, so had to leave it.  The analyst 
therefore would have noted that requirement as only 
partially satisfied.  Our study did not allow for collection 

of that information.  We will consider this for improving 

our future studies. 
Analysis of our retrieval algorithms showed the presence 
of many false positives.  We also noticed that many of 

these were returned with very low relevance.  In order to 

analyze the true effectiveness of our algorithms, we chose 

to implement various thresholds to trim the lists of 

candidate links.   Decreasing the size of the lists this way 

allows us to improve precision at a potential cost to recall.  
We have decided upon four thresholds: Top 4 candidates, 
any candidates with similarity above 0.25, and any 
candidates with similarity within 0.33 and 0.50 of the 
similarity of the top candidate.   
Table 3 shows how overall recall ([R]) and precision 
([P]) for all three of our algorithms change for different 

trimming thresholds.  Note that Above 25% trimming 

yields the highest results for the first two algorithms, but 

not for the retrieval with thesaurus algorithm. 
Table 3. Algorithm results after trimming. 
 Vanilla 
algorithm 
(10x10) 

[R,P] 
Retrieval w/key 
phrases 
algorithm 

(10x10) 
[R,P] 
Retrieval 
w/thesaurus 
algorithm 
(19x50) 
[R,P] 
No 
trimming 
[23%,17.6%] [27.2%,5.2%] 
[85.4%,40.6%] 
Top 4 [23%,17.6%] [27.2%,8.3%] [36.5%,30.6%] 
Above 
25[23%,
75%]
 [27.2%,
25%]
[9.7%,40%] 
Within 
33[23%,23%] [27.2%,15.7%] 
[48.7%,
44.4%]
Within 
50[23%,20%] [27.2%,15.7%] 
[58.5%,42.1%] 
5. Conclusions and future work 
   In this paper, we have studied a method for improving 
candidate link generation by applying IR techniques.     
We started with a classical vector space model algorithm, 

the vanilla vector algorithm.  We found that this algorithm 
does not outperform analysts or existing tools in terms of 

recall or precision, but that it does perform faster and with 
no keyword assignment required of analysts.  Next, we 

developed two extensions to this algorithm.  The first uses 
a simple key-phrase list, one that can be easily pulled from 

the definitions or acronym section of a requirement 

document.  The retrieval with key-phrases algorithm 

resulted in improved recall but with decreased precision.   

Next, we added a simple thesaurus.  We found this 
information to be readily available in the definition list 

and the data dictionary in the appendix of the traced 

documents.  Testing the thesaurus-based retrieval 

algorithm on the 19x50 dataset, we found that recall 

improved to 85% and that precision moved up to 40%. 

   Evaluation of the algorithms against a comparable 

keyword-based tool and analysts
 showed that the retrieval 
with thesaurus algorithm outperforms all in terms of recall 
and sometimes - in terms of precision. 
Proceedings of the 11th IEEE International Requirements Engineering Conference 
1090-705X/03 $17.00  2003 
IEEE 
Authorized licensed use limited to: QUAID E AZAM UNIVERSITY. Downloaded on November 30,2020 at 08:55:58 UTC from IEEE Xplore.  Restrictions apply.  . 
  Note that the techniques have been evaluated using a 
forward trace process.  However, these techniques can just 
as easily be applied to back tracing and lateral tracing.  
   We found that a number of things pose problems for 

analysts:  incomplete or ambiguous requirement 

documents; undefined acronyms; lack of intimate domain 
area or project knowledge; and different lingo in which 

the high- and low-level documents are written. 
   We note that the methods studied in this paper address 
the problem of automating the candidate link generation.  

It is imperative to have the analyst examine the final 
candidate list to effectively complete requirements tracing.  

By improving the candidate link lists, we reduce the 

burden on the analyst.  In addition to that, using feedback-

based retrieval techniques, we can make this last stage of 

the process more efficient.  We are currently in the 
process of developing such a feedback agent for our 
candidate link generator software.  Another interesting 

area for future research is using IR techniques to predict 
the coverage or satisfaction of traced requirements by 
their lower level requirements.   

   The initial results are promising and indicate that 

additional work is warranted.  The results, however, are 

limited and the effectiveness of our IR models on a 

broader scale remains to be seen.  A much larger scale 

study is required before any broad conclusions can be 

reached.  We have secured agreement from the 
International Space Station to allow use to use their 
documents for tracing studies.  This will allow us to trace 

several thousand high-level requirements to tens of 

thousands of low-level elements.  Weare confident that 

our algorithms will only perform better as the document 

collection size increases. 
6. Acknowledgments 
Our work is funded by NASA under grant NAG5-
11732.  Our thanks to Ken McGill, Tim Menzies, 
Stephanie Ferguson, Pete Cerna, Mike Norris, Bill 
Gerstenmaier, Bill Panter, the International Space Station 

project, and the MODIS project for maintaining their 

website that provides such useful data.  We thank Hua 
Shao for assistance with the vanilla retrieval algorithm and 
K.S. Senthil for his assistance with evaluation.
7. References 
[1]Abrahams, M. and Barkley, J., "RTL Verification 
Strategies," IEEE WESCON/98, 15 - 17 September 1998, 
pp. 130-134.  
[2]Anezin, D., "Process and Methods for Requirements 
Tracing (Software Development Life Cycle)," Dissertation, 

George Mason University, 1994. 
[3]Antoniol, G., Canfora, G., Casazza, G., De Lucia, A., and 

Merlo, E. Recovering Traceability Links between Code and 
Documentation. IEEE Transactions on Software 
Engineering, Volume 28, No. 10, October 2002, 970-983. 
[4]Bohner, S., "A Graph Traceability Approach for Software 

Change Impact Analysis," Dissertation, George Mason 
University, 1995.  
[5]Brouse, P., "A Process for Use of Multimedia Information 
in Requirements Identification and Traceability," 
Dissertation, George Mason University, 1992. 
[6]Capability Maturity Model, 
http://www.sei.cmu.edu/cmm/cmms/cmms.html 
[7]Casotto, A.. Run-time requirement tracing, Proceedings of 

the IEEE/ACM International Conference on Computer-
aided Design, Santa Clara, CA, 1993. 
[8]Cleland-Huang, J., Chang, C.K., Sethi, G., Javvaji, K.; Hu, 
H., Xia, J. (2002) Automating speculative queries through 
event-based requirements traceability. 
Proceedings of the 
IEEE Joint International Requirements Engineering 

Conference (RE02)
, Essex, Germany, 9-13 September, 
2002, pages: 289- 296. 
[9]Dag, J., Regnell, B., Carlshamre, P., Andersson, M., 
Karlsson, J.  A feasibility study of automated natural 
language requirements analysis in market-driven 
development, Requirements Engineering, Vol. 7, Issue 1, 

p.20, June 2002. 
[10]Daeza-Yates, R. and Ribeiro-Neto, B. 
Modern Information 
Retrieval, 
Addison-Wesley, 1999. 
[11] W. Frakes, R. Baeza-Yates (Eds.), 
Information Retrieval: 
Data Structures and Algorithms
, Prentice Hall, 1992. 
[12]Hayes, J. Huffman. Risk reduction through requirements 
tracing.  InThe Conference Proceedings of Software 
Quality Week 1990, San Francisco, California, May 1990. 
[13]Holagent Corporation product RDD-100, 
http://www.holagent.com/new/pr
oducts/modules.html 
[14]ISO 9000-3:1997   Quality management and quality 
assurance standards -- Part 3: Guidelines for the application 
of ISO 9001:1994 to the development, supply, installation 
and maintenance of computer software, http://www.iso.ch/ 
[15]Level 1A (L1A) and Geolocation Processing Software 

Requirements Specification, SDST-059A, GSFC SBRS, 
September 11, 1997
.[16]MODIS Science Data Processing Software Requirements 
Specification Version 2, SDST-089, GSFC SBRS, 
November 10, 1997. 
[17]Mundie, T. and Hallsworth, F.  Requirements analysis 
using SuperTrace PC.  In Proceedings of theAmerican 
Society of Mechanical Engineers (ASME)  for the 
Computers in Engineering Symposium at the Energy & 
Environmental Expo 1995, Houston, Texas. 
[18]Pierce, R.  A requirements tracing tool, Proceedings of the 

Software Quality Assurance Workshop on Functional and 

Performance Issues, 1978. 
Proceedings of the 11th IEEE International Requirements Engineering Conference 
1090-705X/03 $17.00  2003 
IEEE 
Authorized licensed use limited to: QUAID E AZAM UNIVERSITY. Downloaded on November 30,2020 at 08:55:58 UTC from IEEE Xplore.  Restrictions apply.  . 
[19]Ramesh, B., "Factors Influencing Requirements 
Traceability Practice," Communications of the ACM, 
December 
1998, Volume 41, No. 12 pp. 37-44. 
[20]Ramesh, B.; Jarke, M
.Toward reference models for 
requirements traceability
;  IEEE Transactions on Software 
Engineering, Volume 27, Issue 1, January 2001,  

page(s): 58 93. 
[21]Rational RequisitePro, 

http://www.rational.com/pr
oducts/reqpro/index.jsp 
[22]Savvidis, I. "A Multistrategy Framework for Analyzing 
System Requirements (Software Development)," 
Dissertation, George Mason University, 1995. 
[23]Sparck Jones, K. and Willet, P. 
Readings in Information 
Retrieva,lMorgan Kaufmann Series in Multimedia 
Information and Systems, Morgan Kaufmann, 1997.  
[24]Team Software Process and Personal Software Process, 
http://www.sei.cmu.edu/tsp/ 
[25]Telelogic product DOORS, 

http://www.telelogic.com/pr
oducts/doorsers/doors/index.cf
m[26]Tsumaki, T. and Morisawa, Y. "A Framework of 
Requirements Tracing using UML," Proceedings of the 
Seventh Asia-Pacific Software Engineering Conference 
2000, 5 - 8 December 
2000, pp. 206 - 213. [27]Watkins, R. and Neal, M. "Why and How of Requirements 
Tracing," 
IEEE Software, Volume 11, Issue 4, July 1994, 
pp. 104-106. 
Proceedings of the 11th IEEE International Requirements Engineering Conference 
1090-705X/03 $17.00  2003 
IEEE 
Authorized licensed use limited to: QUAID E AZAM UNIVERSITY. Downloaded on November 30,2020 at 08:55:58 UTC from IEEE Xplore.  Restrictions apply. Session T2A PREDICTING STUDENT PERFORMANCE: AN APPLICATION OF DATA MI"G METHODS WITH AN EDUCATIONAL WEB-BASED SYSTEM 
Behrouz Minaei-Bidgoli 
I, Deborah A. Kashy ', Gerd Kortemeyer', William 
F. Punch Abstract - Newly developed web-based educational 
technologies offer researchers unique 
opportunifies to study how students learn and what approaches 
to learning lead 
to success. Web-based systems 
routinely collect vas1 quantifies of data on 
user patterns, and data mining methods can be 
applied to these databases. This paper presents 
on approach to classifying students in 
order to predict theirfinal grade based on 
features extracted from logged doto in an education web-based 
system. We design. 
implemenf, and evaluate a series of pattern classifiers and compare 
their performance on an online 
COWSE dnfaset. A combination of multiple classifiers leads 
to a significant improvement in 
classification performance. Furthermore, 
by learning an 
appropriate weighting of 
the features used via a genetic algorithm (CA), we further improve prediction accuracy. The CA is 
demonstrated to successfully improve 
the accuracy of combined 
classifier performance. 
about IO to 12% when comparing fo non-CA classijier. This 
methodmay be ofconsiderable usefulness in identifying students 
at risk early. especially in 
very large classes, 
and allow the 
instructor to provide appropriate advising 
in a timely nmnner. Index Terms 
- Data Mining, Classificorion, Prediction, 
Combination ofMultiple Classifiers, Genetic 
Algorithm INTRODUCTION Many leading educational institutions are 
working to establish an 
online teaching 
and learning presence. Several 
systems with 
different capabilities and 
approaches have been 
developed to deliver online 
education in an academic setting. In particular, 
Michigan State University (MSU) has pioneered some of these systems to provide an infrastructure for online instruction. The research presented here was performed on a part of the latest online educational 
system developed at MSU, the Learning Online Network 
with Compuler-Assisted Personalized Approach 
(LON-CAPA) Two large databases are being developed in LON- CAPA. The first contains educational resources 
such as web pages, demonstrations, simulations, 
and individualized problems designed for use 
on homework assignments, quizzes, and examinations 
[2]. As more instructors develop [I]. educational materials for 
their courses to use 
with the LON- CAPA system, the content of this database grows. The second database contains 
information about student users of LON-CAPA. This database 
stores a wide range of variables (to be described shortly) including 
when, for how 
long, and how many 
times they 
access each resource, the 
number of 
correct responses they give on assigned 
problems, their pattern of correct and 
incorrect responses, 
and so on. Needless to say, with each 
semester, and as more instructors adopt the LON-CAPA system, this database grows 
rapidly. In this study we apply data mining 
methods to the LON- CAPA databases with the goals of answering the following two research questions: I) Can we find classes of students? In other words, do there exist 
groups of 
students who use these online resources in a 
similar way? If so, can we 
identify that 
class for any individual student? With 
this information, 
can we 
help a student use 
the resources better, based on the usage of the resource by other 
students in their groups? Can we classify the problems that have 
been used by students? If 
so, can we show how different types of problems impact 
students' achievement? Can we help instructors to 
develop the homework more effectively and efficiently? Some research and experiments 
have been done to 
reply the second research question 
[3]-[4]. In this paper, regarding 
the first research question, 
we hope to find similar patterns of use in 
the data gathered from LON-CAPA, and eventually be able 
to make predictions 
as to the most-beneficial course of studies for each learner based on their present usage. 
The system could then make suggestions 
to the learner 
as to how to best proceed. 
2) DATASET, CLASS LABELS, FEATURES As test data we selected 
the student and course data 
of an introductory physics course 
for scientists and engineers 
(PHY183), which was held at 
MSU in spring semester 2002. This course included 12 homework sets with a total 
of I84 problems, all of which were 
done online 
using LON-CAPA. About 261 students were initially enrolled 
in the course, 
' Berhouz MinaebBigdali, Michigan State University, Department of Computer Science, Genetic Algorithms Research 
and Applications Group (GARAGe), 
minaeibi@cse.msu.edu * Deborah A. Kashy, Michigan State University, Deplment of Psychology, hashyd@msu.edu ' Gerd Konemeyer, Michigan State University, 
Division of Science and Math Education, korte@lite.msu.edu ' William F. Punch, Michigan State Univerrity, Deparhlent of Computer Science and Engineering, Genetic Algorithms Research 
and Applications Gmup (GARAGe), punch@cse.msu.edu 0-7803-7961-6/03/$17.00 0 2003 IEEE November 54,2003, Boulder, CO 33'd ASEEIIEEE Frontiers in Education Conference 
T2A-13 Authorized licensed use limited to: QUAID E AZAM UNIVERSITY. Downloaded on November 30,2020 at 04:24:22 UTC from IEEE Xplore.  Restrictions apply. Session T2A however some of 
the students dropped 
the course after 
doing a couple of homework sets, 
so they do not have any final 
grades After removing those students, there remained 
221 valid sanples The final grade distribution 
of the students 
IS shown in FIGURE 1 FIGURE 1 GRAPH OF DISTRIBUTION OF GRADES IN COURSE PHYl83 Grade Dlstributlon I 0 15 30 12 5 g20 15 10 OD 10 23 IO 10 50 60 s Dr .td.nlr We can group the students regarding their 
final grades in several ways, 3 of which are: 1. The 9 possible class labels can 
be the same as 
students grades, as shown in table I 2. We can group them into three 
classes, high representing grades from 
3.5 to 4.0, middle representing grades 
from 2.5 to 3, and low representing grades less 
than 2.5, as shown in table 2. We can also categorize students with 
one of two class labels: Passed for grades above 
2.0, and Failed for grades less 
than or equal 
to 2.0, as shown in table 3. 3. TABLE 1 SLECTING 9 CLASS LABELS KEGARDNC TO STUDENTS GRADES 
ciarr Gmde Sludent 11 percentage 1 0.0 2 0.9% 2 0.5 
0 0.0% 1 in in 4 4% TABLE 1 SLECTING 9 CLASS LABELS KEGARDNC TO STUDENTS GRADES 
1 0.0 2 0.9% 7 ns n n no/. ciarr Gmde Sludent 11 percentage 1 in in 4 4% ~~ ... 4 1.5 28 12.4% 5 2.0 23 10.1% 6 2.5 43 18.9% 7 3.0 52 22.9% 8 3.5 41 18.0% 9 4.0 28 12.4% TABLE 2 SELECTNO 3 CLASS LABELS REGARDING TO STLIDENTSGRADES 
Class Grade Student P Percentage High Grade >= 3.5 69 30.40% Middle 2.0 < Grade < 3.5 95 41.80% Low Grade <= 2.0 63 27.80% TABLE 3 SELECTHG 2 CLASS LABELS REGARDHG TO STUDENTS GRADES Class Gnide Student # Percentage Passed Grade > 2.0 I64 72.2% Failed Grade <= 2.0 63 27.80% We can 
predict that the error 
rate in the first class grouping should be 
higher than 
the others, because the 
sample size 
in the 9 classes differs considerably. 
Extractable Features 
An essential step in doing classification is selecting 
the features used 
for classification. 
Below wc discuss the 
features from LON-CAPA 
that were used, how they can 
be visualized (to help in selection) and why we 
normalize the data before classification. The following 
features are stored 
by thc 
LON-CAPA system : 1. 2. 3. 4. Total number 
of correct 
answers. (Succcss 
ratc) Getting the problem right on the first 
try, vs. those with 
high number of tries. (Success at the 
first try) Total number 
of tries for 
doing homework. (Number 
of attempts before correct answer 
is derived) Total time 
that passed from the first attempt, until the correct solution 
was demonstrated, regardless of the 
time spent 
logged in to the system. Also, 
the time at 
which the student got the problem correct 
relative to the due date. Usually better 
students get the homwork completed earlier. Total time spent on the problem regardless of whether they got the correct 
answer or not. Total time 
that passed from the 
first attempt through subsequent 
attempts until the last submission was demonstrated. Participating in the communication mechanisms, 
vs. those working alone. LON-CAPA providcs online 
interaction both with 
other students and with 
the instructor. Were these used? 
7. Reading the supporting 
material before attempting 
homework vs. attempting the homework 
first and then reading upon it. Submitting a lot of attempts in a short 
amount of time without looking 
up materiol in between, versus 
those giving it one try, reading 
up, submitting 
another one, and so forth. 9. Giving up on 
a problem versus students 
who continued trying up to the deadline. 
IO. Time of the first log on (beginning of assignment, middle d the week, last minute) correlated with 
the number of tries or number of solved problems. 
A student who gets 
all correct answers will not 
necessarily be in the successful group if 
thcy took 
an average of 5 tries per 
problem, but it should be verified fromthis research. The present classification experiment focuses 
on the first six features based on the PHY 183 Spring 2002 class data. 
5. 6. 8. CLASSIFICATION Pattern recognition has a wide variety of applications in many different 
fields, such that it is not possible to come up with a single classifier 
that can give good results in all cases. The optimal classifier 
in every case is highly dependent 
on November 5-8.2003. Boulder. CO 0-7803-7961-6/03/$17.00 0 2003 IEEE 33d ASEEilEEE Frontiers in Education Conference TZA-14 Authorized licensed use limited to: QUAID E AZAM UNIVERSITY. Downloaded on November 30,2020 at 04:24:22 UTC from IEEE Xplore.  Restrictions apply. Session T2A the problem domain. 
In practice, one might come across a case where no single classifier can 
classify with an acceptable level of accuracy. 
In such cases 
it would be better 
to pool the results of 
different classifiers to 
achieve the 
optimal accuracy. 
Every classifier operates well 
on different aspects of the training or 
test feature vector. As a result, 
assuming appropriate 
conditions, combining multiple 
classifiers may improve classification performance 
when compared with any single classifier. The scope of 
this survey is restricted to comparing some 
popular non-parametric pattern classifiers 
and a 
single parametric pattern 
classifier according 
to the error estimate. 
Six different classifiers 
using the LON-CAPA datasets are compared in this study. The classifiers used in 
this study include Quadratic Bayesian classifier. 
heorest neighbor (I-NN), k-nearest neighbor (k-NN), Parzen-window, multi- layer perceptron (MLP), and Decision Tree. These classifiers are 
some of the common 
classifiers used in most practical classification problems. 
After some preprocessing operations were made on the dataset, the 
error rate 
of each classifier is reported. finally, to improve performance, a 
combination of 
classifiers is presented. Normalization Having assumed in Bayesian and 
Parzen-window classifiers 
that the features are normally distributed, 
it is necessary that the data 
for each 
feature be normalized. This ensures that each feature has 
the same weight in the decision process. 
Assuming that the 
given data is Gaussian distributed, this 
normalization is performed using 
the mean 
and standard 
deviation of the training data. In order to normalize the training data, it is necessary first to calculate 
the sample 
mean p , and the 
standard deviation 0 of each feature, or 
column, in this dataset, 
and then normalize the data using the equation( I). *, = zL2L (1 ) (r This ensures 
that each 
feature of the training dataset has 
a normal distribution with a mean of zero 
and a standard deviation of 
one. In addition, the 
kNN method requires 
normalization of 
all features nto the same 
range. However, 
we should 
be cautious in using the 
normalization before considering its effect on classifiers performances. 
Combination of Multiple Classifiers (CMC) By combining multiple classifiers 
we hope to improve 
classifier performance. 
There are differcnt ways one can think of combining classifiers: 
The simplest 
way is to find the overall error rate of the classifiers and choose 
the one which has the least 
error rate on the given dataset. This 
is called an 
ofjim CMC. This may not really 
scem to be a CMC; however, in * The first five clarsitiers are coded in MATLABT6.0, and lbrthe decision me classifiers we have used some available sonware packages such as C5.0, CART, QUEST, and CRUISE. general, it has a better performance 
than individual 
classifiers. The second method, which 
is called online CMC, uses all the classifiers followed 
by a vote. 
The class getting 
the maximum votes from the 
individual classifiers will 
be assigned to the test sample. This method intuitively seems to be better than the previous 
one. However, when tried on some cases of OUT dataset, the esults were not better than 
the best result in previous method. So, we changed the rule 
of najority vote from 
getting more than 50% voles to gelling more 
than 75% votes. This resulted in a significant improvement over 
offline CMC. Using the second method, 
we showed that CMC can achieve a significant 
accuracy improvement 
in all three cases of 2, 3, and 9-classes for the PHY183 data, in which we are going 
to use GA to optimize the 
CMC performance. MAPTHE PROBLEM TO GENETIC ALGORITHM Genetic Algorithms have 
been shown to be 
an effective tool 
to use indata mining and pattern 
recognition [5]-[12].. An important aspect 
of GAS in a learning context 
is their use in pattern recognition. 
There are two different ipproaches 
to applying GA in pattern recognition: 
Apply a 
CA directly as a classifier. Bandyopadhyay 
and Murthy in [I31 applied CA to find the decision boundary in N dimensional feature space. 
Use a GA as an optimization tool for resetting the parameters in other classifiers. Most applications of 
GAS in pattern recognition optimize some 
parameters in the classification process. Many researchers have used GAS in feature selection [14]-[17]. GAS has been applied to find an optimal set 
of feature weights 
that improve classification accuracy. First, a traditional 
feature extraction 
method such as Principal Component Analysis (PCA) 
is applied, and 
then a classifier such 
as k-NN is used to calculate the fitness function for GA 
[IS]. Combination ofclassifiers is another area 
that GAS have been used 
to optimize. Kuncheva and Jain 
in [I91 used a CA for selecting the features 
as well as selecting the types of 
individual classifiers in their design of a 
Classifier Fusion 
System. GA is also used in selecting the prototypes 
in the case-based classification 
[ZO]. In this paper 
we will focus on the second approach and use a GA to optimize a combination of classifiers. Our objective is to predict the students final grades based on 
their web-use 
features, which 
are extracted from 
the homework data. We design, implement, 
and evaluate a 
series of pattern classifiers with various parameters 
in order to compare their performance on a dataset from 
LON-CAPA. 0-7803-7961-6/03/$17.00 0 2003 IEEE 33d ASEE/IEEE Frontiers in Education 
Conference TZA-I5 November 5-8,2003, Boulder, CO Authorized licensed use limited to: QUAID E AZAM UNIVERSITY. Downloaded on November 30,2020 at 04:24:22 UTC from IEEE Xplore.  Restrictions apply. Session T2A validation by dividing the total number of misclassified examples into total number OF test examples. Therefore, 
our /ittiess futrction measures the 
error rate achieved by performance (minimize 
the error rate). 
CMC and our objective would be to maximize this EXPERIMENT RESULTS Error rates for the individual classifiers, their combination 
and the GA optimized combination are presented. 
Optimizing the CMC Using a CA We used GATOOIBOX~ for MATLAB to implement a GA to optimize classification performance. 
Our goal is to find a population of best weights for 
every feature vector, which minimize the classification error rate. 
The feature vector 
for our predictors are 
the set of six variables for every student: 
Success rate, Success at the first try, Number of attempts before correct 
answer is derived, the time at which the student got 
the problem correct relative 
to the due date, 
total time spent on the 
problem, and 
the number 
of online interactions of 
the student both with 
other students and 
with the instructor. We randomly 
initialized a population of six dimensional weight vectors with values between 0 and 
I, corresponding to the feature vector and experimented 
with different 
number of population 
sizes. We found good results using a population with 200 individuals. The GA Toolbox supports 
binary, integer, reahalued and floating-point chromosome 
representations. We used the simple genetic 
algorithm (SGA), which is described by G oldberg in [ 2 I]. The S GA uses common 
CA operators to find a population oFsolutions which optimize the fitness 
values. Fitness Function 
During the reproduction phase, each individual 
is assigned a fitness value derived 
from its raw performance measure 
given by 
the objective function. 
This value 
is used in the selection 0 bias towards more fit individuals. Highly 
fit individuals. relative to the whole 
population, have a high probability of being selected 
for mating whereas less fit 
individuals have a correspondingly low probability of being 
selected. The error rate is measured in each round of cross Performance % Classifier 2Classes 3Clasrer 9Clanrer CMCof4C1assifrrs without GA 8387 k 1.73 61.86 f 2.16 49.74 f 1.86 CA Optimized CMC, ~~~~i~,ji~d~~l 94.09f2.84 72.13f0.39 62.25c0.63 lmprovment 10.2zi 1.92 10.26i. 1.84 12.51 & 1.75 TABLE 4 COMPANNC THEERROR RATEOFALLCLASSIFIERS ONPHY 183 OATASETIN THECASESOP 2-aASSES 3cLASSESS AND9-aASSES USING IO-FOLO CROSS VALIDATIOK WITHOUTW For GA optimization, 
we used 200 individuals in our population, running the 
CA for 500 generations. We 
ran the program 10 times and 
show the averages 
in table 5. In every run, 500 X 200 times fitness function are 
used in the IO-fold cross validation 
to measure the average 
performance of 
CMC. Thus every classifier 
is called 3 x IO6 times for the case of 2-ciasses. 3-classes and 9-classes. The time overhead 
for fitness evaluation is therefore 
a critical issue. Since using the MLP 
in this process 
takes about 
2 minutes while the 
other four 
non-tree Classifiers (Bayes, INN, 3. and Parzen window) take only 3 seconds collectively, we omitted the MLP 
from our 
classifiers group so we could obtain the results in a reasonable time. 
TABLE 5 COMPANNCTHECMCPERFORMANCE o~PHY183 DATASET USINGGA Authorized licensed use limited to: QUAID E AZAM UNIVERSITY. Downloaded on November 30,2020 at 04:24:22 UTC from IEEE Xplore.  Restrictions apply. Session T2A FIGURE 2 WITHOUTCA. CWRTOF COMPARINGCMCAVERAGEPERFORMANCE, USING GA AND L,""C Pn(o"".nse WiVlD", OA 0 OA Opfimizcd CMC I rm 2-c,.**cr Figure 3 shows the best result 
of the ten runs 
over our dataset. These charts represent 
the population mean, the best 
individual at each generation 
and the best value yielded by the run. FIGURE 3 GRAPH OFGA @TIMIZED CMCPERFORMANCE MTHECASEOF 2,3,AND Finally, we 
can examine the individuals 
(weights) for 
features by which 
we obtained the improved results. 
This feature weighting indicates 
the importance of each feature for making the required classification. 
In most cases the results are similar 
to Multiple Linear Regressions 
or tree- based software that use 
statistical methods to measure feature importance. Table 6 shows the 
importance of the six features in the 3classes case 
using the Entropy splitting 
criterion. Based on entropy, a statistical property called 
information goin measures how 
well a 
given feature separates the training 
examples in relation to their target 
classes. Entropy characterizes impurity of an arbitrary collection of examples S at a specific node N. In [22] the impurity of a node N is denoted 
by i(N) such that: 
where 49) is the fraction 
of examples at node N that go to TABLE 6 FEATURE IMPORTANCE INl-(ZASSES USING FNTROPY CRmRlON 1mpOrtanee % Tatal_Col~ect _Answers 100.00 Total_Number-of_T"eS 58.61 First_Ciot_Colrect 27.70 Time-Spent-to-Solve 24.60 Total-Time-Sp ert 24.47 Communication 9.21 The CA results also 
show that the "Total 
number of correct answers" and 
the 'Total number of tries" 
are the 
most important 
features for 
the classification. The second 
column in table 6 shows the percentage 
of feature As a result, 
having the information generated through 
our experiment the instructor 
would be able to identify 
students at risk early, especially 
in very large classes, 
and allow the instructor 
to provide appropriate advising 
in a timely manner. .. - -- 0-7803-7961-6/03/$17.00 0 2003 IEEE November 5-8,2003, Boulder, CO 33'd ASEElIEEE Frontiers 
in Education Conference 
T2A-17 Authorized licensed use limited to: QUAID E AZAM UNIVERSITY. Downloaded on November 30,2020 at 04:24:22 UTC from IEEE Xplore.  Restrictions apply. Session T2A [5] Freitas, A.A. "A survey of Evolutbnaly Algorithms for Data Mining and Knobledge Discovery",See: 
www.pgia.pucpr.brl-alexiplpers. To appear in: A. Ghosh and S. Tsutsui. (Ede.)Advnncer in Evnlurionnry Compulnrion. Sprhgec Vdag, 2M12. lain, A. K.; Zongker, D. "Fcature Selection: Evaluation, Application, and Small Sample Performance", IEEE Transodion on Pollen, Annlysis ond Mnchineln:el/ige,ice, Val. 19, No. 2, February 1991. 
Wiley & Sans, 1998. Pei, M., Goodman, E.D., and Punch,W.F. "Pattem 
Discovely from Data Using 
Genetic Algonlhmr", Proceeding of I" Pncrlic-Asin Confirenee Knowledge Discovev & Dol" Mining (PAKDD-97). 
Feb 1997. CONCLUSIONS AND FUTURE WORK Four classifiers were used A combination of multiple classifiers leads 
to a significant accuracy improvement in all Weighing the features and using a genetic algorithm 
to minimize the error rate improves the prediction accuracy at least 
10% in the all 
features is low, the 
feature weighting worked 
much better than feature selection, ~h~ successful optimization of student classification 
in all three cases 
demonstrates the merits of usine the LON-CAPA data to Dredicl the students' segregate the [6] of 2, and   cl asses, where the number of [7] Falkenauer E. GenelicA~go~ilhmsnndGroupingProbiems John [8l ~ [9] Park Y and Song M. "A genetic algorithm for clustering problems. Genetic Programming" 
Proceeding of 3rd Aanuol Confimtee. 568- 575. Morgan Kauhann.1998. Programs". 3rd Ed. Springer-Verlng. 1996. algorithms for canceptleaming". Modine Leoming 13, 161 -188, final grades based on their features, which are 
extracted from the homework data. 
We are going to gather 
more sample 
data by combining one course data during 
several Semesters to avoid overfittins in the case of %Classes. We 
also try to find 
the paths that 
students usually choose 
to solve the different types 
of the problems from activity log to extract more relevant features. 
We also want to aoolv Evolutionarv 
Aleorithms to 
find 1993. [ Michalewicr z. Algonthmr + Data Structures= Pvolution [I I] De Jong K.A., Spears W.M. and Gordon D.F. (I 993). "Using genetic ,, , 1- Association Rules and Bpendency among the groups 
of problems (Molhenioricol. Optional Response. 
Numerical, Java Appler. and so forth) of LON-CAPA homework data 
sets. The 
present investigation 
has dealt only with homework 
data; other components 
ofthe course such 
as quizzes, mid- term examination, 
and attendance can be predictive of outcome, and will bc 
included in further studies [4]. As more and more students 
enter the onlinc 
learning environment, databases concerning 
student access and study patterns will grow. In this paper 
we have 
shown that data mining efforts can be useful 
in predicting student outcomes. 
We hope to refine 
our techniques so that the 
information generated by data mining can be usefully applied 
by instructors to increase student learning. 
ACKNOWLEDGMENT This work was partially supported by the National Science Foundation under ITR 
0085921. We are 
thankful to Prof. 
Edwin Kashy for 
his help and cooperation. REFERENCES Konemeyer, G., Bauer, W., Kashy, D. A., Khshy, E., & Speier, C., 'me LeamingOnline Network with CAPA Inilialivc", Procecdbigs of ,lie Fronrierr in Educnlion corgGereiicf, 2001. See also: http:llwww.lancapa.org Kanhy, D. A., Albertelli, G., Ashkenazi, G., Kanhy E. Ng, H. K., & Thoennessen, M., "Individualized intractive cxcrciscs: A promising role far network technology", Proceedings o/rhe Froriliers in Edueolion confirence, 200 I Albenelli,G., Minaei-Bigdoli, B., Punch. W.F., Kortemeyer. G.. & Kashy, E., "Concept Feedback 
In Computer-Assisted Assigfimentr", 
Proceedings of rhe Froriliers in Educnliori cot~eu~ence, 2W2. Kashy, D. 
A., Albenelli, G..Karhy. E. &Thoenncssen, M. "Teaching with ALN Tcchnology: Benefits 
and Casts",Jorminl of E,igi,zeeritzg Educnrion. 90,499-506,2001 0-7803-7961-6/03/$17.00 0 2003 IEEE November 5-8.2003, Boulder, CO 33'd ASEEilEEE Frontiers in Education Conference T2A-18 Authorized licensed use limited to: QUAID E AZAM UNIVERSITY. Downloaded on November 30,2020 at 04:24:22 UTC from IEEE Xplore.  Restrictions apply. 1556-603X/10/$26.002010IEEE
 NOVEMBER 2010 | IEEE COMPUTATIONAL INTELLIGENCE MAGAZINE    
13Research  FrontierItamar Arel, Derek C. Rose, and Thomas P. Karnowski

The University of Tennessee, USA
I. IntroductionMimicking the efficiency and 
robustness by which the human 

brain represents information 
has been a core challenge in arti-

ficial intelligence research for 

decades. Humans are ex 
-posed to myriad of senso-

ry data received every 

second of the day and are 

somehow able to capture 

critical aspects of this 

data in a way that allows 

for its future use in a con-

cise manner. Over 50 years 

ago, Richard Bellman, who 

introduced dynamic programming 

theory and pioneered the field of opti-

mal control, asserted that high dimen-

sionality of data is a fundamental hurdle 

in many science and engineering appli-

cations. The main difficulty that arises, 

particularly in the context of pattern 

classification applications, is that the 

learning complexity grows exponen-

tially with linear increase in the dimen-

sionality of the data. He coined this 

phenomenon the curse of dimensional-

ity [1]. The mainstream approach of 

overcoming the curse has been to 

pre-process the data in a manner that 

would reduce its dimensionality to that 

which can be effectively processed, for 

example by a classification engine. This 

dimensionality reduction scheme is 

often referred to as feature extraction. 

As a result, it can be argued that the 

intelligence behind many pattern rec-
ognition systems has shifted to the 
human-engineered feature extraction 

process, which at times can be challeng-

ing and highly application-dependent 
[2]. Moreover, if incomplete or 
erroneous features are ex 
-tracted, the classification 
process is inherently lim-
ited in performance.
Recent neuroscience 
findings have provided 

insight into the princi-
ples governing informa-
tion representation in the 
mammalian brain, leading to 
new ideas for designing sys-
tems that represent information. 
One of the key findings has been that 

the neocortex, which is associated with 

many cognitive abilities, does not explic-

itly pre-process sensory signals, but rath-

er allows them to propagate through a 

complex hierarchy [3] of modules that, 

over time, learn to represent observa-

tions based on the regularities they 

exhibit [4]. This discovery motivated the 

emergence of the subfield of deep 

machine learning, which focuses on 

computational models for information 

representation that exhibit similar char-

acteristics to that of the neocortex. 
In addition to the spatial dimension-ality of real-life data, the temporal com-

ponent also plays a key role. An observed 

sequence of patterns often conveys a 

meaning to the observer, whereby inde-

pendent fragments of this sequence 

would be hard to decipher in isolation. 

Meaning is often inferred from events or 

observations that are received closely in 
time [5] [6]. To that end, modeling the 
temporal component of the observations 

plays a critical role in effective informa-

tion representation. Capturing spa-

tiotemporal dependencies, based on 

regularities in the observations, is there-

fore viewed as a fundamental goal for 

deep learning systems.
Assuming robust deep learning is 
achieved, it would be possible to train 

such a hierarchical network on a large set 

of observations and later extract signals 

from this network to a relatively simple 

classification engine for the purpose of 

robust pattern recognition. Robustness 

here refers to the ability to exhibit classi-

fication invariance to a diverse range of 

transformations and distortions, including 

noise, scale, rotation, various lighting 

conditions, displacement, etc.
This article provides an overview of 
the mainstream deep learning approach-

es and research directions proposed over 

the past decade. It is important to 

emphasize that each approach has 

strengths and weaknesses, depending on 

the application and context in which it 

is being used. Thus, this article presents a 

summary on the current state of the 

deep machine learning field and some 

perspective into how it may evolve. 

Convolutional Neural Networks 

(CNNs) and Deep Belief Networks 

(DBNs) (and their respective variations) 

are focused on primarily because they 

are well established in the deep learning 

field and show great promise for future 

work. Section II introduces CNNs and 

subsequently follows by details of DBNs 

in Section III. For an excellent further 
n ar
ti-for-rswhoi[2]. Moerrotrapptimamnew 
h BRAND X PICTURESDeep Machine LearningA New Frontier 
in Artificial Intelligence Research
Digital Object Identifier 10.1109/MCI.2010.938364Authorized licensed use limited to: QUAID E AZAM UNIVERSITY. Downloaded on November 30,2020 at 04:54:03 UTC from IEEE Xplore.  Restrictions apply. 14    IEEE COMPUTATIONAL INTELLIGENCE MAGAZINE | NOVEMBER 2010
FIGURE 2 
Conceptual example of convolutional neural network. The input image is convolved 
with three trainable filters and biases as in Figure 1 to produce three feature maps at the C1 

level.  Each group of four pixels in the feature maps are added, weighted, combined with a bias, 

and passed through a sigmoid function to produce the three feature maps at S2. These are again 

filtered to produce the C3 level.  The hierarchy then produces S4 in a manner analogous to S2. 

Finally these pixel values are rasterized and presented as a single vector input to the conven-
tional neural network at the output. 
NNInputC1S2C3S4
in-depth look at the foundations of 
these technologies, the reader is referred 

to [7]. Section IV contains other deep 

architectures that are currently being 

proposed. Section V contains a brief note 

about how this research has impacted 

government and industry initiatives. The 

conclusion provides a perspective of the 

potential impact of deep-layered archi-

tectures as well as key questions that 

remain to be answered.
II. Convolutional Neural Networks
CNNs [8] [9] are a family of multi-layer 

neural networks particularly designed 

for use on two-dimensional data, such as 

images and videos. CNNs are influenced 

by earlier work in time-delay neural 

networks (TDNN), which reduce learn-

ing computation requirements by shar-

ing weights in a temporal dimension and 
are intended for speech and time-series 
processing [53]. CNNs are the first truly 

successful deep learning approach where 

many layers of a hierarchy are successful-

ly trained in a robust manner. A CNN is 

a choice of topology or architecture that 

leverages spatial relationships to reduce 

the number of parameters which must 

be learned and thus improves upon gen-

eral feed-forward back propagation 

training. CNNs were proposed as a deep 

learning framework that is motivated by 

minimal data preprocessing require-

ments. In CNNs, small portions of the 

image (dubbed a local receptive field) 

are treated as inputs to the lowest layer 

of the hierarchical structure. Information 

generally propagates through the differ-

ent layers of the network whereby at 

each layer digital filtering is applied in 

order to obtain salient features of the 
data observed. The method provides a 
level of invariance to shift, scale and 

rotation as the local receptive field allows 

the neuron or processing unit access to 

elementary features such as oriented 

edges or corners.
One of the seminal papers on the 
topic [8] describes an application of 

CNNs to the classification handwritten 

digits in the MNIST database. Essentially, 

the input image is convolved with a set 

of N small filters whose coefficients are 

either trained or pre-determined using 

some criteria. Thus, the first (or lowest) 

layer of the network consists of feature 

maps which are the result of the convo-

lution processes, with an additive bias 

and possibly a compression or normal-

ization of the features. This initial stage is 

followed by a subsampling (typically a 2 

3 2 averaging operation) that further 
reduces the dimensionality and offers 

some robustness to spatial shifts (see Fig-

ure 1). The subsampled feature map then 

receives a weighting and trainable bias 

and finally propagates through an activa-

tion function. Some variants of this exist 

with as few as one map per layer [13] or 

summations of multiple maps [8].
When the weighting is small, the 
activation function is nearly linear and 

the result is a blurring of the image; 

other weightings can cause the 

 activation output to resemble an AND 
or OR function. These outputs form a 

new feature map that is then passed 

through another sequence of convolu-

tion, sub-sampling and activation func-

tion flow, as illustrated in Figure 2. This 

process can be repeated an arbitrary 

number of times. It should be noted 

that subsequent layers can combine one 

or more of the previous layers; for 

example, in [8] the initial six feature 

maps are combined to form 16 feature 

maps in the subsequent layer. As 

described in [33], CNNs create their 

invariance to object translations by a 

method dubbed feature pooling (the 

S layers in Figure 2). However, feature 

pooling is hand crafted by the network 

organizer, not trained or learned by the 

system; in CNNs, the pooling is 

tuned by parameters in the learning 

process but the basic mechanism (the 
InputfxbxCxwx+1bx+1Sx+1*XFIGURE 1
 The convolution and subsampling process: the convolution process consists of con-
volving an input (image for the first stage or feature map for later stages) with a trainable filter fx then adding a trainable bias bx to produce the convolution layer Cx. The subsampling consists of 
summing a neighborhood (four pixels), weighting by scalar wx+1, adding trainable bias bx+1, and 
passing through a sigmoid function to produce a roughly 2x smaller feature map Sx+1.Authorized licensed use limited to: QUAID E AZAM UNIVERSITY. Downloaded on November 30,2020 at 04:54:03 UTC from IEEE Xplore.  Restrictions apply. NOVEMBER 2010 | IEEE COMPUTATIONAL INTELLIGENCE MAGAZINE    
15combination of inputs to the S layers, 
for example) are set by the network 

designer. Finally, at the final stage of the 

process, the activation outputs are for-

warded to a conventional feedforward 

neural network that produces the final 

output of the system.The intimate relationship between 
the layers and spatial information in 

CNNs renders them well suited for 

image processing and understanding, and 

they generally perform well at autono-

mously extracting salient features from 

images. In some cases Gabor filters have 

been used as an initial pre-processing 

step to emulate the human visual 

response to visual excitation [10]. In 

more recent work, researchers have 

applied CNNs to various machine 

learning problems including face detec-

tion [11] [13], document analysis [38], 

and speech detection [12]. CNNs have 

recently [25] been trained with a tem-

poral coherence objective to leverage 

the frame-to-frame coherence found in 

videos, though this objective need not 

be specific to CNNs. 
III. Deep Belief Networks
DBNs, initially introduced in [14], are 

probabilistic generative models that stand 

in contrast to the discriminative nature 

of traditional neural nets. Generative 

models provide a joint probability distri-

bution over observable data and labels, 

facilitating the estimation of both 

P(Observation
|Label) as well as 
P(Label|Observation
), while discrimina-
tive models are limited to the latter, 

P(Label|Observation
). DBNs address 
problems encountered when traditional-

ly applying back-propagation to deeply-

layered neural networks, namely: (1) 

necessity of a substantial labeled data set 

for training, (2) slow learning (i.e. con-

vergence) times, and (3) inadequate 

parameter selection techniques that lead 

to poor local optima. 
DBNs are composed of several layers 
of Restricted Boltzmann Machines, a 

type of neural network (see Figure 3). 

These networks are restricted to a sin-

gle visible layer and single hidden layer, 

where connections are formed between 

the layers (units within a layer are not 
connected). The hidden units are trained 
to capture higher-order data correlations 

that are observed at the visible units. Ini-

tially, aside from the top two layers, which 

form an associative memory, the layers of 

a DBN are connected only by directed 

top-down generative weights. RBMs are 

attractive as a building block, over more 

traditional and deeply layered sigmoid 

belief networks, due to their ease of 

learning these connection weights. To 

obtain generative weights, the initial pre-

training occurs in an unsupervised greedy 

layer-by-layer manner, enabled by what 

Hinton has termed 
contrastive divergence 
[15]. During this training phase, a vector 
v is presented to the visible units that for-
ward values to the hidden units. Going in 

reverse, the visible unit inputs are then 

stochastically found in an attempt to 

reconstruct the original input. Finally, 

these new visible neuron activations are 

forwarded such that one step reconstruc-

tion hidden unit activations, 
h, can be 
attained. Performing these back and forth 

steps is a process known as Gibbs sam-

pling, and the difference in the correla-

tion of the hidden activations and visible 

inputs forms the basis for a weight 

update. Training time is significantly 

reduced as it can be shown that only a 

single step is needed to approximate 

maximum likelihood learning. Each layer 
added to the network improves the log-
probability of the training data, which we 

can think of as increasing true represen-

tational power. This meaningful expan-

sion, in conjunction with the utilization 

of unlabeled data, is a critical component 

in any deep learning application. 
At the top two layers, the weights are 
tied together, such that the output of the 

lower layers provides a reference clue or 

link for the top layer to associate with 

its memory contents. We often encoun-

ter problems where discriminative per-

formance is of ultimate concern, e.g. in 

classification tasks. A DBN may be fine 

tuned after pre-training for improved 

discriminative performance by utilizing 

labeled data through back-propagation. 

At this point, a set of labels is attached to 

the top layer (expanding the associative 

memory) to clarify category boundaries 

in the network through which a new set 

of bottom-up, recognition weights are 

learned. It has been shown in [16] that 

such networks often perform better than 

those trained exclusively with back-

propagation. This may be intuitively 

explained by the fact that back-propaga-

tion for DBNs is only required to per-

form a local search on the weight 

(parameter) space, speeding training and 

convergence time in relation to tradi-

tional feed-forward neural networks. 
Top Level Units
Hidden UnitsHidden UnitsHidden UnitsAssociative Memory
Detection Weights
Generative Weights
HiddenVisible
RBM Layer
Weights
Observation Vector 
v(e.g., 32 
 32 Image)Label UnitsFIGURE 3
 Illustration of the Deep Belief Network framework.Authorized licensed use limited to: QUAID E AZAM UNIVERSITY. Downloaded on November 30,2020 at 04:54:03 UTC from IEEE Xplore.  Restrictions apply. 16    IEEE COMPUTATIONAL INTELLIGENCE MAGAZINE | NOVEMBER 2010
Performance results obtained when 
applying DBNs to the MNIST hand-
written character recognition task have 

demonstrated significant improvement 

over feedforward networks. Shortly 

after DBNs were introduced, a more 

thorough analysis presented in [17] 

solidified their use with unsupervised 

tasks as well as continuous valued 

inputs. Further tests in [18] [19] illus-

trated the resilience of DBNs (as well as 

other deep architectures) on problems 

with increasing variation. 
The flexibility of DBNs was recently 
expanded [20] by introducing the notion 

of Convolutional Deep Belief Networks 

(CDBNs). DBNs do not inherently 

embed information about the 2D struc-

ture of an input image, i.e. inputs are 

simply vectorized formats of an image 

matrix. In contrast, CDBNs utilize the 

spatial relationship of neighboring pixels 

with the introduction of what are 

termed convolutional RBMs to provide 

a translation invariant generative model 

that scales well with high dimensional 

images. DBNs do not currently explicit-

ly address learning the temporal rela-

tionships between observables, though 

there has been recent work in stacking 

temporal RBMs [22] or generalizations 

of these, dubbed temporal convolution 

machines [23], for learning sequences. 

The application of such sequence learn-

ers to audio signal processing problems, 

whereby DBNs have made recent head-

way [24], offers an avenue for exciting 

future research. 
Static image testing for DBNs and 
CNNs occurs most commonly with the 

MNIST database [27] of handwritten 

digits and Caltech-101 database [28] of 

various objects (belonging to 101 catego-

ries). Classification error rates for each of 

the architectures can be found in [19] 

[20] [21]. A comprehensive and up-to-

date performance comparison for various 

machine learning techniques applied to 

the MNIST database is provided in [27]. 
Recent works pertaining to DBNs 
include the use of stacked auto-encoders 

in place of RBMs in traditional DBNs 

[17] [18] [21]. This effort produced deep 

multi-layer neural network architectures 

that can be trained with the same prin-
ciples as DBNs but are less strict in the 
parameterization of the layers. Unlike 

DBNs, auto-encoders use discriminative 

models from which the input sample 

space cannot be sampled by the archi-

tecture, making it more difficult to inter-

pret what the network is capturing in its 

internal representation. However, it has 

been shown [21] that denoising auto-

encoders, which utilize stochastic cor-

ruption during training, can be stacked 

to yield generalization performance that 

is comparable to (and in some cases bet-

ter that) traditional DBNs. The training 

procedure for a single denoising autoen-

coder corresponds to the goals used for 

generative models such as RBMs.
IV. Recently Proposed 
Deep Learning Architectures
There are several computational archi-
tectures that attempt to model the neo-

cortex. These models have been inspired 

by sources such as [42], which attempt 

to map various computational phases in 

image understanding to areas in the cor-

tex. Over time these models have been 

refined; however, the central concept of 

visual processing over a hierarchical 

structure has remained. These models 

invoke the simple-to-complex cell orga-

nization of Hubel and Weisel [44], which 

was based on studies of the visual corti-

cal cells of cats. 
Similar organizations are utilized by 
CNNs as well as other deep-layered 

models (such as the Neocognitron [40] 

[41] [43] and HMAX [32] [45]), yet 

more explicit cortical models seek a 

stronger mapping of their architecture to 

biologically-inspired models. In particu-

lar, they attempt to solve problems of 

learning and invariance through diverse 

mechanisms such as temporal analysis, in 

which time is considered an inseparable 

element of the learning process.
One prominent example is Hierar-
chical Temporal Memory (HTM) devel-

oped at the Numenta Corporation [30] 

[33]. HTMs have a hierarchical structure 

based on concepts described in [39] and 

bear similarities to other work pertaining 

to the modeling of cortical circuits. With 

a specific focus on visual information 

representation, in an HTM the lowest 
level of the hierarchy receives its inputs 
from a small region of an input image. 

Higher levels of the hierarchy correspond 

to larger regions (or receptive fields) as 

they incorporate the representation con-

structs of multiple lower receptive fields. 

In addition to the scaling change across 

layers of the hierarchy, there is an impor-

tant temporal-based aspect to each layer, 

which is created by translation or scan-

ning of the input image itself. 
During the learning phase, the first 
layer compiles the most common input 

patterns and assigns indices to them. 

Temporal relationships are modeled as 

probability transitions from one input 

sequence to another and are clustered 

together using graph partitioning tech-

niques. When this stage of learning con-

cludes, the subsequent (second) layer 

concatenates the indices of the current 

observed inputs from its children mod-

ules and learns the most common con-

catenations as an alphabet (another group 

of common input sequences, but at a 

higher level). The higher layers charac-

terization can then be provided as feed-

back down to the lower level modules. 

The lower level, in turn, incorporates this 

broader representation information into 

its own inference formulation. This pro-

cess is repeated at each layer of the hier-

archy. After a network is trained, image 

recognition is performed using the 

Bayesian belief propagation algorithm 

[46] to identify the most likely input pat-

tern given the beliefs at the highest layer 

of the hierarchy (which corresponds to 

the broadest image scope). Other archi-

tectures proposed in the literature, which 

resemble HTMs, include the Hierarchi-

cal Quilted SOMs of Miller & Lommel 

[47] that employ two-stage spatial clus-

tering and temporal clustering using self-

organizing maps, and the Neural 

Abstraction Pyramid of Behnke [48].
A framework recently introduced by 
the authors for achieving robust infor-

mation representation is the Deep Spa-

tioTemporal Inference Network 

(DeSTIN) model [26]. In this frame-

work, a common cortical circuit (or 

node) populates the entire hierarchy, and 

each of these nodes operates indepen-
dently and in parallel to all other nodes. 
Authorized licensed use limited to: QUAID E AZAM UNIVERSITY. Downloaded on November 30,2020 at 04:54:03 UTC from IEEE Xplore.  Restrictions apply. NOVEMBER 2010 | IEEE COMPUTATIONAL INTELLIGENCE MAGAZINE    
17This solution is not constrained to a lay-
er-by-layer training procedure, making 

it highly attractive for implementation 

on parallel processing  
platforms. Nodes 
independently characterize patterns 

through the use of a belief state con-

struct, which is incrementally updated as 

the hierarchy is presented with data.
This rule is comprised of two con-
structs: one representing how likely sys-

tem states are for segments of the 

observation, 
P(observation
|state), and 
another representing how likely state to 

state transitions are given feedback from 

above, 
P(subsequent state|state,feedback
). 
The first construct is unsupervised and 

driven purely by observations, while the 

second, modulating the first, embeds the 

dynamics in the pattern observations. 

Incremental clustering is carefully 

applied to estimate the observation dis-

tribution, while state transitions are esti-

mated based on frequency. It is argued 

that the value of the scheme lies in its 

simplicity and repetitive structure, facili-

tating multi-modal representations and 

straightforward training. 
Table 1 provides a brief comparison 
summary of the mainstream deep 

machine learning approaches described 

in this paper.
V. Deep Learning Applications
There have been several studies demon-

strating the effectiveness of deep learning 

methods in a variety of application 

domains. In addition to the MNIST 

handwriting challenge [27], there are 

applications in face detection [10] [51], 
speech recognition and detection [12], 
general object recognition [9], natural 

language processing [24], and robotics. 

The reality of data proliferation and 

abundance of multimodal sensory infor-

mation is admittedly a challenge and a 

recurring theme in many military as well 

as civilian applications, such as sophisti-

cated surveillance systems. Consequently, 

interest in deep machine learning has 

not been limited to academic research. 

Recently, the Defense Advanced 

Research Projects Agency (DARPA) has 

announced a research program exclu-

sively focused on deep learning [29]. 

Several private organizations, including 

Numenta [30] and Binatix [31], have 

focused their attention on commercial-

izing deep learning technologies with 

applications to broad domains. 
VI. The Road Ahead
Deep machine learning is an active area 

of research. There remains a great deal of 

work to be done in improving the learn-

ing process, where current focus is on 

lending fertile ideas from other areas of 

machine learning, specifically in the con-

text of dimensionality reduction. One 

example includes recent work on sparse 

coding [57] where the inherent high 

dimensionality of data is reduced through 

the use of compressed sensing theory, 

allowing accurate representation of signals 

with very small numbers of basis vectors. 

Another example is semi-supervised 

manifold learning [58] where the dimen-

sionality of data is reduced by measuring 

the similarity between training data sam-
ples, then projecting these similarity mea-
surements to lower-dimensional spaces. 

In addition, further inspiration and tech-

niques may be found from evolutionary 

programming ap 
 
proaches [59, 60] where 
conceptually adaptive learning and core 

architectural changes can be learned with 

minimal engineering efforts.
Some of the core questions that 
necessitate immediate attention include: 

how well does a particular scheme scale 

with respect to the dimensionality of the 

input (which in images can be in the 
millions)? What is an efficient frame-

work for capturing both short and long-

term temporal dependencies? How can 

multimodal sensory information be most 

naturally fused within a given architec-

tural framework? What are the correct 

attention mechanisms that can be used 

to augment a given deep learning 

 technology so as to improve robustness 
and invariance to distorted or missing 

data? How well do the various solutions 

map to parallel processing platforms that 

facilitate processing speedup?
While deep learning has been suc-
cessfully applied to challenging pattern 

inference tasks, the goal of the field is far 

beyond task-specific applications. This 

scope may make the comparison of vari-

ous methodologies increasingly complex 

and will likely necessitate a collaborative 

effort by the research community to 

address. It should also be noted that, 

despite the great prospect offered by 

deep learning technologies, some 

domain-specific tasks may not be directly 

improved by such schemes. An example 
TABLE I
 Summary of mainstream deep machine learning approaches.APPROACH 
(ABBREVIATION)
UNSUPERVISED 
PRE-TRAINING?
GENERATIVE VS. 
DISCRIMINA
TIVENOTES
CONVOLUTIONAL 
NEURAL NETWORKS (CNN
S)NODISCRIMINA
TIVEUTILIZES SPATIAL/TEMPORAL RELATIONS
HIPS TO 
REDUCE LEA
RNING REQUIREM
ENTSDEEP BELIEF NETWORKS (DBN
S)HELPFULGENERA
TIVEMULTI-LAYERED REC
URRENT NEURAL NETWORK 
TRAINED WITH ENERGY 
MINIMIZING ME
THODS STACKED (DENOISING) 
AUTO-ENCODERS
HELPFULDISCRIMINA
TIVE (DENOISING 
ENCODER MAPS TO 

GENERATIVE MODEL)
STACKED NEURAL NETWORKS THAT LEARN COM-
PRESSED ENCODINGS THROUGH 

RECONSTRUCTION ERROR
HIERARCHICAL TEMPORAL 
MEMORY
NOGENERATIVEHIERARCHY OF ALTERNATING SPATIAL RECOGNITION 
AND TEMPORAL INFER
ENCE LAYERS WITH SUPERVISED 
LEARNING METHOD AT TOP LAYER
DEEP SPATIOTEMPORAL 
INFER
ENCE NETWORK (DESTIN)
NODISCRIMINA
TIVEHIERARCHY OF UNSUPERVISED SPATIAL-TEMPORAL 
CLUSTERING UNITS WITH BAYESIAN STATE-TO-STATE 

TRANSITIONS AND TOP-DOWN FEEDBACK
Authorized licensed use limited to: QUAID E AZAM UNIVERSITY. Downloaded on November 30,2020 at 04:54:03 UTC from IEEE Xplore.  Restrictions apply. 18    IEEE COMPUTATIONAL INTELLIGENCE MAGAZINE | NOVEMBER 2010
is  identifying and reading the routing 
numbers at the bottom of bank checks. 
 Though these digits are human readable, 

they are comprised of restricted character 

sets which specialized readers can recog-

nize flawlessly at very high data rates 

[49]. Similarly, iris recognition is not a 

task that humans generally perform; 

indeed, without training, one iris looks 

very similar to another to the untrained 

eye, yet engineered systems can produce 

matches between candidate iris images 

and an image database with high preci-

sion and accuracy to serve as a unique 

identifier [50]. Finally, recent develop-

ments in facial recognition [51] show 

equivalent performance relative to 

humans in their ability to match query 

images against large numbers of candi-

dates, potentially matching far more than 

most humans can recall [52]. Neverthe-

less, these remain highly specific cases 

and are the result of lengthy feature engi-

neering optimization processes (as well as 

years of research) that do not map to 

other, more general applications. Fur-

thermore, deep learning platforms can 

also benefit from engineered features 

while learning more complex represen-

tations which engineered systems typi-

cally lack.Despite the myriad of open research 
issues and the fact that the field is still in 

its infancy, it is abundantly clear that 

advancements made with respect to 

developing deep machine learning sys-

tems will undoubtedly shape the future 

of machine learning and artificial intelli-

gence systems in general. 
References[1] R. Bellman, 
Dynamic Programming
. Princeton, NJ: 
Princeton Univ. Press, 1957.
[2] R. Duda, P. Hart, and D. Stork, 
Pattern Recognition
, 2nd ed. New York: Wiley-Interscience, 2000. 
[3] T. Lee and D. Mumford, Hierarchical Bayesian in-
ference in the visual cortex, 
J. Opt. Soc. Amer.
, vol. 20, 
pt. 7, pp. 14341448, 2003.
[4] T. Lee, D. Mumford, R. Romero, and V. Lamme, 
The role of the primary visual cortex in higher level vi-

sion, 
Vision Res.
, vol. 38, pp. 24292454, 1998. 
[5] G. Wallis and H. Blthoff, Learning to recognize 
objects, 
Trends Cogn. Sci.
, vol. 3, no. 1, pp. 2331, 1999.
[6] G. Wallis and E. Rolls, Invariant face and object rec-
ognition in the visual system, 
Prog. Neurobiol.
, vol. 51, 
pp. 167194, 1997. 
[7] Y. Bengio, Learning deep architectures for AI, 
Found. Trends Mach. Learn.
, vol. 2, no. 1, pp. 1127, 2009. 
[8] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, 
Gradient-based learning applied to document recogni-

tion, 
Proc. IEEE
, vol. 86, no. 11, pp. 22782324, 1998.
[9] F.-J. Huang and Y. LeCun, Large-scale learning with 
SVM and convolutional nets for generic object categori-

zation, in 
Proc. Computer Vision and Pattern Recognition 
Conf. (CVPR06)
, 2006. 
[10] B. Kwolek, Face detection using convolutional neu-

ral networks and Gabor filters, in 
Lecture Notes in Com-
puter Science
, vol. 3696. 2005, p. 551. 
[11] F. H. C. Tivive and A. Bouzerdoum, A new class 

of convolutional neural networks (SICoNNets) and their 

application of face detection, in 
Proc. Int. Joint Conf. Neu-
ral Networks
, 2003, vol. 3, pp. 21572162.
[12] S. Sukittanon, A. C. Surendran, J. C. Platt, and C. 

J. C. Burges, Convolutional networks for speech detec-

tion, 
Interspeech
, pp. 10771080, 2004. 
[13] Y.-N. Chen, C.-C. Han, C.-T. Wang, B.-S. Jeng, 

and K.-C. Fan, The application of a convolution neu-

ral network on face and license plate detection, in 
Proc. 
18th Int. Conf. Pattern Recognition (ICPR06)
, 2006, pp. 
552555.

[14] G. E. Hinton, S. Osindero, and Y. Teh, A fast learn-

ing algorithm for deep belief nets, 
Neural Comput.
, vol. 
18, pp. 15271554, 2006. 

[15] G. E. Hinton, Training products of experts by min-

imizing contrastive divergence, 
Neural Comput.
, vol. 14, 
pp. 17711800, 2002. 

[16] G. E. Hinton and R. R. Salakhutdinov, Reducing 

the dimensionality of data with neural networks, 
Science
, vol. 313, no. 5786, pp. 504507, 2006.

[17] Y. Bengio, P. Lamblin, D. Popovici, and H. Laro-

chelle, Greedy layer-wise training of deep networks, 

in 
Advances in Neural Information Processing Systems 19 
(NIPS06)
. 2007, pp. 153160. 
[18] M. Ranzato, F. J. Huang, Y. Boureau, and Y. LeCun, 

Unsupervised learning of invariant feature hierarchies 

with applications to object recognition, in 
Proc. Com-
puter Vision and Pattern Recognition Conf.
, 2007. 
[19] H. Larochelle, D. Erhan, A. Courville, J. Bergstra, 

and Y. Bengio, An empirical evaluation of deep archi-

tectures on problems with many factors of variation, in 

Proc. 24th Int. Conf. Machine Learning (ICML07)
, 2007, 
pp. 473480.

[20] H. Lee, R. Grosse, R. Ranganath, and A. Ng, Con-

volutional deep belief networks for scalable unsupervised 

learning of hierarchical representations, in 
Proc. 26th Int. 
Conf. Machine Learning
, 2009, pp. 609616.
[21] P. Vincent, H. Larochelle, Y. Bengio, and P.-A. 

Manzagol, Extracting and composing robust features 

with denoising autoencoders, in 
Proc. 25th Int. Conf. Ma-
chine Learning (ICML08)
, 2008, pp. 10961103.
[22] I. Sutskever and G. Hinton, Learning multilevel dis-

tributed representations for high-dimensional sequences, in 

Proc. 11th Int. Conf. Artificial Intelligence and Statistics
, 2007. 
[23] A. Lockett and R. Miikkulainen, Temporal convo-

lution machines for sequence learning, Dept. Comput. 

Sci., Univ. Texas, Austin, Tech. Rep. AI-09-04, 2009.

[24] H. Lee, Y. Largman, P. Pham, and A. Ng, Unsuper-

vised feature learning for audio classification using con-

volutional deep belief networks, in 
Advances in Neural 
Information Processing Systems 22 (NIPS09),
 2009. 
[25] H. Mobahi, R. Collobert, and J. Weston, Deep 

learning from temporal coherence in video, in 
Proc. 26th 
Annu. Int. Conf. Machine Learning
, 2009, pp. 737744.
[26] I. Arel, D. Rose, and B. Coop, DeSTIN: A deep 

learning architecture with application to high-dimension-

al robust pattern recognition, in 
Proc. 2008 AAAI Work-
shop Biologically Inspired Cognitive Architectures (BICA)
. [27] The MNIST database of handwritten digits [On-

line]. Available: http://yann.lecun.com/exdb/mnist/ 

[28] Caltech 101 dataset [Online]. Available: http://

www.vision.caltech.edu/Image_Datasets/Caltech101/ 

[29] http://www.darpa.mil/IPTO/solicit/baa/BAA09-

40_PIP.pdf 

[30] http://www.numenta.com 

[31] http://www.binatix.com 

[32] T. Serre, L. Wolf, S. Bileschi, M. Riesenhuber, and 

T. Poggio, Robust object recognition with cortex-like 

mechanisms, 
IEEE Trans. Pattern Anal. Machine Intell.
, vol. 29, no. 3, pp. 411426, 2007.

[33] D. George, How the brain might work: A hierar-

chical and temporal model for learning and recognition, 

Ph.D. dissertation, Stanford Univ., Stanford, CA, 2008.

[34] T. Dean, G. Carroll, and R. Washington, On the 

prospects for building a working model of the visual cor-
tex, in 
Proc. Nat. Conf. Artificial Intelligence
, 2007, vol. 
22, p. 1597.
[35] T. Dean, A computational model of the cerebral 

cortex, in 
Proc. Nat. Conf. Artificial Intelligence
, 2005, vol. 
20, pp. 938943.

[36] T. S. Lee and D. Mumford, Hierarchical Bayesian 

inference in the visual cortex, 
J. Opt. Soc. Amer. A
, vol. 
20, no. 7, pp. 14341448, 2003. 

[37] M. Szarvas, U. Sakai, and J. Ogata, Real-time 

pedestrian detection using LIDAR and convolutional 

neural networks, in 
Proc. 2006 IEEE Intelligent Vehicles 
Symp.
, pp. 213218.
[38] P. Y. Simard, D. Steinkraus, and J. C. Platt, Best 

practices for convolutional neural networks applied to vi-

sual document analysis, in 
Proc. 7th Int. Conf. Document 
Analysis and Recognition
, 2003, pp. 958963.
[39] J. Hawkins and S. Blakeslee, 
On Intelligence. Times 
Books
, Oct. 2004. 
[40] K. Fukushima, Neocognitron for handwritten digit 

recognition, 
Neurocomputing
, vol. 51, pp. 161180, 2003. 
[41] K. Fukushima, Restoring partly occluded patterns: 

A neural network model, 
Neural Netw.
, vol. 18, no. 1, 
pp. 3343, 2005.

[42] D. Marr, 
Vision: A Computational Investigation into the 
Human Representation and Processing of Visual Information
. W. H. Freeman, 1983. 

[43] K. Fukushima, Neocognitron: A self-organizing 

neural network model for a mechanism of pattern recog-

nition unaffected by shift in position, 
Biol. Cybern.
, vol. 
36, no. 4, pp. 193202, 1980.

[44] D. H. Hubel and T. N. Wiesel, Receptive fields, bin-

ocular interaction and functional architecture in the cats 

visual cortex, 
J. Physiol.
, vol. 160, pp. 106154, 1962. 
[45] M. Riesenhuber and T. Poggio, Hierarchical mod-

els of object recognition in cortex, 
Nat. Neurosci.
, vol. 2, 
no. 11, pp. 10191025, 1999.

[46] J. Pearl, 
Probabilistic Reasoning in Intelligent Systems: 
Networks of Plausible Inference
. San Mateo, CA: Morgan 
Kaufmann, 1988.

[47] J. W. Miller and P. H. Lommel, Biometric sensory 

abstraction using hierarchical quilted self-organizing 

maps, 
Proc. SPIE
, vol. 6384, 2006. 
[48] S. Behnke, 
Hierarchical Neural Networks for Image Inter-
pretation
. New York: Springer-Verlag, 2003.
[49] S. V. Rice, F. R. Jenkins, and T. A. Nartker, The 

fifth annual test of OCR accuracy, 
Information Sciences 
Res. Inst.
, Las Vegas, NV, TR-96-01, 1996.
[50] E. M. Newton and P. J. Phillips, Meta-analysis of 

third-party evaluations of iris recognition, 
IEEE Trans. 
Syst., Man, Cybern. A
, vol. 39, no. 1, pp. 411, 2009.
[51] M. Osadchy, Y. LeCun, and M. Miller, Synergistic 

face detection and pose estimation with energy-based mod-

els, 
J. Mach. Learn. Res.
, vol. 8, pp. 11971215, May 2007.
[52] A. Adler and M. Schuckers, Comparing human and 

automatic face recognition performance, 
IEEE Trans. 
Syst., Man, Cybern. B
, vol. 37, no. 5, pp. 12481255, 2007.
[53] A. Waibel, T. Hanazawa, G. Hinton, K. Shikano, 

and K. Lang, Phoneme recognition using time-delay 

neural networks, 
IEEE Trans. Acoust., Speech, Signal Pro-
cessing
, vol. 37, pp. 328339, 1989. 
[54] K. Lang, A. Waibel, and G. Hinton, A time-delay 

neural-network architecture for isolated word recogni-

tion, 
Neural Netw.
, vol. 3, no. 1, pp. 2344, 1990.
[55] R. Hadseel, A. Erkan, P. Sermanet, M. Scoffier, 

U. Muller, and Y. LeCun, Deep belief net learning in a 

long-range vision system for autonomous off-road driving, 

in 
Proc. Intelligent Robots and Systems
, 2008, pp. 628633.
[56] G. E. Hinton and R. R. Salakhutdinov, Reducing 

the dimensionality of data with neural networks, 
Science
, vol. 313, pp. 504507, 2006. 

[57] K. Kavukcuoglu, M. Ranzato, R. Fergus, and Y. Le-

Cun, Learning invariant features through topographic 

filter maps, in 
Proc. Int. Conf. Computer Vision and Pattern 
Recognition
, 2009. 
[58] J. Weston, F. Ratle, and R. Collobert, Deep learn-

ing via semi-supervised embedding, in 
Proc. 25th Int. 
Conf. Machine Learning
, 2008, pp. 11681175.
[59] K. A. DeJong, Evolving intelligent agents: A 50 

year quest, 
IEEE Comput. Intell. Mag.
, vol. 3, no. 1, pp. 
1217, 2008.

[60] X. Yao and M. Islam, Evolving artificial neural net-

work ensembles, 
IEEE Comput. Intell. Mag.
, vol. 2, no. 1, 
pp. 3142, 2008.
  Authorized licensed use limited to: QUAID E AZAM UNIVERSITY. Downloaded on November 30,2020 at 04:54:03 UTC from IEEE Xplore.  Restrictions apply. Privacy-Preserving Data Mining Rakesh Agrawal Ramakrishnan Srikant IBM Almaden Research Center 650 Harry Road, San Jose, CA 95120 Abstract A fruitful direction for future data mining research will be the development of techniques that incorporate privacy concerns. Specifically, we address the following question. Since the primary task in data mining is the development of models about aggregated data, can we develop accurate models without access to precise information in individual data records? We consider the concrete case of building a decision-tree classifier from tredning data in which the values of individual records have been perturbed. The resulting data records look very different from the original records and the distribution of data values is also very different from the original distribution. While it is not possible to accurately estimate original values in individual data records, we propose a-novel reconstruction procedure to accurately estimate the distribution of original data values. By using these reconstructed distributions, we are able to b~ld classifiers whose accuracy is comparable to the accuracy of classifiers built with the original data. 1 Introduction Explosive progress in networking, storage, and proces- sor technologies has led to the creation of ultra large databases that record unprecedented amount of trans- actional information. In tandem with this dramatic increase in digital data, concerns about informational privacy have emerged globally [Tim97] [Eco99] [eu998] [Off98]. Privacy issues are further exacerbated now that the World Wide Web makes it easy for the new data to be automatically collected and added to databases [HE98] [Wes98a] [Wes98b] [Wes99] [CRA99a] [Cra99b]. The concerns over massive collection of data are natu- rally extending to analytic tools applied to data. Data mining, with its promise to efficiently discover valuable, non-obvious information from large databases, is par- Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial edvent -age and that copies bear this notice and the ful| citation on the tirst page. To copy otherwise, to republish, to post on servers or to redistribute "to lists, requires prior specific permission and/or a fee. ACM SIGMOD 2000 Dallas, TX, USA  2000 ACM 1-58113-218-2/00/0005...$5,00 ticularly vulnerable to misuse [CM96] [The9S] [Off98] [ECB99]. A fruitful direction for future research in data mining will be the development of techniques that incorporate privacy concerns [Agr99]. Specifically, we address the following question. Since the primary task in data mining is the development of models about aggregated data, can we develop accurate models without access to precise information in individual data records? The underlying assumption is that a person will be willing to selectively divulge information in exchange of value such models can provide [Wes99]. Example of the value provided include filtering to weed out unwanted information, better search results with less effort, and automatic triggers [HS99]. A recent survey of web users [CRA99a] classified 17% of respondents as privacy fun- damentalists who will not provide data to a web site even if privacy protection measures are in place. How- ever, the concerns of 56% of respondents constituting the pragmatic majority were significantly reduced by the presence of privacy protection measures. The re- maining 27% were marginally concerned and generally willing to provide data to web sites, although they of- ten expressed a mild general concern about privacy. An- other recent survey of web users [Wes99] found that 86% of respondents believe that participation in information- for-benefits programs is a matter of individual privacy choice. A resounding 82% said that having a privacy policy would matter; only 14% said that was not impor- tant as long as they got benefit. Furthermore, people are not equally protective of every field in their data records [Wes99] [CRA99a]. Specifically, a person  may not divulge at all the values of certain fields;  may not mind giving true values of certain fields;  may be willing to give not true values but modified values of certain fields. Given a population that satisfies the above assump- tions, we address the concrete problem of building decision-tree classifiers [BFOS84] [Qui93] and show that that it is possible to develop accurate models while re- 439 specting users' privacy concerns. Classification is one the most used tasks in data mining. Decision-tree clas- sifters are relatively fast, yield comprehensible models, and obtain similar and sometimes better accuracy than other classification methods [MST94]. Related Work There has been extensive research in the area of statistical databases motivated by the de- sire to be able to provide statistical information (sum, count, average, maximum, minimum, pth percentile, etc.) without compromising sensitive information about individuals (see excellent surveys in [AW89] [Sho82].) The proposed techniques can be broadly classified into query restriction and data perturbation. The query re- striction family includes restricting the size of query result (e.g. [FelT2] [DDS79]), controlling the overlap amongst successive queries (e.g. [DJL79]), keeping au- dit trail of all answered queries and constantly check- ing for possible compromise (e.g. [CO82]), suppression of data cells of small size (e.g. [Cox80]), and cluster- ing entities into mutually exclusive atomic populations (e.g. [YC77]). The perturbation family includes swap- ping values between records (e.g. [Den82]), replacing the original database by a sample from the same distribu- tion (e.g. [LST83] [LCL85] [Rei84]), adding noise to the values in the database (e.g. [TYW84] [War65]), adding noise to the results of a query (e.g. [Bec80]), and sam- pling the result of a qu6ry (e.g. [DenS0]). There are neg- ative results showing that the proposed techniques can- not satisfy the conflicting objectives of providing high quality statistics and at the same time prevent exact or partial disclosure of individual information [AW89]. The statistical quality is measured in terms of bias, pre- cision, and consistency. Bias represents the difference between the unperturbed statistics and the expected value of its perturbed estimate. Precision refers to the variance of the estimators obtained by the users. Con- sistency represents the lack of contradictions and para- doxes. An exact disclosure occurs if by issuing one or more queries, a user is able to determine the exact value of a confidential attribute of an individual. A partial disclosure occurs if a user is able to obtain an estimator whose variance is below a given threshold. While we share with the statistical database liter- ature the goal of preventing disclosure of confidential information, obtaining high quality point estimates is not our goal. As we will see, it is sufficient for us to be able to reconstruct with sufficient accuracy the orig- inal distributions of the values of the confidential at- tributes. We adopt from the statistics literature two methods that a person may use in our system to mod- ify the value of a field [CS76]: Value-Class Membership. Partition the values into a set of disjoint, mutually-exhaustive classes and return the class into which the true value xi falls. * Value Distortion. Return a value xi + r instead of zi where r is a random value drawn from some distribution. We discuss further these methods and the level of privacy they provide in the next section. We do not use value dissociation, the third method proposed in [CS76]. In this method, a value returned for a field of a record is a tru e value, but from the same field in some other record. Interestingly, a recent proposal [ECB99] to construct perturbed training sets is based on this method. Our hesitation with this approach is that it is a global method and requires knowledge of values in other records. The problem of reconstructing original distribution from a given distribution can be viewed in the general framework of inverse problems [EHN96]. In [FJS97], it was shown that for smooth enough distributions (e.g. slowly varying time signals), it is possible to to fully recover original distribution from non-overlapping, contiguous partial sums. Such partial sums of true values are not available to us. We cannot make a priori assumptions about the original distribution; we only know the distribution used in randomizing values of an attribute. There is rich query optimization literature on estimating attribute distributions from partial information [BDF+97]. In the OLAP literature, there is work on approximating queries on sub-cubes from higher-level aggregations (e.g. [BS97]). However, these works did not have to cope with information that has been intentionally distorted. Closely related, but orthogonal to our work, is the extensive literature on access control and security (e.g. [Din78] [STg0] [Opp97] [RG98]). Whenever sensitive information is exchanged, it must be transmitted over a secure channel and stored securely. For the purposes of this paper, we assume that appropriate access controls and security procedures are in place and effective in preventing unauthorized access to the system. Other relevant work includes efforts to create tools and standards that provide platform for implementing a system such as ours (e.g. [Wor] [Ben99] [GWB97] [Cra99b] [AC99] [LM99] [LEW99]). Paper Organization We discuss privacy-preserving methods in Section 2. We also introduce a quanti- tative measure to evaluate the amount of privacy of- fered by a method and evaluate the proposed methods against this measure. In Section 3, we present our re- construction procedure for reconstructing the original data distribution given a perturbed distribution. We also present some empirical evidence of the efficacy of the reconstruction procedure. Section 4 describes tech- niques for building decision-tree classifiers from per- turbed training data using our reconstruction proce- dure. We present an experimental evaluation of the 440 accuracy of these techniques in Section 5. We conclude with a summary and directions for future work in Sec- tion 6. We only consider numeric attributes; in Section 6, we briefly describe how we propose to extend this work to include categorical attributes. We focus on attributes for which the users are willing to provide perturbed values. If there is an attribute for which users are not willing to provide even the perturbed value, we simply ignore the attribute. If only some users do not provide the value, the training data is treated as containing records with missing values for which effective techniques exist in the literature [BFOS84] [Qui93]. 2 Privacy-Preserving Methods Our basic approach to preserving privacy is to let users provide a modified value for sensitive attributes. The modified value may be generated using custom code, a browser plug-in, or extensions to products such as Microsoft's Passport (http://www.passport.com) or Novell's DigitalMe (http://www.digitalme.com). We consider two methods for modifying values [CS76]: Value-Class Membership In this method, the val- ues for an attribute are partitioned into a set of disjoint, mutually-exclusive classes. We consider the special case of dlscretlzatlon in which values for an attribute are discretized into intervals. All intervals need not be of equal width. For example, salary may be discretized into 10K intervals for lower values and 50K intervals for higher values. Instead of a true attribute value, the user provides the interval in which the value lies. Dis- cretization is the method used most often for hiding individual values. Value Distortion Return a value zi + r instead of zi where r is a random value drawn from some distribution. We consider two random distributions:  Uniform: The random variable has a uniform distribution, between [-a, + a]. The mean of the random variable is 0.  Gaussian: The random variable has a normal distribution, with mean p = 0 and standard deviation o" [Fis63]. We fix the perturbation of an entity. Thus, it is not possible for snoopers to improve the estimates of the value of a field in a record by repeating queries [AW89]. 2.1 Quantifying Privacy For quantifying privacy provided by a method, we use a measure based on how closely the original values of a modified attribute can be estimated. If it can be Confidence 50% i 95% 99.9% I)iS'cretization ' 0.5 x W '! 0:95x W 0.999'x W Uniform 0.5  2a i 0.95  2a 0.999  2a Gaussian 1.34 x ~r : 3.92 x a 6.8 x o" Table 1: Privacy Metrics estimated with c% confidence that a value  lies in the interval [xt, ~2], then the interval width (x2 - ~1) defines the amount of privacy at c% confidence level. Table 1 shows the privacy offered by the different methods using this metric. We have assumed that the intervals are of equal width W in Discretization. Clearly, for 2a -- W, Uniform and Discretization provide the same amount of privacy. As o~ increases, privacy also increases. To keep up with Uniform, Discretization will have to increase the interval width, and hence reduce the number of intervals. Note that we are interested in very high privacy. (We use 25%, 50%, 100% and 200% of range of values of an attribute in our experiments.) Hence Discretization will lead to poor model accuracy compared to Uniform since all the values in a interval are modified to the same value. Gaussian provides significantly more privacy at higher confidence levels compared to the other two methods. We, therefore, focus on the two value distortion methods in the rest of the paper. 3 Reconstructing The Original Distribution For the concept of using value distortion to protect privacy to be useful, we need to be able to reconstruct the original data distribution from the randomized data. Note that we reconstruct distributions, not values in individual records. We view the n original data values ~1, ~r2,  ., x,~ of a one-dimensional distribution as reMizations of n inde- pendent identically distributed (rid) random variables X1, X~,..., Xn, each with the same distribution as the random variable X. To hide these data values, n in- dependent random variables Y1, Y2,..., Y,~ have been used, each with the same distribution as a different ran- dom variable Y. Given zlYl, z2+Y~,  ., z,~+Y,~ (where Yi is the realization of Yi) and the cumulative distribu- tion function Fy for Y, we would like to estimate the cumulative distribution function Fx for X. Reconstruction Problem Given a cumulative dis- tribution Fy and the realizations of n lid random sam- ples Xt + Yz, X2+ Y2, . . .,Xn + Yn, estimate Fx. Let the value of Xi+Y~ be w~(= x~+yi). Note 441 that we do not have the individual values zi and yi, only their sum. We can use Bayes' rule [Fis63] to estimate the posterior distribution function Fir: (given that Xi+Y, = wt) for Xi, assuming we know the density functions Ix and/r for X and Y respectively. E~,(a) =f /x,(z I x~+rl = w~) dz .:x,+r, (~0~ I x, = ~) Ix, (~) d~ f X x+Y1 (W,) (using Bayes' rule for density functions) ff /x,+r, (~ I x, ~ ~_) /xl (~____) oo ,E~o h,+r, (w~ I X~ = ~') Ix, (~') d~' (expanding the denominator) f_aoo fx,+Y~ (wl IX1 = z) .fx, (z) dz I_~ h,+r, (w, I x~ = ~) h, (~) dz (inner integral is independent of outer) :-?o~ h, (~,-~) :x, (z) d~ f-moo .,fr, (w~ -z) fx, (z) dz (since Yi is independent of Xi) f:oo h(~-~) h(~) d~ (since/x, -~ ?x and/y, _= fr) dz To estimate the posterior distribution function Fir given zi +yi,x~+y2,...,zn+yn, we average the distribution functions for each of the Xi. Fjr(a)=~Fjc,=~f.~ofr(w, z)fx(z)dz i=1 i=l The corresponding posterior density function, f~ is obtained by differentiating FJ: 1 ~ h(w, - a) h(a) -- oo ..... 5,(a) = ,~ :2~ h (w, - ~1 :x (~1 d~ i=i (1) Given a sufficiently large number of samples, we expect f~ in the above equation to be very close to the real density function fx. However, although we know fr, 1 we do not know fx. Hence we use the uniform distribution as the initial estimate f~, and iteratively refine this estimate by applying Equation I. This algorithm is sketched out in Figure 1. Using Partitioning to Speed Computation As- sume a partitioning of the domain (of the data values) into intervals. We make two approximations: 1For example, if Y is the standard normal, ]y(z) = (llx/~2~))e-='/~. (1) /~ := Uniform distribution (2) j := 0//Iteration number repeai~ (4) i :=j+i until (stopping criterion met) Figure 1: Reconstruction Algorithm * We approximate the distance between z and wi (or between a and wi) with the distance between the mid-points of the intervals in which they lie, and  We approximate the density function fx (a) with the average of the density function over the intervM in which a lies. Let I(z) denote the interval in which x lies, ra(Iv) the mid-point of interval Ip, and re(z) the mid- point of interval I(z). Let &(Iv) be the average value of the density function over the interval Iv, i.e. fx(lv) = lip Lx(z)dz / fI,, dz. By applying these two approximations to Equation 1, we get 1 ~ h(rn(wi)-m(a))h(l(a)) f~(a) = ~ :-~o/Y (ra(wi)--m(z)) fx (l(z)) dz i=1 Let Iv, p = 1...k denote the k intervals, and L v the width of interval I v . We can replace the integral in the denominator with a sum, since re(z) and fx(I(z)) do not change within an interval: 1 ~. /y(m(w,) - re(a))fx(I(~)) /~(a) = g ~ ~ ....... = Et=i fr(ra(w,) - m(I~)) fx(It) Lt (2) We now compute the average value of the posterior density function over the interval Ip. /~(z)dz / r, t ~ h(m(~o,)-m(~))fx(I(~))d~ /Z ,=, ' (substituting Equation 2) E~=,/y(m(~0,)-,~(h)) fx (.~) p i:, (since I(z) = Iv within Ip) _ 1_~ fy(m(wi)-m(Ip))fx(Ip) - n,=t E~=ih(rn(w,)-rn(It))fx(It)L, (since fl, dz = Lp) 442 1000 8OO 6O0 QE "5 200 0 -1 1000 80O to ~2 oo ~600 gE o ,00 2OO 0 -1 (a) Plateau Original Randomized Reconstructed .-~ ..... -0.5 0 0.5 1 1.5 Attribute Value Gaussian 1200 lOOO i 8OO a: "5 600 =e4oo Z 200 0 2 -1 (b) Triangles ,., ;- Original Randomized ...... Recbnslructed ........ -0.5 0 0.5 1 1.5 2 Attribute Value (c) Plateau Original Randomized ..... Reconstructed ,-= ..... / i \ -0.5 0 0,5 1 1,5 Attribute Value Uniform 1200 1000 to ~600 e=400 Z 200 0 2 -1 (d) Triangles , , . .... . " Original Randomized ...... Reconstructed -,~ ..... -0.5 0.5 1 1.5 2 Attribute Value Figure 2: Reconstructing the Original Distribution Let N(Ip) be the number of points that lie in interval Ip (i.e. number of elements in the set {wiIwi E Ip}. Since rn(wi) is the same for points that lie within the same interval, /~ (i,) = fy(m(l,)-m(Ip)) fx(Ip) ! ~2N(I,1  ~ ,, n ,=t Zt=, fy(rn(l,)-rn(h)) fx(It)Lt Finally, let Pr~(X E Ip) be the posterior probability of X belonging to interval Ip, i.e. Pr'(X E Ip) =/)(Ip) x Lp. Multiplying both sides of the above equation by Lp, and using Pr(X e Ip) = fx(Ip) x Lp, we get: Pr'(x e i,) = (3) ~ ~ g(I,) xk kIY(m(I') - m(/,)) Pr(X e I,) n ,=~ E~=~/y(m(/,) - m(/,)) Pr(X e Z~) We can now substitute Equation 3 in step 3 of the algorithm (Figure 1), and compute step 3 in O(m 2) time. 2 ~A naive implementation of Equation 3 will lead to O(m s) time. However, since the denominator is independent of Ip, we can re-use the results of that computation to get O(m 2) time. Stopping Criterion With omniscience, we would stop when the reconstructed distribution was statisti- cally the same as the original distribution (using, say, the X 2 goodness-of-fit test [Cra46]). An alternative is to compare the observed randomized distribution with the result of randomizing the current estimate of the origi- nal distribution, and stop when these two distributions are statistically the same. The intuition is that if these two distributions are close to each other, we expect our estimate of the original distribution to also be close to the real distribution. Unfortunately, we found empir- ically that the difference between the two randomized distributions is not a reliable indicator of the difference between the original and reconstructed distributions. Instead, we compare successive estimates of the original distribution, and stop when the difference between successive estimates becomes very small (1% of the threshold of the X 2 test in our implementation). Empirical Evaluation Two original distributions, "plateau" and "triangles", are shown by the "Original" line in Figures 2(a) and (b) respectively. We add a Gaussian random variable with mean 0 and standard 443 ~. 2~ ~ ..~ . =::.: ~ .o_! ~ .....  ;, ...... ,~ : ~. ~i ': ~.~..~.:~.~ ....... .,~ : . .  rid 0 1 3 4 5 Age Salary Credit Risk 23 50K High 17 30K ' High 43 40K' ! High 68 50K I L ~. .... 32 70K i Low 20 20K' I High" (a) Training Set Age < 25 //C~alary < 50K High Low (b) Decision Tree Figure 3: Credit Risk Example deviation of 0.25 to each point in the distribution. Thus a point with value, say, 0.25 has a 95% chance of being mapped to a value between -0.26 and 0.74, and a 99.9% chance of being mapped to a value between - 0.6 and 1.1. The effect of this randomization is shown by the "Randomized" line. We apply the algorithm (with partitioning) in Figure 1, with a partition width of 0.05. The results are shown by the "Reconstructed" line. Notice that we are able to pick out the original shape of the distribution even though the randomized version looks nothing like the original. Figures 2(c) and (d) show that adding an uniform, discrete random variable between 0.5 and -0.5 to each point gives similar results. 4 Decision-Tree Classification over Randomized Data " 4.1 Background We begin with a brief review of decision tree classifi- cation, adapted from [MAR96] [SAM96]. A decision tree [BFOS84] [Qui93] is a class discriminator that re- cursively partitions the training set until each parti- tion consists entirely or dominantly of examples from the same class. Each non-leaf node of the tree con- tains a split point which is a test on one or more at- tributes and determines how the data is partitioned. Figure 3(b) shows a sample decision-tree classifier based on the training shown in Figure 3a. (Age < 25) and (Salary < 50K) are two split points that partition the records into High and Low credit risk classes. The de- cision tree can be used to screen future applicants by classifying them into the High or Low risk categories. A decision tree classifier is developed in two phases: a growth phase and a prune phase. In the growth Partition(Data S) begin (1) if (most points in S are of the same class) then (2) return; (3) for each attribute A do (4) evaluate splits on attribute A; (5) Use best split to partition S into $1 and $2; ~'(6) Partition(S1); (7) Partition(S2); end Initial call: Partition(TrainingData) Figure 4: The tree-growth phase phase, the tree is built by recursively partitioning the data until each partition contains members belonging to the same class. Once the tree has been fully grown, it is pruned in the second phase to generalize the tree by removing dependence on statistical noise or variation that may be particular only to the training data. Figure 4 shows the algorithm for the growth phase. While growing the tree, the goal at each node is to determine the split point that "best" divides the training records belonging to that node. We use the gini index [BFOS84] to determine the goodness of a split. For a data set S containing examples from rn classes, gini(S) = 1 - ~ p~ where pj is the relative frequency of class j in S. If a split divides S into two subsets $1 and S~, the index of the divided data ginisvti~(S) is given by gini,vti~(S ) = ~gini(S1) + n~n gini(S2 ). Note that calculating this index requires only the distribution of the class values in each of the partitions. 4.2 Training Using Randomized Data To induce decision trees using perturbed training data, we need to modify two key operations in the tree-growth phase (Figure 4):  Determining a split point (step 4).  Partitioning the data (step 5). We also need to resolve choices with respect to recon- structing original distribution:  Should we do a global reconstruction using the whole data or should we first partition the data by class and reconstruct separately for each class?  Should we do reconstruction once at the root node or do reconstruction at every node? We discuss below each of these issues. For pruning phase based on the Minimum Description Length principle [MAR96], no modification is needed. 444 Determining split points Since we partition the do- main into intervals while reconstructing the distribu- tion, the candidate split points are the interval bound- aries. (In the standard algorithm, every mid-point be- tween any two consecutive attribute values is a candi- date split point.) For each candidate split-point, we use the statistics from the reconstructed distribution to compute gini index. Partitioning the Data The reconstruction proce- dure gives us an estimate of the number of points in each interval. Let 11,-..Ira be the m intervals, and N(Ip) be the estimated number of points in interval Ip. We as- sociate each data value with an interval by sorting the values, and assigning the N(I1) lowest values to inter- val/1, and so on. a If the split occurs at the boundary of interval Ip-1 and Ip, then the points associated with intervMs I1,..., Ip-1 go to $1, and the points associ- ated with intervals Ip,..., Im go to $2. We retain this association between points and intervals in case there is a split on the same attribute (at a different split-point) lower in the tree. Reconstructing the Original Distribution We consider three different algorithms that differ in when and how distributions are reconstructed:  Global: Reconstruct the distribution for each attribute once at the beginning using the complete perturbed training data. Induce decision tree using the reconstructed data.  ByClass: For each attribute, first split the training data by class, then reconstruct the distributions separately for each class. Induce decision tree using the reconstructed data.  Local: As in ByClass, for each attribute, split the training data by class and reconstruct distributions separately for each class. However, instead of doing reconstruction only once, reconstruction is done at each node (i.e. just before step 4 in Figure 4). To avoid over-fitting, reconstruction is stopped after the number of records belonging to a node become small. A final detail regarding reconstruction concerns the number of intervals into which the domain of an attribute is partitioned. We use a heuristic to determine the number of intervals, m. We choose m such that there are an average of 100 points per interval. We then bound m to be between 10 and 100 intervals i.e. if rn < 10, rn is set to 10, etc. Clearly, Local is the most expensive algorithm in terms of execution time. Global is the cheapest 8The interval associated with a data value should not be considered an estimator of the original value of that data value. algorithm. ByClass falls in between. However, it is closer to Global than Local since reconstruction is done in ByClass only at the root node, whereas it is repeated at each node in Local. We empirically evaluate the classification accuracy characteristics of these algorithms in the next section. 4.3 Deployment In many applications, the goal of building a classifi- cation model is to develop an understanding of differ- ent classes in the target population. The techniques just described directly apply to such applications. In other applications, a classification model is used for pre- dicting the class of a new object without a preassigned class label. For this prediction to be accurate, although we have been able to build an accurate model using randomized data, the application needs access to non- perturbed data which the user is not willing to disclose. The solution to this dilemma is to structure the applica- tion such that the classification model is shipped to the user and applied there. For instance, if the classifica- tion model is being used to filter information relevant to a user, the classifier may be first applied on the client side over the original data and the information to be presented is filtered using the results of classification. 5 Experimental Results 5.1 Methodology We compare the classification accuracy of Global, ByClass, and Local algorithms against each other and with respect to the following benchmarks:  Original, the result of inducing the classifier on unperturbed trMning data without randomization.  Randomized, the result of inducing the classifier on perturbed data but without making any correc- tions for randomization. Clearly, we want to come as close to Original in accuracy as possible. The accuracy gain over Randomized reflects the advantage of reconstruction. We used the synthetic data generator from [AGI+92] for our experiments. We used a training set of 100,000 records and a test set of 5,000 records, equally split between the two classes. Table 2 describes the nine attributes, and Table 3 summarizes the five classification functions. These functions vary from having quite simple decision surface (Function 1) to complex non-linear surfaces (Functions 4 and 5). Functions 2 and 3 may look easy, but are quite difficult. The distribution of values on age are identical for both classes, unless the classifier first splits on salary. Further, the classifier has to exactly find five split-points on salary: 25, 50, 75, 100 and 125 to perfectly classify the data. The width of each of these intervals is less 445 Group A Group B Function 1 (age < 40) V ((60 _< age) otherwise Function 2 ((age < 40) A (50K < salary _< 100K)) V otherwise ((40 ~ age < 60) A (75K < salary > 125K)) V ((age > 60) A (25K < salary < 75K)) Function 3 ((age < 40) A (((elevel E [0..1]) A (25K < salary < 75K)) V otherwise ((elevel e [2..3]) ^ (50K < salary < 100K)))) V ((40 _< age < 60) A (((elevei E [1..3]) A (50K _< salary < 100K)) V (((elevei = 4)) ^ (75K < sa|ary _< 12SK)))) V ((age > 60) A (((elevel E [2..4]) A (5OK < salary < 100g)) V ((elevei = 1)) A (25K < salary < 75K)))) Function 4 (0.67 x (salary + commission) - 0.2 x loan - 10K) > 0 otherwise Function 5 (0.67  (salary + commission) - 0.2 x loan + 0.2  equity - 10K) > 0 otherwise where equity = 0.1  hvalue  max(hyears - 20, 0) Table 3: Description of Functions Attribute Description salary commission age elevel car zipcode hvalue hyears loan uniformly distributed from 20K to 150K salary > 75K =~ commission = 0 else uniformly distributed from 10K to 75K uniformly distributed from 20 to 80 uniformly chosen from 0 to 4 uniformly, chosen from 1 to 20 uniformly chosen from 9 zipcodes uniformly distributed from k  50K to k x 150K, where k E {0... 9} depends on zipcode uniformly distributed from 1 to 30 uniformly distributed from 0 to 500K Table 2: Attribute Descriptions than 20% of the range of the attribute. Function 2 also contains embedded XORs which are known to be troublesome for decision tree classifiers. Perturbed training data is generated using both Uniform and Gaussian methods (Section 2). All accuracy results involving randomization were averaged over 10 runs. We experimented with large values for the amount of desired privacy: ranging from 25% to 200% of the range of values of an attribute. The confidence threshold for the privacy level is taken to be 95% in all our experiments. Recall that if it can be estimated with 95% confidence that a value x lies in the interval [~i, z2], then the interval width (z2- xz) defines the amount of privacy at 95% confidence level. For example, at 50% privacy, Salary cannot be estimated (with 95% confidence) any closer than an interval of width 65K, which is half the entire range for Salary. Similarly, at 100% privacy, Age cannot be estimated (with 95% confidence) any closer than an interval of width 60, which is the entire range for Age. 5.2 Comparing the Classification Algorithms Figure 5 shows the accuracy of the algorithms for Uniform and Gaussian perturbations, for privacy levels of 25% and 100%. The x-axis shows the five functions from Table 3, and the y-axis the accuracy. Overall, the ByClass and Local algorithms do re- markably well at 25% and 50% privacy, with accuracy numbers very close to those on the original data. Even at as high as 100% privacy, the algorithms are within 5% (absolute) of the original accuracy for Functions 1, 4 and 5 and within 15% for Functions 2 and 3. The advan- tage of reconstruction can be seen from these graphs by comparing the accuracy of these algorithms with Ran- domized. Overall, the Global algorithm performs worse than ByClass and Local algorithms. The deficiency of Global is that it uses the same merged distribution for all the classes during reconstruction of the original distribution. It fares well on Functions 4 and 5, but the performance of even Randomized is quite close to Original on these functions. These functions have a diagonal decision surface, with equal number of points on each side of the diagonal surface. Hence addition of noise does not significantly affect the ability of the classifier to approximate this surface by hyper- rectangles. As we stated in the beginning of this section, though they might look easy, Functions 2 and 3 are quite difficult. The classifier has to find five sprit-points on salary and the width of each interval is 25K. Observe that the range over which the randomizing function spreads 95% of the values is more than 5 times the width of the splits at 100% privacy. Hence even small errors in reconstruction result in the split points being a little off, and accuracy drops. The poor accuracy of Original for Function 2 at 25% privacy may appear anomalous. The explanation lies in 446 Privacy Level = 25% Gaussian 100 , ... . , 95~ N 90 8 85 < 80 75 Fnl Original Local -+--- ByClass -~.-- Global ..... ..... Randomized -~ .... ! Fn2 t Fn3 Dataset t Fn4 100 95 90 o ~ 85 80 75 Fn5 Fnl Uniform "x J~ Original , Local -+--- ByClass -o-- Global  -.* ..... Randomized -- .... ! 1 ! Fn2 Fn3 Fn4 Dataset Fn5 Privacy Level = 100% Gaussian I .... %,. .~ "" ^. ,j, ~ 770 '~ ...... o o < Original Local -+--- 60 ByClass -~--- Global -* ...... Randomized ...... 5O Fnl t I I Fn2 Fn3 Fn4 Dataset Fn5 100 Uniform  /  70 ..................... .../"~" ........ ./'/ ............ ~~,.. i t ., Original  =' Local -+--- 60 ByClass -o--- ..,,. ..... Global "'-~ ..... ............... Randomized -~ .... 50 I I 1 Fnl Fn2 Fn3 Fn4 Fn5 Dataset o o O < Figure 5: Classification Accuracy there being a buried XOR in Function 2. When Original reaches the corresponding node, it stops because it does not find any split point that increases gini. However, due to the perturbation of data with randomization, the other algorithms find a "false" split point and proceed further to find the real split. 5.3 Varying Privacy Figure 6 shows the effect of varying the amount of privacy for the ByClass algorithm. (We omitted the graph for Function 4 since the results were almost identical to those for Function 5.) Similar results were obtained for the Local algorithm. The x-axis shows the privacy level, ranging from 10% to 200%, and the y-axis the accuracy of the algorithms. The legend ByClass(G) refers to ByClass with Gaussian, Random(U) refers to Randomized with Uniform, etc. Two important conclusions can be drawn from these graphs:  Although Uniform perturbation of original data results in a much large degradation of accuracy before correction compared to Gaussian, the effect of both distributions is quite comparable after correction.  The accuracy of the classifier developed using perturbed data, although not identical, comes fairly close to Original (i.e. accuracy obtained from using unperturbed data). 6 Conclusions and Future Work In this paper, we studied the technical feasibility of realizing privacy-preserving data mining. The basic premise was that the sensitive values in a user's record will be perturbed using a randomizing function so that they cannot be estimated with sufficient precision. Randomization can be done using Gaussian or Uniform perturbations. The question we addressed was whether, given a large number of users who do this perturbation, 447 100 90 o~ 8O o 70 o 60 50 0 80 o 70 o o Function 1 "C'"-~.. "'~----~ ..... + ........ :::: ........... k k "\\. \ "\ Original o\ ByClass(G) ........ ByClass(U) -e-~,. '~,.. Random(G) --* ..... Random(U) ...... ",., "~~~. I I I 50 100 150 200 Privacy Level (%) Function 3 100 :: : , : : ; ,p 9O "-.:~:::: " ............ :~ ......... I ~ .'--..~-.. _...~. .... - ...... :~:::::.:~::.: ............... Original ~ ' ByCtass(G) -+--- "x, ByClass(U) -~--- ''--, 60 Random(G) ,... ...... ", Random(U) ...... '".,., 50 ......... 0 50 1 O0 150 200 Privacy Level (%) 100 90 8O o L~ = 70 o o 60 50 0 100 90 o~ 8O o 70 0 60 Function 2 i i i Original ----~ ........ ~..-~ ....... ByClass(G) -+--- -., .................... ByClass(U) -~--- "',., Random(G) -..~ ....... ""-., Random(U)-* .... "-,. , I I I 50 100 150 200 Privacy Level (%) Function 5 5O I 0 5O Original ---*-- ByClass(G) ..... ByClass(U) -~--- Random(G) ...,~ ...... Random(U) ...... I 100 Privacy Level (%) I 150 200 Figure 6: Change in Accuracy with Privacy can we still construct sufficiently accurate predictive models. For the specific case of decision-tree classification, we found two effective algorithms, ByClass and Local. The algorithms rely on a Bayesian procedure for correcting perturbed distributions. We emphasize that we reconstruct distributions, not individual records, thus preserving privacy of individual records. As a matter of fact, if the user perturbs a sensitive value once and always return the same perturbed value, the estimate of the true value cannot be improved by successive queries. We found in our empirical evaluation that:  ByClass and Local are both effective in correcting for the effects of perturbation. At 25% and 50% privacy levels, the accuracy numbers are close to those on the original data. Even at 100% privacy, the algorithms were within 5% to 15% (absolute) of the original accuracy. Recall that if privacy were to be measured with 95% confidence, 100% privacy means that the true value cannot be estimated any closer than an interval of width which is the entire range for the corresponding: attribute. We believe that a small drop in accuracy is a desirable trade-off for privacy in many situations. Local performed marginally better than ByClass, but required considerably more computation. Inves- tigation of what characteristics might make Local a winner over ByClass (if at all) is an open problem. For the same privacy level, Uniform perturbation did significantly worse than Gaussian before correct- ing for randomization, but only slightly worse after correcting for randomization. Hence the choice be- tween applying the Uniform or Gaussian distribu- tions to preserve privacy should be based on other considerations: Gaussian provides more privacy at higher confidence thresholds, but Uniform may be easier to explain to users. 448 Future Work We plan to investigate the effectiveness of randomization with reconstruction for categorical at- tributes. The basic idea is to randomize each categorical value as follows: retain the value with probability p, and choose one of the other values at random with proba- bility 1-p. We may then derive an equation similar to Equation 1, and iteratively reconstruct the original distribution of values. Alternately, we may be able to extend the anMyticM approach presented in [War65] for boolean attributes to derive an equation that directly gives estimates of the original distribution. Acknowledgments A hallway conversation with Robert Morris provided initial impetus for this work. Peter Haas diligently checked the soundness of the reconstruction procedure. References [AC99] M.S. Ackerman and L. Cranor. Privacy critics: UI components to safeguard users' privacy. In A CM Conf. Human Factors in Computing Sys- tems (CHI'99}, 1999. [AGI+92] Rakesh Agrawal, Sakti Ghosh, Tomasz Imielin- ski, Bala Iyer, and Arun Swami. An interval clas- sifter for database mining applications. In Proc. of the VLDB Con]erence, pages 560-573, Van- couver, British Columbia, Canada, August 1992. [Agr99] Rakesh Agrawal. Data Mining: Crossing the Chasm. In 5th Int'l Con]erence on Knowl- edge Discovery in Databases and Data Mining, San Diego, 12alifornia, August 1999. Available from http : //w~w. almaden, ibm. eom/s/quest/ papers/kdd99_chasm, ppt. lAW89] Nabil R. Adam and John C. Wortman. Security- control methods for statistical databases. ACM Computing Surveys, 21(4):515-556, Dec. 1989. [BDF+97] D. Barbara, W. DuMouchel, 12. Faloutsos, P. J. Haas, J. M. Hellerstein, Y. Ioarm.idis, H. V. Jagadish, T. Johnson, R.Ng, V. Poosala, and K. Sevcik. The New Jersey Data Reduction Report. Data Engrg. Bull., 20:3-45, Dec. 1997. [Bec80] Leland L. Beck. A security mechanism for statistical databases. A CM TODS, 5(3):316--338, September 1980. [Ben99] Paola Benassi. 'IYuste: an online privacy seal program. Comm. ACM, 42(2):56-59, Feb. 1999. [BFOS84] L. Breiman, J. H. Friedman, R. A. Olshen, and t2. J. Stone. Classification and Regression Trees. Wadsworth, Belmont, 1984. [BS97] D. Barbara and M. Sullivan. Quasi cubes: Exploiting approximations in multidimensional databases. SIGMOD Record, 26(3):12-17, 1997. [12M96] [co82] [CoxS0] [12ra46] [CRA99a] [Cra99b] [CS76] [DDS79] [Den80] [Den82] [Din78] [DJL79] [ECB99] [Eco99] [EHN96] [eu998] C. Clifton and D. Marks. Security and privacy implications of data mining. In A CM SIGMOD Workshop on Research Issues on Data Mining and Knowledge Discovery, pages 15-19, May 1996. F.Y. 12hin and G. Ozsoyoglu. Auditing and infrence control in statistical databases. IEEE Trans. Softw. Eng., SE8(6):113-139, April 1982. L.H. Cox. Suppression methodology and sta- tistical disclosure control. J. Am. Stat. Assoc., 75(370):377-395, April 1980. H. 12ramer. Mathematical Methods of Statistics. Princeton University Press, 1946. L.F. Cranor, J. Reagle, and M.S. Ackerman. Be- yond concern: Understanding net users' atti- tudes about online privacy. Technical Report TR 99.4.3, AT&T Labs-Research, April 1999. Available from http://,,,, research, art. corn/ 1 ibrary/trs/TRs/99/99.4/99.4.3/report, hem. Lorrie Faith Cranor, editor. Special Issue on Internet Privacy. Comm. A12M, 42(2), Feb. 1999. R. 12onway and D. Strip. Selective partial access to a database. In Proc. ACM Annual Con]., pages 85-89, 1976. D.E. Denning, P.J. Denning, and M.D. Schwartz. The tracker: A threat to statistical database security. ACM TODS, 4(1):76-96, March 1979. D.E. Denning. Secure statistical databases with random sample queries. ACM TODS, 5(3):291- 315, Sept. 1980. D.E. Denning. Cryptography and Data Security. Addison-Wesley, 1982. C.T. Dinardo. Computers and Security. AFIPS Press, 1978. D. Dobkin, A.K. Jones, and R.J. Lipton. Secure databases: Protection against user influence. ACM TODS, 4(1):97-106, March 1979. V. EstiviU-Castr0 and L. Brankovic. Data swapping: Balancing privacy against precision in mining for logic rules. In M. Mohania and A.M. Tjoa, editors, Data Warehousing and Knowledge Discovery Da WaK-99, pages 389-398. Springer- Verlag Lecture Notes in Computer Science 1676, 1999. The Economist. The End of Privacy, May 1999. H.W. Engl, M. Hanke, and A. Neubaue. Regu- larization of Inverse Problems. Kluwer, 1996. The European Union's Directive on Privacy Protection, October 1998. Available from http ://~ww. echo. lu/legal/en/dataprot/ direct iv/direct iv. htlal. 449 ? [FelT2] [Fis63] [FJS97] [GWB97] [HE98] [HS99] [LCL85] fLEW99] [LM99] [LST83] [MAR96] [MST94] [o~8] [Opp97] [Qui93] I.P. Fellegi. On the question of statistical confidentiality: : J. Am. Star. Assoc., 67(337):7- 18, March 1972. Marek Fizz. Probability 2~heory and Mathemati- cal Statistics. Wiley, 1963: C. Faloutsos, H.V. Jagadish, and N.D. Sidiropoulos. Recovering information from sum- mary data. In Proc. of the 23rd Int'l Conference on Very Large Databases, pages 36-45, Athens, Greece, 1997. Ian Goldberg, David Wagner, and Eric Brewer. Privacy-enhancing technologie~ for the internet. In IEEE COMPCON, February 97. C. Hine and J. Eve. Privacy in %he marketplace. The Information Society, 42(2):5~6-59, 1998. \ John Hagel and Marc Singer. Net Worth. Harvard Business School Press, 1999. Chong K. Liew, Uinam J. Choi, and Chang J. Liew. A data distortion by probability distribu- tion. ACM TODS, 10(3):395-411, 1985. Tessa Lau, Oren Etzioni, and Daniel S. Weld. Privacy interfaces for information management. Comm. ACM, 42(10):89-94, October 1999. J.B. Lotspiech and R.J.T. Morris. Method and system for client/server communications with user information revealed as a function of willingness to reveal and whether the information is required. U.S. Patent No. 5913030, June 1999. E. Lefons, A. Silvestri, and F. Tangorra. An analytic approach to statistical databases. In 9th Int. Conf. Very Large Data Bases, pages 260-- 274. Morgan Kaufmann, Oct-Nov 1983. Manish Mehta, Rakesh Agrawal, and Jorma Ris- sanen. SLIQ: A fast scalable classifier for data mining. In Proc. of the Fifth Int 'l Conference on Extending Database Technology (EDBT), Avi- gnon, France, March 1996. D. Michie, D. J. Spiegelhalter, and C. C. Taylor. Machine Learning, Neural and Statistical Classi- fication. Ellis Horwood, 1994. Office of the Information and Privacy Com- missioner, Ontario. Data Mining: Stak- ing a Claim on Your Privacy, January 1998. Available from http ://.ww. ip. on. ca/ web.site, eng/mat t ors/sum_pap/papers/ dat amine, him. R. Oppliger. Internet security: Firewalls and beyond. Comm. ACM, 40(5):92-102, May 1997. J. Ross Quinlan. C~.5: Programs for Machine Learning. Morgan Kaufman, 1993. [Rei84] [RG98] [SAM96] [Sho82] [ST90] [The98] [Wim97] [TyW84] [w~5] [Wes98a] [Wes98b] [Wes99] [Wo~] [Yc77] Steven P. Reiss. Practical data-swapping: The first steps. ACM TODS, 9(1):20-37, 1984. A. Rubin and D. Greer. A survey of the world wide web security. IEEE Computer, 31(9):34-41, Sept. 1998. John Sharer, Rakesh Agrawal, and Manish Mehta. SPRINT: A scalable parallel classifier for data mining. In Proc. of the 2~nd Int'l Con- ference on Very Large Databases, Bombay, India, September 1996. A. Shoshani. Statistical databases: Characteris- tics, problems and some solutions. In Proceedings of the Eighth International Conference on Very Large Databases (VLDB), pages 208-213, Mex- ico City, Mexico, September 1982. P.D. Stachour and B.M. Thuraisingham. Design of LDV: A multilevel secure relational database management system. IEEE Trans. Knowledge and Data Eng., 2(2):190-209, 1990. Kurt Thearling. Data mining and privacy: A conflict in making. DS*, March 1998. Time. The Death of Privacy, August 1997. J.F. Traub, Y. Yemini, and H. Woznaikowski. The statistical security of a statistical database. ACM TODS, 9(4):672-679, Dec. 1984. S.L. Warner. Randomized response: A survey technique for eliminating evasive answer bias. J. Am. Star. Assoc., 60(309):63-69, March 1965. A.F. Westin. E-commerce and privacy: What net users want. Technical report, Louis Har- ris & Associates, June 1998. Available from http ://www .privaeyexehange. org/iss/ surveys/eeommsum, html. A.F. Westin. Privacy concerns 8z con- sumer choice. Technical report, Louis Har- ris & Associates, Dec. 1998. Available from ht tp://.ww, privacyexchange, org/iss/ surveys/1298toc, html. A.F. Westin. Freebies and privacy: What net users think. Technical report, Opinion Re- search Corporation, July 1999. Available from http://www, privaeyexchange, org/iss/ surveys/sr990714, html. The World Wide Web Consortium. The Platform for Privacy Preference (P3P}. Available from ht tp ://www. w3. org/P3P/P3FAQ, html. C.T. Yu and F.Y. Chin. A study on the protection of statistical databases. In Proc. A CM SIGMOD Int. Conf. Management of Data, pages 169-181, 1977. 450 www.SID.irArchive of SIDQuery-oriented Text Summarization using Sentence 
Extraction Technique  
 Mahsa Afsharizadeh 
Faculty of Engineering 
The University of Kashan 
Kashan
, I.R. Iran 
mafsharizade4@gmail.com 
Hossein Ebrahimpour-Komleh 
Faculty of Engineering 
The University of Kashan 
Kashan, I.R. Iran 
ebrahimpour@kashanu.ac.ir 
 Ayoub Bagheri 
Faculty of Engineering 
The University of Kashan 
Kashan, I.R. Iran 
a.bagheri@kashanu.ac.ir
  Abstract
 Today there is a huge amount of information from a 
lot of various resources such as World Wide Web, news articles, 
e-books and emails
. On the one hand, human beings face a 
shortage of time, and on the other hand, due to the social and 

occupational needs, they need to obtain the most important 

information from various resources
 Automatic 
text 
summarization
 enables 
us to 
access the most im
portant content 
in the
 shortest possible time. In this paper 
a query
-oriented text 
summarization technique is proposed by extracting the most 

informative sentences. 
To this end
, a number of
 features
 are 
extracted from the sentences
, each of which evaluates
 the 
importance of the sentences 
from an aspect
. In this paper 11 of 
the best features are extracted 
from each of the sentences. 
This 
paper
 has shown 
that use of
 more suitable features leads to 
improved summaries generated. 
In order to evaluate the 
automatic
 generated
 summaries, the 
ROUGE
 criterion has been 
used.
 Keywords: query-oriented summarization
, natural language 
processing
, text mining
, extractive summarization. 
I. INTRODUCTION
 The 
huge growth of information in various sources, 
including the World Wide Web, news articles, e-books and 
emails, has left mankind with a huge amount of information. 

The busy lifestyle of humans in the modern world has also 
minimized the time available for discovering information from 
this massive volume. This has led to the emergence of a kind 

of contradiction in the modern society of today's world. On the 

one hand, the lack of time and, on the other hand, the need to 
be aware of various information due to job and social needs, 
leads to requiring methods to facilitate access to information 

in the shortest possible time. 
Because of this issue
, the fields such as text mining, 
natural language processing and artificial intelligence also 

have come together and seek to find a solution to this problem.
 The result of the 

 efforts in these fields has led to 
the emergence of an interesting and important topic called 

automatic text summarization. Automatic summarization of 
text documents in the shortest possible time has made it 
possible for people to access the m
ost important content
. Manual text summarization 
requires a large number of 
specialized people in different fields and spent a great deal of 
time and effort in this direction. It's not possible to produce 

summaries
 for texts without people specialized in d
ifferent 
fields.
 These people, having enough knowledge and 
experience in the context of those texts, are able to do 
text
 summarization 
using the power of thought and reasoning.
 This 
suggests that automating this operation by the machine is a 

very useful
 process and requires the use of various 
information from areas such as artificial intelligence, natural 

language processing,
 and
 text 
mining.
 Automatic text summarization faces some challenges. For 
example, extracting the proper features for sentences and 

words in the text has a significant impact on the performance 
of the summarization system, leading to select the most 

appropriate sentences that contain the most important 

information about the main subject of the text. Text 

summarization methods are divided into two categories: 

extractive and abstractive. Extractive summarization extracts 

important sentences from source documents and group them 
together to generate summary. Abstractive summarization 
creates a brief useful summary by generating new sentences
. In this paper we propose an extractive technique
 for query
-oriented summarization
. The extractive summarization should identify and extract 
the important sentences in a document. So far, various 

methods have been used in the extractive summarization 

method. One of the
se methods is TF-IDF [1]. This is a 
numerical criterion that indicates the importance of a word in 

a document among a corpus of documents. 
TF shows the 
frequency of a word in a document. IDF is a measure that 

reduces the weight of frequently occurring words in the corpus 

and increases the weight of words that rarely occur. The words 

with high 
TF-IDF value, have a strong relationship with the 
document in which they are located. Many people have used 

TF-IDF to measure the importance of sentences in extractive 
summarization, for example [2]
, [3]
, [4], and [5]. 
www.SID.irArchive of SID 2 
 Another method used in extractive summarization is fuzzy 
logic in which, scoring sentences can be done using fuzzy 
logic
. At first, appropriate features are extracted from the text. 
Then, with regard to the extracted features for each sentence, 
a score is calculated using the fuzzy system. There are also 

some works done in this area, including [6]
, [7]
, [8]
, [9], and 
[10]. 
Extractive summarization is also done using graph-based 
methods. In this method, each sentence from the document is 

considered as a vertex in the graph. If there is a common 

semantic relationship between the two sentences, they will be 
connected and weighted through the edges. A graph-based 
ranking algorithm is used to decide the importance of a vertex 
in a graph. The most important vertices in this graph are 
considered as important sentences and included in the 
summary. Among those who have worked in this field can be 

mentioned [11]
, [12]
, [13], and [14]. 
Another method used in extractive summarization is LSA
i. 
The LSA method is an algebraic statistical method that derives 
the meaning and similarity of sentences based on 

information [15]
. The idea of using the LSA method for 
summarizing text documents was introduced in 2001 [16]. The 
LSA method is an automated technique for extracting 
relationships Between words in text documents. In this 

method, a Term-
by-Sentence matrix is first created from the 
original text. Then SVD is applied to it. This action leads to 
the discovery of hidden dimensions that are related to the 
various topics discussed in the document. Finally, the resulting 
matrices are used to identify and extract the most important 

sentences. These were some of the work done in the field of 
extractive summarization. 
The rest of the paper is as follows: part II describes the 
proposed extractive summarization method. Part III expresses 
the experimental results. Finally, part IV concludes the paper. 
II. THE 
PROPOSED 
EXTRACTIVE 
SUMMARIZATION
 In this paper a query-oriented extractive summarization 
method is proposed. The extractive summarization method 
extracts the sentences containing useful information and 
displays them in summary. The main challenge in these 
methods is how to identify and select important sentences in a 

document
. For this purpose, a score is given to each sentence 
based on the extracted features from them. Then the sentences 
are ranked according to these score values.
 The proposed 
Scheme is shown in Figure 1. In this paper we extend 
Ahuja
 and 
Anand
 work [17] by adding some other useful features 
[18]. Our proposed scheme is consisted of five steps, each of 
them is explained below. 
A. Data Preparation 
Data preparation is the first step in the proposed method. 
We apply the proposed summarization system to DUC 2007 
corpus [19]. This corpus has 45 clusters. Each of them has 25 
text documents. There are various text formats in these 

documents. There are plenty of redundant characters in them. 

In this step, the sentences are extracted from raw text with 

different formats. 
B. Text Pre-processing 
Text pre-processing is an important step in all text 
processing tasks. It contains some important parts such as 

tokenization, stop words removal, stemming and Part Of 
Speech Tagging
ii. 
1)
 Tokenization 
There are a lot of tokens that are not worthy of content 
such as question mark, surprise mark, comma and so on. For 
this purpose, in the tokenization process, these meaningless 
tokens are deleted from the text and the sentences are broken 
down to meaningful tokens. These meaningful tokens are 
separated from each other by the white space or punctuations. 
2)
 Stop Words Removal 
Stop words are the words that are frequently repeated in 


so on. clearing the text of these words is the task of stop words 
removal step. 
3)
 Stemming 
Stemming step reduces the words into their stems. For 

their stem

 4)
 POS tagging 
This step is used to specify the word category. Therefore, 
the words are categorized into the groups like 
nouns
, verbs
, adjectives
, adverbs
 etc
.  C. Feature extraction 
After text pre-

appropriate features from the text.  
In this paper, the extractive summarization technique is 
used. The most informative sentences are identified and 
extracted from the text. The main challenge in this technique 
is to decide which sentences are the most important. For this 
purpose, some useful features should be extracted from each 
sentence and a score is assigned it based on its feature values. 
Then the high ranked sentences are selected to be present in 

the summary. 
There are a number of features in the 
Ahuja
 and 
Anand
 work for the purpose of extractive summarization [17]
. Choosing the appropriate features has a great impact on the 

performance of the summarization. We use from these 
features for summarizing DUC 2007 corpus [19]. In order to 
enhance the performance of the summarization, we use a 
number of query dependent features that leads to higher 

ROUGE values [18]. The experimental results show this 
improvement. In this paper, we use from eleven appropriate 
features. The features are listed below: 
1-
 Document Feature 
2-
 Sentence Position 
3-
 Normalized Sentence Length 
4-
 Numerical Data 
5-
 Proper Noun 
  www.SID.irArchive of SID 3 
        
 
       Figure 1
.  The proposed summarization method 
6-
 Topic Frequency 
7-
 Topic Token Frequency 
8-
 Headline Frequency 
9-
 Start Cluster Frequency 
10- Skip Bi-gram Topic Frequency 
11- Cluster Frequency 
 Document Feature
: This feature calculates a weight for a 
sentence based on the total content of its document. It is sum 

of the weights of separate words in the sentence.  

 
Sentence Position
: The main idea behind this feature is that 
the sentences that appear in the beginning and end of the text, 
are usually more important than the sentences that appear 
across the text. This feature is calculates based on (1): 

In this equation, 
N is the total number of sentences in the 
document and 
x is the index of the interested sentence. 
 
Normalized Sentence Length
: This feature says that long 
sentences are more informative than short ones. It assigns a 
weight to a sentence based on the ratio of its length to the 
length of the longest sentence in its document. 
 Numerical Data
: The sentence with the numerical data is 
probably an informative sentence. It is computed by dividing 

the count of numerical data in the sentence to the length of 
sentence.  

 
Proper Noun
: Sentences with proper nouns are likely to carry 
valuable information. Therefore, they are a good choice for 
presence in the summary. This feature is computed based on 
the ratio of the count of total proper nouns in the sentence to 
the length of the sentence. 
 Topic Frequency
: This feature calculates the relative 
frequency of the content words (set of the words after stop 
words removal procedure) in the sentence in its topic 
description. 

 
Topic Token Frequency
: It is similar to 
Topic Frequency 
feature. The only difference between them is that 
Topic Token 
Frequency
 is computed on all the tokens instead of only the 
content words. 
 Headline Frequency
: This feature calculates the relative 
frequency of the content words in the sentence on the set of 

headlines in the cluster. 

 
Start Cluster Frequency
: It calculates sum of the relative 
frequencies of the content words in the sentence on the set of 
the first 100 words in its document and all of the documents in 
its cluster. 
 Skip Bi-gram Topic Frequency
: This feature is like to 
Topic 
Frequency
 feature. But it is computed on the skip bigrams 
rather than unigrams. A bigram is a sequence of two adjacent 

words in the text. A skip bigram is a bigram that allow gaps 

between its pair of words. 
 Cluster frequency
: It computes sum of the relative frequencies 
of the content words in the sentence in the total content words 

in its cluster. 
D. Sentence Scoring 
All of the feature values of a sentence are calculated in the 
previous step (11 features). In the 
Sentence Scoring
 step, a 
score is assigns to each sentence based on a linear function of 
its feature values. Total score for a sentence 
s is calculated by 
(2): 

The first five weights are the same as the original paper 
[17]. The rest of the weights are experimentally set as: 0.9, 

0.6, 0.5, 0.4, 0.4 and 0.2, respectively. 
E. Summary Generation 
After the 
Sentence Scoring
 stage, all of the sentences are 
ranked based on their scores. Top ranked sentences are 
selected to generate the summary. Considering that the 

summary length should be 250 words maximum. 
III. EXPERIMENTAL 
RESULTS
 In this paper, DUC 2007 corpus is used for summarization 
purpose. DUC 2007 corpus is consisted of 45 clusters each 

one has a set of 25 relevant text documents. Each cluster is 

about a topic. The purpose is to generate a fluent 250-word 

summary for each cluster. Each topic and its document cluster 

have given to 4 different NIST assessors. The
se assessors have 
created a 250-word summary of the document cluster. These 
Raw Data
  Data 
Preparation
  Text 
Pre-Processing
  Feature 
Extraction
  Sentence 
Scoring
  Summary 
Generation
  Summary
  1. Tokenization
 2. Stop Words Removal 
3. Stemming 
4. POS Tagging
  1. Document Feature
 2. Sentence Position 
3. Normalized Sentence Length 
4. Numerical Data 
5. Proper Noun 
6. Topic Frequency 
7. Topic Token Frequency 
8. Headline Frequency 
9. Start Cluster Frequency 
10. Skip Bi-gram Topic 
Frequency 
11. Cluster Frequency
  www.SID.irArchive of SID 4 
 multiple reference summaries are used in the evaluation of 
summary content. The comparison results for ROUGE 
measure (ROUGE-1 to ROUGE-4) between 
Ahuja
 [17] and 
the proposed method are shown in TABLE I. The comparison 

results in in this table show that the proposed method has 

improved the average recall, average precision and average F-
measure. 
The comparison results for ROUGE-L, ROUGE-W, ROUGE-
S and ROUGE-SU are shown in Figure 2 to Figure 5, 

respectively. The proposed method in all of the figures show 

higher results in compare to 
Ahuja
 method [17]
. This is 
because of enriching the feature set by appending some query 
oriented features to the primary feature set. So the extracted 
features will be both informative and query relevant. The 
experimental results in this paper have shown the 
improvement of the proposed method in comparison with 

Ahuja
 method [17]. The comparison of ROUGE-2 average 
recall and ROUGE-SU4 average recall results for our 

proposed model with 
Ahuja 
model and some peer systems that 
had participated in DUC evaluations are shown in Table II. 

For better visualizing this comparison, the results of Table II. 

are shown in a chart. Figure 6 shows this chart. 
IV.
 CONCLUSION
 In this paper a query-oriented text Summarization technique 
using sentence extraction is proposed. In the extractive 
summarization technique, the most informative sentences in 
the text are identified and selected to attend in the summary. 
To identify sentences containing valuable information, a set of 

appropriate features are extracted from the text. Whatever the 
extracted features of the sentences are more appropriate, the 
most informative sentences are more accurately identified and 

the quality of the generated summary improves. 
TABLE I. 
 ROUGE
 COMPARISON RESULTS BETWEEN 
PROPOSED 
METHOD AND 
AHUJA
  Avg
-Recall
 Avg
-Precision
 Avg
-F ROUGE
-1 Ahuja
 0.36480
 0.36790
 0.35181
 0.36116
 0.35808
 0.36439
 Proposed
 ROUGE
-2 Ahuja
 0.06887
 0.07579
 0.06667
 0.07467
 0.06774
 0.07521
 Proposed
 ROUGE
-3 Ahuja
 0.02012
 0.02469
 0.01957
 0.02439
 0.01984
 0.02453
 Proposed
 ROUGE
-4 Ahuja
 0.00939
 0.01229
 0.00915
 0.01212
 0.00926
 0.01220
 Proposed
 TABLE II. 
 EVALUATION 
RESULTS 
(ROUGE-2
 AND ROUGE-SU)
 BETWEEN 
PROPOSED 
METHOD AND 
SOME OTHER 
METHODS             
Result
   Method
 Average 
ROUGE
-2 Recall
 Average 
ROUGE
-SU 
Recall
 Peer 16
 0.03813
 0.07385
 Peer 1
 0.06039
 0.10507
 Peer 27
 0.06238
 0.11884
 Ahuja
 0.06887
 0.12922
 Peer 6
 0.07135
 0.12517
 Proposed
 0.07579
 0.13023
   Figure 2
.  ROUGE-L results for Ahuja and proposed method 
 Figure 3
.  ROUGE-W results for Ahuja and proposed method 
 Figure 4
.  ROUGE-S results for Ahuja and proposed method 
 Figure 5
.  ROUGE-SU results for Ahuja and proposed method 
0.32059 0.30913 0.31467 0.33087 0.32486 0.32773 Recall
Precision
F-measure
0.290.30.310.320.330.34Ahuja
Proposed
0.09225 0.1651 0.11832 0.0957 0.17439 0.12352 Recall
Precision
F-measure
00.10.20.30.4Ahuja
Proposed
0.12735 0.11904 0.12293 0.12833 0.12441 0.12621 Recall
Precision
F-measure
0.110.1150.120.1250.13Ahuja
Proposed
0.12922 0.12081 0.12475 0.13023 0.12624 0.12808 Recall
Precision
F-measure
0.1150.120.1250.130.135Ahuja
Proposed
www.SID.irArchive of SID Figure 6
.  ROUGE Results between Proposed Method and Some other 
Methods 
Each of these features takes the importance of sentences 
from a different perspective.
 In this paper,
 the proposed 
method by 
Ahuja
 has been improved by appending some 
appropriate query based features. The extracted features 
by 
Ahuja
 can
 be used for generic summarization but they are not 
sufficient for query based summarization. 
They are
 similarity 
between sentence to its document, the position of the sentence 
in the document, the length of the sentence, the existence of 

numerical data 
and t
he presence of
 proper nouns in the 
sentence.
 In 
order to identify 
both 
the 
informative
 and query 
relevant
 sentences, the feature set is enriched with a number of 
other
 appropriate
 features.
 The first set of features can identify 
informative sentences and t
he second set of appropriate 
features will help to extract the query relevant sentences. 

These
 features are related to the topic and headlines. 
Furthermore,
 skip bigrams
 are also considered in addition to 
the 
unigrams
. Finally, by 
using more complete set o
f convenient features, 
better results in summarization is 
achieved. 
The results of the experiments show 
this 
improvement
.    REFERENCES
 [1] Ramos, J. 
Using tf-idf to determine word relevance in document 
queries
. in 
Proceedings of the first instructional conference on 
machine learning
. 2003. 
[2] Garca-Hernndez, R.A. and Y. Ledeneva. 
Word sequence models 
for single text summarization
. in 
Advances in Computer-Human 
Interactions, 2009. ACHI'09. Second International Conferences 
on. 2009. IEEE. 
[3] Sarkar, K. 
An approach to summarizing Bengali news documents
. in 
proceedings of the International Conference on Advances in 
Computing, Communications and Informatics
. 2012. ACM. 
[4] Baralis, E., et al., 
Mwi-sum: A multilingual summarizer based on 
frequent weighted itemsets.
 ACM Transactions on Information 
Systems (TOIS), 2015. 
34(1): p. 5. 
[5] Jayashree, R., S. Murthy, and B.S. Anami. 
Categorized Text 
Document Summarization in the Kannada Language by Sentence 
Ranking
. in 
Intelligent Systems Design and Applications (ISDA), 
2012 12th International Conference on
. 2012. IEEE. 
[6] Babar, S. and P.D. Patil, 
Improving performance of text 
summarization.
 Procedia Computer Science, 2015. 
46: p. 354-
363. 
[7] Ghalehtaki, R.A., H. Khotanlou, and M. Esmaeilpour. 
A combinational method of fuzzy, particle swarm optimization and 
cellular learning automata for text summarization
. in 
Intelligent 
Systems (ICIS), 2014 Iranian Conference on
. 2014. IEEE. 
[8] Hannah, M.E., T. Geetha, and S. Mukherjee. 
Automatic extractive 
text summarization based on fuzzy logic: a sentence oriented 
approach
. in 
International Conference on Swarm, Evolutionary, 
and Memetic Computing
. 2011. Springer. 
[9] Modaresi, P. and S. Conrad. 
From Phrases to Keyphrases: An 
Unsupervised Fuzzy Set Approach to Summarize News Articles
. in 
Proceedings of the 12th International Conference on Advances in 

Mobile Computing and Multimedia
. 2014. ACM. 
[10] Suanmali, L., M.S. Binwahlan, and N. Salim. 
Sentence features 
fusion for text summarization using fuzzy logic
. in 
Hybrid 
Intelligent Systems, 2009. HIS'09. Ninth International Conference 
on. 2009. IEEE. 
[11] Mihalcea, R. 
Graph-based ranking algorithms for sentence 
extraction, applied to text summarization
. in 
Proceedings of the 
ACL 2004 on Interactive poster and demonstration sessions
. 2004. Association for Computational Linguistics. 
[12] Malliaros, F.D. and K. Skianis. 
Graph-based term weighting for 
text categorization
. in 
Advances in Social Networks Analysis and 
Mining (ASONAM), 2015 IEEE/ACM International Conference 
on. 2015. IEEE. 
[13] Litvak, M. and M. Last. 
Graph-based keyword extraction for 
single-document summarization
. in 
Proceedings of the workshop 
on Multi-source Multilingual Information Extraction and 
Summarization
. 2008. Association for Computational Linguistics. 
[14] Cheng, K., Y. Li, and X. Wang. 
Single Document Summarization 
Based on Triangle Analysis of Dependency Graphs
. in 
Network-
Based Information Systems (NBiS), 2013 16th International 
Conference on
. 2013. IEEE. 
[15] Landauer, T.K., P.W. Foltz, and D. Laham, 
An introduction to 
latent semantic analysis.
 Discourse processes, 1998. 
25(2-3): p. 
259-284. 
[16] Gong, Y. and X. Liu. 
Generic text summarization using relevance 
measure and latent semantic analysis
. in 
Proceedings of the 24th 
annual international ACM SIGIR conference on Research and 
development in information retrieval
. 2001. ACM. 
[17] Ahuja, R. and W. Anand, 
Multi-document Text Summarization 
Using Sentence Extraction
, in 
Artificial Intelligence and 
Evolutionary Computations in Engineering Systems
. 2017, 
Springer. p. 235-242. 
[18] Toutanova, K., et al. 
The pythy summarization system: Microsoft 
research at duc 2007
. in 
Proc. of DUC
. 2007. 
[19] 
DUC 2007.
 2007. 
                                                            i Latent Semantic Analysis 
ii POS tagging
 0.03813 0.07385 0.06039 0.10507 0.06238 0.11884 0.07135 0.12517 0.06887 0.12922 0.07579 0.13023 00.050.10.15ROUGE
-2 ROUGE
-SU Proposed
Ahuja
Peer 6
Peer 27
Peer 1
Peer 16
Journal of 
The Colloquium 
for
 Information System Security Education (CISSE)
 Edition 6, Issue 1 
- 
September 2018
  
  1 
 IoTCP: A Novel Trusted Computing 
Proto
col for IoT
 Shuangbao (Paul) Wang, Amjad Ali, Ujjwal Guin, Anthony (Tony) Skjellum
  Abstract 
- The ability to understand, predict, secure and exploit the vast array of 
heterogeneous network of things is phenomenal. With the ever
-increasing threats to cyber
 physical systems and Internet of Things, security on those networks of data
-gathering sensors 
and systems has become a unique challenge to industries as well as to military in the battlefield. 
To address those problems, we propose a trusted computing prot
ocol that employs discrete 
Trusted Platform Modules and Hardware Security Modules for key management, a 

blockchain
-based package verification algorithm for over
-the
-air security, and a secure 
authentication mechanism for data communication. The IoT
-based T
rusted Computing 
Protocol implements integrated hardware security, strong cryptographic hash functions, and 

peer-
based blockchain trust management. We have tested the protocol under various 

circumstances where devices have built
-in securities while others 
do not. We apply the new 
protocol to a SCADA system that contains more than 3,000 edge devices. The preliminary 

results show that proposed protocol establishes trust, improves security, integrity, and privacy.
  Keywords
 Trusted Computing, IoT, IoBT, Blockc
hain, TPM
   Journal of 
The Colloquium 
for
 Information System Security Education (CISSE)
 Edition 6, Issue 1 
- 
September 2018
  
  2 
 1 INTRODUCTION
 The Internet of Things (IoT) is fed with billions of devices and trillions of sensors. 
IoT networks undergird critical infrastructure, such as electric grids, transportation 
hubs, and nuclear power plants. They also link to systems
 containing valuable and 
sensitive personal information, such as hospitals, schools, and government 

institutions. A failure in of one of these systems or a cascade of such failures across 

systems, either in their operations or security, could lead to poten
tially catastrophic 

consequences for the population of that region, city and beyond [15, 29]. Yet many 

of the hardware and software elements used to control, monitor, and connect these 

systems are not designed with built
-in security, while others are outmo
ded and may 
not interface with newer technologies. For this reason, every IoT project must 

address the security and trust, and that can be hardened against tempering and 

compromise [30]. The ideal IoT, Internet of Battlefield Things (IoBT), and Cyber 

Physi
cal Systems (CPS) should be able to continue operating under attacks, and 
provide guaranteed performance [25
28]. Therefore, it is critical to secure edge 
devices and actuators, and make IoT networks resilient in the face of cybersecurity 

threats [1, 2, 10, 30].
 To ensuring the security and authenticity of IoT infrastructure is challenging 
since the edge devices have been manufactured in an environment with limited 

trust and government oversight [30]. These devices often have resource constraints 

such as lo
w power requirements, low area budget, limited memory, and/or 

extremely low
-cost. The attacks can originate either from the hardware or the 
software. Hardware attacks against a system can occur with physical tampering of 

an edge device and/or by the introduction of a cloned/counterfeit device into the 

IoT system [31
-35]. On the other hand, software attacks against the system can be 
performed through network attacks, such as Phishing, Denial of Service (DoS), and 

data spoofing [36
-37]. 
 Trusted Computing (TC
) is to guarantee the trustworthy of IT systems [8, 9]. 
It has shown promises in research, industrial projects [22], and military 

implementations. Practically, deploying a large system with Trusted Computing 
Journal of 
The Colloquium 
for
 Information System Security Education (CISSE)
 Edition 6, Issue 1 
- 
September 2018
  
  3 
 Base (TCB) is still a challenge especially in Io
T systems, where various protocols 
and legacy systems exist. 
 This paper first discusses security issues in IoT/CPS systems and then proposes 
a novel IoT
-based Trusted Computing Protocol (IoTCP) that integrates hardware 
security, strong cryptographic hash 
functions, and blockchain technology to 
establish trust among devices and to secure data communications. We apply 
machine learning and intelligent services to deliver adaptive cyber
-physical 
capability and services necessary to enable effective command and
 control across 
IoT/CPS systems. Finally, we illustrate an implementation of a testbed SCADA 

system with a dashboard and data analytics features.
 2 BACKGROUND
 Trusted computing is to build digital identities and increase assurance of trust 
among 
devices that are connected to the network. It adds a level of security on top 
of what is provided by the operating systems and hardware. Trusted computing base 

adds a hardware device, which has non
-volatile storage and cryptographic execution 
engines on ea
ch device.
 Trusted Platform Modules (TPM) and Hardware Security Modules (HSM) are 
cryptographic hardware that improve the overall security of a system. TPM is 

usually embedded into a device. For a device that do not come with a TPM, an 

HSM can be added. 
 Message Queue Telemetry Transport (MQTT) [12] is a device
-to-device IoT 
connectivity protocol. It has the advantages of small code footprint, low network 

bandwidth requirement, and lightweight therefore a good candidate especially for 

IoT/CPS systems. 
 A bl
ockchain is a distributed ledger [13]. For IoT/IoBT/CPS systems, it records 
all actions and operations [6]. Each block contains the data and a hash of a previous 

block. If any of the previous block is tempered with, it will affect the hash of the 

current b
lock. The temper
-proof property makes it a perfect candidate to guarantee 
Journal of 
The Colloquium 
for
 Information System Security Education (CISSE)
 Edition 6, Issue 1 
- 
September 2018
  
  4 
 packages authenticity for over the air updates. Guin et al. recently proposed to 
integrate blockchain technology to authenticate resource
-constrained, low
-cost 
edge devices [38]. SRA
M-
based physically unclonable functions (PUFs) were used 
to generate a unique digital fingerprints to identify edge devices.
 2.1 Trusted Platform Modules
 A TPM is a robust chip that is integrated into the systems providing hardware 
security and establishing 
trust within devices such as computers, weapons, vehicles, 
robots, wearables, actuators, and storage [5]. TPM enables server, gateways, and 

sensors to extend secure authentication [8, 11] and integrity with a TPM chip on 

each device. Mutual authentication 
of devices is required at session start and signing; 

and decipher are performed on the device [4]. 
 TPMs are basic building blocks used in many specifications, for providing an 
anchor of trust. They can be used for validating basic boot properties before 

allowing network access, or for storing platform measurements, or for providing 
self
-measurement to provide anchors of trust to hypervisors (in virtualization). The 
TPM 2.0 Profile Specification allows subsets of proven security to be implemented 

in a varie
ty of devices, from traditional clients to embedded and IoT systems, with 
smaller footprints, lower power consumption, and lower cost [16].
 2.2 TPM-
based Security Systems
 Bosch uses TPM in its cameras and other edge devices that act like a 
cryptographic coprocessor to the device. The TPM runs its own firmware, which 

is continuously maintained to provide optimal protection against possible threats 

known to the threat intelligent sensors [18]. Communication between the device 

firmware and the TPM chip happens via
 a secure agent inside the TPM. TPMs 
provide application program interfaces and commands for applications. It is 

impossible for the firmware or operating systems (OS) to modify anything inside 

the TPM directly. 
 Journal of 
The Colloquium 
for
 Information System Security Education (CISSE)
 Edition 6, Issue 1 
- 
September 2018
  
  5 
 Chrome OS uses TPMs for a variety of tasks, 
including software and firmware 
rollback prevention, protection of user data encryption keys, and attestation of the 
mode of a device [7]. 
 Microsoft Windows OS uses TPMs to offer features such as disk encryption, 
virtual smart cards, and device health att
estations [14]. During the start
-up process, 
the TPM releases the decryption keys only after comparing a hash of the OS 

configuration values with a snapshot taken earlier. This verifies the integrity of the 

Windows OS start
-up process. 
 Virtual smart cards
 use TPM to emulate the functionalities of physical smart 
cards, rather than requiring the use of a separate physical smart card and reader. 

Virtual smart cards are created in the TPM and offer similar properties to physical 

smart cards. Their keys are not
 exportable from the TPM, and the cryptographic 
component is isolated from the rest of the system [7, 14].
 2.3 Hardware Security Modules
 A hardware security module (HSM) is a hardware appliance that provides secure 
key storage and cryptographic operations within a tamper
-resistant hardware 
module [1]. HSMs provide both logical and physical protection to those devices, 

including cryptographic keys, from non
-authorized users and potential adversaries. 
The cryptographic function handled by most HSMs are asymmetric
 key pairs (and 
certificates) used in public
-key cryptography [3, 19]. Some HSMs can also handle 
symmetric keys and other arbitrary data.
 2.4 Challenges with TPMs and HSMs
 Though TPM provides a good level of security, abusing remote validation by 
manufacturers
 may decide what software would be allowed to run. In addition, the 
user actions may be recorded in a proprietary database without the user actually 

knowing. This has happened in smart TVs, smart toys, and other voice
-activated 
devices. As a result, privacy becomes an issue. Modern HSMs allow disabling certain 

functions in a lock
-down mode to improve security. However, an attack may 
Journal of 
The Colloquium 
for
 Information System Security Education (CISSE)
 Edition 6, Issue 1 
- 
September 2018
  
  6 
 extract a number of bits from a secret key and use the short key to launch a brute 
force attack (CVE
-2015-5464). Hash Message 
Authentication Code (HMAC) 
obfuscates private keys makes it hard to steal. However incidents such as replay 

attacks may still occur, due to the fact that the hash value is a constant over time. 

More cryptographic features must be added to avoid such attack
s.
 3 DESIGN OF A SECURE T
RUSTED COMPUTING PRO
TOCOL
 IoT devices such as IP cameras are the most exposed devices facing the most 
threats. Besides cyber threats, data can be hacked and stolen. Such might happen as 

the ultimate attempt by an attacker to retrieve
 certificates and keys to later
-on simulate an edge device by his own equipment, trying to hack deeper into the IoT 

systems [3]. 
 To build trust and secure communications, we propose a novel protocol that 
uses discrete TPM (dTPM) and HSM for establishing t
rust and key management. 
We use HMAC to generate strong cryptographic key and use MQTT protocol for 

authentication and data communication.
 3.1 Key Management with Discrete TPM
 A discrete TPM is an isolated, separate feature chip that all necessary computing 
resources are contained within the discrete chip package. A discrete TPM has full 
control of dedicated internal resource including RAM, non
-volatile memory, and 
cryptographic logic. Due to the architecture, vulnerabilities exist. 
 A device without a TPM must
 store private keys in its file system, where it 
might reside in an especially encrypted file but the key to this must also be stored 

somewhere in the file system. If hacking into a devices certificate store does not 

reveal what is being looked for, a sid
e-
channel attack may do, such an attack uses 
analytic hardware equipment to listen to the data of the system while performing 

its tasks. When triggering the authentication process, at some point, the key will 

appear unencrypted. This leaves vulnerabilities
 to attackers. 
 Journal of 
The Colloquium 
for
 Information System Security Education (CISSE)
 Edition 6, Issue 1 
- 
September 2018
  
  7 
 IoT systems consist of a network of devices, gateways, and sensors. Many are 
not security
-enabled, which run on a variety of old protocols. Since dTPMs are 
only integrated into some devices, those do not have built
-in dTPMs are not able 
to use the security protocol. To solve this problem, we use HSM to provide 
hardware security.
 3.2 Package Verification with Blockchain
 Private keys, if loaded with a certificate, are stored inside the TPM and then are 
no longer retrievable. They can then only be used through cryptographic operations 
provided by the TPM, specifically its secure agents. The private key is password 

protected and is secret until safe storage within the TPM. The encryption engine 

provides key handling support for symmetrical encryption
 such as AES with up to 
256-bit key length. Once the key is delivered, the AES encryption or decryption 
for communication or over payload is then done by the hardware accelerated 

encryption engine in the main CPU. 
 We implemented a Multi
-Factor Package Ver
ification algorithm (MFPV) for 
private management and over
-the-air update security. MFPV has a secure 
cryptographic hash (H) that hashes a message along with the key. It is 

computationally difficult to create the same hash without the key. The original 

mes
sage with its hash value can only be verified at an edge device with the same 

key. Note that the hash is computed twice in order to resist some forms of 

cryptologic analysis such as the birthday attack or Nostradamus attack [17]. Software 

packages are hash
ed first on the gateways before sending to the edge devices. When 
a package is pushed to an edge device, the device broadcasts a ledger to form a 

blockchain to guarantee authenticity and to prevent modification. The detailed 

package verification steps are 
as follows:
 Step 1.
 A hash is computed using MFPV algorithm using the equation: 
  MFPV (H, key, message) = H(APPEND(message, H(APPEND(key, 
 message)
) 
Journal of 
The Colloquium 
for
 Information System Security Education (CISSE)
 Edition 6, Issue 1 
- 
September 2018
  
  8 
  where, key is a unique ID on an edge device. The transmitting message 
 to be delivered as follows: 
  APPEND(message, MFPV (SHA3, key, message))
  For SHA3
-512, we can obtain equivalent security of 256 bits. Here, 
 the block size is 576 bits with unlimited m
essage size. HMAC with 
 SHA3 [19] has been mathematically proven strong in counter attacks. 
 However, data leaks could still happen if a gateway is compromised.
 Step 2.
 Each edge device serves as a sensor by broadcasting to the network 
 notifying a new package has been published and verified by the device. 
 The verification can be performed as follows:
 MFPV (APPEND(message, MFPV (APPEND(key, message))
  where, key is obtained directly from the Non
-Volatile Memory 
 (NVRAM). The information 
(ledger) each device broadcasts forms a 
 blockchain, which is resistant to package modification.
 Step 3.
 After a blockchain is verified, a confirmation is sent out to indicate the 
 package is trusted.
  Step 4.
 With both a correct hash value and the confirmation from peers
  
in 
 the blockchain, the edge device starts the OTA update.
 3.3 Light
-weight Secure Authentication
 For secure communication between edge devices and actuators, we use MQTT, 
a dTPM and a device SDK 
- 
client libraries to connect, authenticate, and exchange 
mess
age between devices and an actuator. MQTT enables low power usage, 
minimized data packets, small code footprint, and, most importantly, low network 
bandwidth. Its advantages make it a perfect candidate for sensor communication 

and mobile applications. As M
QTT supports TLS, mutual authentication can be 
Journal of 
The Colloquium 
for
 Information System Security Education (CISSE)
 Edition 6, Issue 1 
- 
September 2018
  
  9 
 implemented with strong encryption. MQTT consists of three main components: 
MQTT broker, MQTT subscriber, and MQTT publisher [12]. 
 We also use device shadows to replicate all connected devices such that each 
edge device has an identical shadow stored in the cloud. The device and its 

corresponding shadow are constantly compared making sure the integrity of data 

and interactions. In the event of a security breach, the monitoring agent will issue 

an alarm indicat
ing there is a conflict of states between the edge device and the 
corresponding shadow. An action must be taken to resume communication. Adding 

a device shadow can assure that a device can still interact with applications even 

when they are offline. The combination of MQTT and device shadow not only 

secures authentication and communication but also improves fault tolerance. 
 Figure 1 depicts the handshake process before a device is authenticated. The 
MQTT Broker can accept or reject the connection based on 
device authentication 
results. For secure communication, device authorization is necessary. The MQTT 

Broker checks the authorization policy to determine whether the device is 

authorized to publish/subscribe. When both authentication and authorization 

condi
tions are met, the device establishes its session to start communication.
  Journal of 
The Colloquium 
for
 Information System Security Education (CISSE)
 Edition 6, Issue 1 
- 
September 2018
  
  10  Figure 1: Secure Edge Device Authentication Protocol
 4 CASE STUDY AND PRELIMINARY RESULTS
 Using the newly proposed IoTCP protocol, we implemented on a water 
treatment SCADA system th
at has more than 3,000 PLCs [21, 24]. The data 
acquisition layer uses US Department of Defense developed API to collect data [20, 

21]. The defense
-in-depth architecture reduces the risk to CPS networks from 
being hacked or data from being stolen. We connec
t devices that are equipped with 
TPMs directly to the actuators using the newly developed security communication 

protocol. We add HSM modules to legacy devices so that they can connect with 

gateways with security and trust. The blockchain based package ver
ification 
algorithm ensures OTA security. Secure authentication with MQTT provides low 

bandwidth requirement and enhanced data security. Initial tests show that the 

proposed protocol has the advantages of easy connecting with various devices and 

reducing the risks of cyber intrusions, as a result of the lightweight and low 

bandwidth protocol. dTPM not only establishes trust between devices and actuators 

but also provides security for OTA updates. Figure 2 is a diagram of the water 

treatment SCADA system.
  Figure 2: A Water Treatment SCADA System Diagram
 Journal of 
The Colloquium 
for
 Information System Security Education (CISSE)
 Edition 6, Issue 1 
- 
September 2018
  
  11  IoTCP integrates dTPM
 and HSM to provide hardware level security, uses 
blockchain to improve OTA security, and uses MQTT with HMAT key scheme 
to secure authentication and communications.
 We utilized the research project as a case study in our master
-level cybersecurity 
courses
 [23, 29]. Students were divided into four teams. One team focused on 
overall security assessment and tested the IoTCP against a number of security 

assessment tools. The team especially focused on replay attack and remote rollback 

attack. The second team f
ocused on blockchain and MFPV package verification 
algorithm. The third team worked on secure communication, it verified the secure 

authentication protocol. The forth team built a dashboard to monitor and display 

the sensor data, service status and other d
ata analytics results.
 5 CONCLUSION
 IoT, IoBT, and CPS networks connect home appliances, sensors, traffic, vehicles, 
medical aids, weapons, smart grids, and industrial automation. The heterogeneous 

collection of microcontrollers, sensors, data interfaces and
 networks makes it 
difficult to establish trust and secure the data communication. IoTCP combines 

dTPM and HSM to secure key management, uses blockchain for package 

verification and OTA update security, and uses a low bandwidth and lightweight 

protocol 
- M
QTT to ensure authentication and communication security. 
Preliminary tests show that IoTCP is especially useful for connecting heterogeneous 

edge devices including legacy protocols. The device shadows add another level of 

security, enhance integrity and im
prove privacy. We are in the process of applying 
machine learning in modeling device behaviors based on the vast amount of xAPI 

data we gathered. Once the model is trained with acceptable accuracy, we will use 

it to predict whether a device can be trusted or actions have to be taken.
 Journal of 
The Colloquium 
for
 Information System Security Education (CISSE)
 Edition 6, Issue 1 
- 
September 2018
  
  12  6 ACKNOWLEDGEMENT
 This research is funded in part by grants from US National Science Foundation 
(NSF) [EAGER
-1419055, DGE
-1439570, 1821926, and 1755733] and National 
Security Agency (NSA) [S
-004-2017].
   Journal of 
The Colloquium 
for
 Information System Security Education (CISSE)
 Edition 6, Issue 1 
- 
September 2018
  
  13  REFERENCES
 [1]
 HP Study Reveals 70 Percent of Internet of Things Devices Vulnerable to Attack. 
(2014). Retrieved 3/1/2017 from http: //www8.hp.com/us/en/hp
-news/press
-release.html?id=1744676#.WLd1
-ThF04c 
 [2]
  The Internet Things Reference Model. (2014). Retrieved 3/4/2017 from 
http://cdn
.iotwf.com/resources/71/IoT_Reference_Model_ 
White_Paper_June_4_2014.pdf 
 [3]
 Internet Security Threat Report. (2016). Retrieved 3/4/2017 from 

https://www.symantec.com/content/dam/symantec/docs/reports/istr
-21-2016-en.pdf 
 [4]
 Security and Identity for AWS IoT. (2
017). Retrieved 3/4/2017 from 
http://docs.aws.amazon.com/iot/latest/developerguide/iot
-security
-identity.html 
 [5]
 Will Arthur, David Challener, and Kenneth Goldman. 2015. History of the TPM. 

Apress, Berkeley, CA. 1
5 pages. https://doi.org/10. 1007/978
-1-
4302
-6584
-9_1 
 [6]
 Francesco Buccafurri, Gianluca Lax, Serena Nicolazzo, and Antonino Nocera. 2017. 

Overcoming Limits of Blockchain for IoT Applications. In Proceedings of the 12th 

International Conference on Availability, Reliability and Security (ARES 17). ACM,
 
New York, NY, USA, Article 26, 6 pages. 

https://doi.org/10.1145/3098954.3098983 
 [7]
 Lisa J. K. Durbeck, Peter M. Athanas, and Nicholas J. Macias. 2014. Secure
-by-construction Composable Componentry for Network Processing. In Proceedings of 

the 2014 Symposium
 and Bootcamp on the Science of Security (HotSoS 14). ACM, 
New York, NY, USA, Article 27, 2 pages. 

https://doi.org/10.1145/2600176.2600203 
 [8]
 Trusted Computing Group. 2016. Secure the Internet of Things. (2016). Retrieved 

3/11/2017 from https://trustedcompu
tinggroup.org/trusted
-computing/ 
 [9]
 Kylene Hall, Tom Lendacky, and Emily Ratliff. 2005. Trusted Computing and 

Linux. TCG, 91
110.
 [10]
 Privacy Rights Clearing House. 2017. Data Breaches. (2017). Retrieved 3/7/2017 

from https://www.privacyrights.org/data
-breaches 
breaches. 
 [11]
 W. Hufstetler, M. Ramos, and P. Wang. 2017. NFC Unlock: Secure Two
-Factor 
Computer Authentication Using NFC. IEEE 14th International Conference on 

Mobile Ad Hoc and Sensor Systems (MASS 2017) (2017), 507 
 
510. 
 Journal of 
The Colloquium 
for
 Information System Security Education (CISSE)
 Edition 6, Issue 1 
- 
September 2018
  
  14  [12]
 MQTT.org. 2017. (2017). http://mqtt.org/ Downloaded March 3, 2017. 
 [13]
 Satoshi Nakamoto. 2008. Bitcoin: A Peer
-to-Peer Electronic Cash System. (2008), 1
9. 
 [14]
 Himanshu Raj and et. al. 2016. fTPM: A Firmware
-based TPM 2.0 Implementation. 
(2016), 1
22. 
 [15]
 Devarshi Shah, Anthony Skjellum, Jin Wang,
 and Peter He. 2017. Challenges and 
Opportunities for IoT
-Enabled Cybermanufacturing: Some Initial Findings from an 
Iot-
enabled Manufacturing Technology Testbed. (2017). 
https://pdfs.semanticscholar.org/9319/ 

fb10f1f3154f982d5baed6ab5919ed63ef18.pdf?_ga=1.6028727.52173338.1488642784 

Paper 66. 
 [16]
 Jianxiong Shao, Yu Qin, Dengguo Feng, and Weijin Wang. 2015. Formal Analysis of 

Enhanced Authorization in the TPM 2.0. In Proceedings of the 10th ACM 

Symposium on Information, Computer and Communications Security (ASI
A CCS 15). ACM, New York, NY, USA, 273
284. 
https://doi.org/10.1145/2714576.2714610 
 [17]
 Mark Stamp. 2005. Information security 
- 
principles and practice. Wiley. 
 [18]
 Bosch Security Systems. 2016. Trusted Platform Module explained. (2016). 
 [19]
 Apostol Vassilev. 2016. Approved Security Functions for FIPS PUB 140
-2, Security 
Requirements for Cryptographic Modules. (2016). Retrieved 3/7/2017 from 

http://csrc.nist.gov/publicat
ions/fips/fips140
-2/fips1402annexa.pdf NIST. 
 [20]
 Paul Wang and William Kelly. 2015. A Novel Threat Analysis and Risk Mitigation 

Approach to Prevent Cyber Intrusions. Colloquium for Information System Security 

Education (CISSE) 3 (2015), 157
174. Issue 1. 
 [21]
 Shu
angbao Wang. 2016. Dual
-Data Defense in Depth Improves SCADA Security. 
Signal (2016), 42
44. Issue 10. 
 [22]
 Shuangbao Wang, Amjad Ali, and William Kelly. 2015. Data Security and Threat 

Modeling for Smart City Infrastructure. IEEE Cyber Security of Smart Cities
, 
Industrial Control System and Communications (2015), 1
6. 
 [23]
 Shuangbao Wang and William Kelly. 2014. inVideo 
- 
A Novel Big Data Analytics 
Tool for Video Data Analytics. IEEE/NIST IT Pro Conference, 1
19. 
https://doi.org/0.1109/ITPRO.2014.7029303 
 Journal of 
The Colloquium 
for
 Information System Security Education (CISSE)
 Edition 6, Issue 1 
- 
September 2018
  
  15  [24]
 Shuangbao 
Wang and William Kelly. 2017. Smart Cities Architecture and Security in 
Cybersecurity Education. The Colloquium of Information Systems Security 
Education (CISSE) (2017). Issue 2. 
 [25]
 Shuangbao Wang and Robert Ledley. 2007. Modified Neumann Architecture with 

Micro
-OS for Security. CIICT, 303
310. 
 [26]
 Shuangbao Wang and Robert S. Ledley. 2006. Connputer 
- 
A Framework of 
Intrusion-
Free Secure Computer Architecture. In Proceedings of the 2006 
International Conference on Security & Management, SAM 2006, Las Vegas, 

Nevada, USA, June 26
-29, 2006. 220
225. 
 [27]
 Shuangbao Wang and Jiayin Zhang. 2014. A Video Data Search Engine for Cyber
-Physical Traffic and Security Monitoring Systems. IEEE/ACM Fourth International 

Conference on Cyber
-Physical Systems, 225
226. 
 [28]
 Shuangbao Paul Wang and Robert S. Ledley. 2013. Computer Architecture and 

Security. Wiley. 
 [29]
 Zhi-
Kai Zhang, Michael Cheng Yi Cho, and Shiuhpyng Shieh. 2015. Emerging 
Security Threats and Countermeasures in IoT. In Proceedings of the 10th ACM 

Symposium on Information, Computer and Communications Security (ASIA 

CCS 15). ACM, New York, NY, USA, 1
6. 
https://doi.org/10.1145/2714576.2737091 
 [30]
 U. Guin, A. Singh, M. Alam, J. Canedo, and A. Skjellum, A Secure Low
-Cost Edge 
Device Authentication Scheme for the Internet of Things, in International 

Conference on VLSI Design, 2018.
 [31]
 M. M. Tehranipoor, U. Guin, and S. Bhunia, Invasion of the hardware snatchers, 

IEEE Spectrum, vol. 54, no. 5, pp. 36
41, 2017.
 [32]
 U. Guin, S. Bhunia, D. Forte, and M. Tehranipoor, SMA: A System
-Level Mut
ual 
Authentication for Protecting Electronic Hardware and Firmware, IEEE 

Transactions on Dependable and Secure Computing (TDSC), 2016.
 [33]
 M. M. Tehranipoor, U. Guin, and D. Forte, Counterfeit Integrated Circuits: 

Detection and Avoidance. Springer, 2015.
 [34]
 U. Guin, K. Huang, D. DiMase, J. Carulli, M. Tehranipoor, and Y. Makris, 

Counterfeit integrated circuits: A rising threat in the global semiconductor supply 

chain, Proceedings of the IEEE, vol. 102, no. 8, pp. 1207
1228, Aug 2014.
 Journal of 
The Colloquium 
for
 Information System Security Education (CISSE)
 Edition 6, Issue 1 
- 
September 2018
  
  16  [35]
 U. Guin, D. DiMase, and M. Tehranipoor, Counterfeit integrated circuits: 
Detection, avoidance, and the challenges ahead, Journal of Electronic Testing, vol. 

30, no. 1, pp. 9
23, 2014.
 [36]
 T. Borgohain, U. Kumar, and S. Sanyal, Survey of security and privacy issues of 

Internet of thin
gs, arXiv preprint arXiv:1501.02211, 2015.
 [37]
 H. He, C. Maple, T. Watson, A. Tiwari, J. Mehnen, Y. Jin, and B. Gabrys, The 

security challenges in the IoT enabled cyber
-physical systems and opportunities for 
evolutionary computing & other computational intelligence, in Evolutionary 

Computation (CEC), 2016 IEEE Congress on. IEEE, 2016, pp. 1015
1021.
 [38]
 U. Guin, P. Cui, and A. Skjellum, Ensuring Proof
-of-
Authenticity of IoT Edge 
Devices using Blockchain Technology, IEEE International Conference on 

Blockchain, 
2018.
   Procedia Economics and Finance   27  ( 2015 )  77  84 
Available online at www.sciencedirect.com
2212-5671  2015 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license 
(http://creativecommons.org/licenses/by-nc-nd/4.0/
).Peer-review under responsibility of Faculty of Economic Sciences, Lucian Blaga University of Sibiu
doi: 10.1016/S2212-5671(15)00975-2 ScienceDirect
22nd Interna
tional Economic Conference
  IECS 2015 Econo
mic Pr
ospect
s in the C
ontext
 of Growing Gl
obal and Regi
onal Interdepe
ndencies
, IECS 2015 A Data Mining Bas
ed Approach to a Firm
s Marketing Channel 
Esra Kahya Ozyirmidokuz
a,*, Kumru Uyarb, Mustafa Hakan Ozyir
midokuz
c aErciyes Unive
rsity, Co
mputer T
echnolog
ies De
partment
, Melikgazi, Kayser
i, 38039, Turke
y bNuh
 Naci Yazgan 
Unive
rsity,Prod
uction and
 Market
ing Departmen
t, Kayseri, 
38170, Tur
key 
cBosch Ther
motec
hnic
, Ankara, 0
6810, Turke
y Abstrac
t Firms n
eed to co
llect and analyze marke
ting data in order to have a compe
titive advantage in the 
sector. The aim of this r
esearch is to ex
tract knowle
dge from
  an internati
onal firms marke
ting cha
nnel to improve the efficiency of the marke
ting system. Th
e Cross Indus
try Sta
ndard Pro
cess for Data Mining (CRISP
-DM) is u
sed to analy
ze the survey data. Data are clustered by a
pplyi
ng a Kohonen Self 
Organizing Map (SOM)  to re
duce
 the a
ttribut
es. Anomaly det
ection
 analysis is a
ppli
ed. We generate a
  C5.
0 Decision Tree (DT) model used for predicting the marketing channel firms complaints with very high accuracy. Decision rules 
are also extracted. 
 2015 The Authors. Published by Elsevier B.V. 

Peer-review under responsibility of Faculty of Economic Sciences, "Lucian Blaga" University of Sibiu". 
Keywords:
 Knowledge discovery in databases; data mining; DT, rule induction, marketing channel complaints 
1. Introduction 
A marketing channel, which delivers a firms products and 
services to consumers, is one of the critical success 
factors in a marketing system to achieve marketing objectives. If a firm doesnt know its marketing channel well, it 
  * Corresponding author. Tel.: +90-532-582-8886. 
E-mail address:
 esrakahya@erciyes.edu.tr (E. K. Ozyirmidokuza) 
 2015 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license 
(http://creativecommons.org/licenses/by-nc-nd/4.0/
).Peer-review under responsibility of Faculty of Economic Sciences, Lucian Blaga University of Sibiu
78   Esra Kahya Ozyirmidokuz et al.  /  Procedia Economics and Finance   27  ( 2015 )  77  84 
cant manage it. Firstly, the firm must collect data from its marketing channel. The firm needs to analyze data seriously 
to have a competitive advantage in the sector.  
Data Mining (DM), which is the process of automatically searching large volumes of data to extract knowledge 
from them a in a human-understandable structure, helps analysts to recognize relationships within data.  
Applying DM techniques to marketing data is extremely useful to find interesting, previously unknown, hidden 
patterns, which can then be better defined, in massive datase
ts. In this manner, DM helps to find important knowledge 

from the marketing channel. The achieved knowledge has a strategic importance in terms of competition and 

improvement of marketing and production for the firm.  This is because knowledge achieved can help to improve the 

communication between the marketing channel and the firm 
by better controlling the processes, and by knowing the 
details about them. 
One important type of knowledge that can be obtained from data mining is the decision tree (DT), which is 
constructed from existing data to classify future data. DTs are an effective method of classifying data set entries and 

can provide good decision support capabilities. DTs have seve
ral advantages over other data mining methods, 

including being human- interpretable, well-organized, computationally inexpensive, and capable of dealing with noisy 

data. Due to these merits, DTs are probably the most popular mining method [1]. There have been numerous studies 

in marketing which use decision trees (DTs) [2, 3, 4, 5, 6, 7, 8, 9].  
Among the data mining techniques, cluster analysis helps in the classification of data. Cluster analysis seeks to 
maximize between-group variances and minimize within-group variances, including both hierarchical and non-

hierarchical methods [10]. 
In the literature, Kohonens SOM is one of the techniques used for dimension reduction. Malone et al. [11]  
demonstrated a trained SOM (Self-Organizing Map) which 
could provide initial information for extracting rules that 
describe cluster boundaries. Wang et al. [12] used an SOM for pattern analysis and a fuzzy inference system to capture 

the chaotic trend to provide short-term (hourly) and long-term 
(daily) Web traffic trend predictions. Fessant et al. [13] 
used Kohonen SOMs and they showed how the mining of network measurement data can reveal the usage patterns of 

ADSL customers. Maiorana [14] proposed a feature selection method based on a clustering algorithm belonging to 

the Kohonen SOM family. Gomez-Carracedo et al. [15] ap
plied Kohonen SOMs to perform pattern recognition in 
four datasets of roadside soil samples obtained in four sampling seasons over a one year period. They used CART as 

an objective variable selection step before the SOM grouping. 
Eshghi et al. [16] compared 
three clustering techniques: 
traditional clustering methods, Kohonen maps and latent 
class models. Nohuddin et al. [17] introduced a technique 
that uses frequent pattern mining and SOM techniques to identify, group and analyze trends in sequences of time 

stamped social networks so as to identify interesting trends. In recent years, the Kohonen SOM method has been used 

in marketing [18, 19, 20, 21, 22]. 
In this research, we use CRISP-DM, which was developed in 1996 by analysts representing DaimlerChrysler, 
SPSS, and NCR. CRISP provides a nonproprietary and freely availab
le standard process for fitting DM into the general 
problem-solving strategy of a business or research unit. According to CRISP-DM, which is shown in Fig. 1, a given 

DM project has a life cycle consisting of six phases. The phase sequence is adaptive. That is, the next phase in the 

sequence often depends on the outcomes associated with the preceding phase [23]. 

  79 Esra Kahya Ozyirmidokuz et al.  /  Procedia Economics and Finance   27  ( 2015 )  77  84 
  Fig
. 1. CRISP-DM proces
s   In the d
ata prepar
ation phase, firstly
, we a
pplie
d anomaly d
etection analysis which is used to redu
ce the number o
f record
s in a d
atabase. In a
dditi
on, we d
etermined the im
porta
nt f
eatures with Kohonen SOMs as a dimensi
on-redu
ction technique to redu
ce the f
eatures. Alth
ough
 many different t
echn
iques could have been used in this study, e.g
., PCA, factor
 analysis,and 
attribute relevan
ce analysis, we used Kohonen 
Networks (KNs) in clus
tering due to the strengt
h of Kohonen maps th
at lies in th
eir ability to m
odel
 non-linear r
elationships between d
ata. The Kohonen map is one o
f the maj
or unsupervised ar
tific
ial neur
al network m
odel
s. Kohonen Maps are use
ful tools for DM model
s with 
large data sets. Hig
h-dimension
al data are projected to a lower dimension re
presentation scheme that 
can be easil
y underst
ood. In additi
on, Kohonen
 Maps 
can be used to 
process qualitative var
iables as we
ll as quan
titative ones
 [24].   DT indu
ction is the m
odeli
ng s
tep in the f
oreca
sting 
process
 and consists of the d
etermin
ation of the DTs t
o gener
alize previ
ously defined network behavi
or classes. DTs are 
easier to 
unders
tand and they 
offer an 
acceptable level of 
accuracy. Sever
al advan
tages of the DTs as a 
classific
ation t
ool are given in the 
literature 
[25]. In this res
earch, a C5.0 DT is app
lied to the 
preprocess
ed d
ata in order to 
predict the mark
eting chann
el firms comp
laints a
bout th
eir 
international firm. Decision rules are extracted. The managers of the firm can easily understand these rules and predict 
the marketing channel firms future behavior. 
The paper is organized as follows. The Kohonen Network (KN) is introduced in Section 2. Section 3 gives the 
details of the application. The conclusions are drawn in Section 4. 
2. Kohonen Networks 
The identification of information, or patterns, in large subs
ets of data is a property of the fields of data-mining 
and feature extraction. Unsupervised learning techniques are a subset of these fields which enable the identification 
and grouping of patterns without having seen that pattern before or having its key characteristics described; to do 
80   Esra Kahya Ozyirmidokuz et al.  /  Procedia Economics and Finance   27  ( 2015 )  77  84 
this
, a simi
larity m
easure is defined and the g
roups are 
clustered tog
ether into a lowe
r dimension
al space. Self- Organizing
 Maps
 (SOM) are one such 
technique which enable the ma
pping of d
ata with a large f
eature s
et into tw
o-dimension
al space. Mor
eover, 
SOMs 
allow the visu
al unders
tandi
ng of d
ata structures
. These 
can then be used as a
n aid in identifying and 
classifyi
ng anomal
ies in the d
atasets [
26]. 
The go
al of 
SOMs is to c
onvert a complex hig
h-dimensional input sign
al into a simp
ler low
-dimension
al discr
ete map
. Thus, SOMs are very sui
table for cluster analysis, when the 
underlyi
ng hidden p
atterns among
 record
s and fields are s
ought. SOMs structure the output n
odes into 
clusters of n
odes, where n
odes in close
r proximity are m
ore 
sim
ilar to 
each othe
r than othe
r nodes that are farthe
r apart. A typ
ical SOM arch
itecture is shown in Fig
. 2 (Larose, 2005).     
 
 
 
 
   Fig
. 2. Top
ology of
 a simp
le SOM KNs are a type of 
SOM, which 
itself
 represents a sp
ecial class of neur
al netw
orks. KNs can be considered as 
a non-hierarchical method of clus
ter analysis. As n
on-hierarch
ical methods of clus
tering, they assign an input v
ector 
to the n
earest clus
ter, on the basis of a 
predetermined dis
tance fun
ction, but they try to 
preserve a degr
ee of dependen
ce among the 
clusters by int
roducing a dis
tance between them. Consequen
tly, each output neu
ron has 
its own neigh
borhood, which is ex
pressed in 
terms of a dis
tance m
atrix. The output neu
rons are char
acterized by 
a dis
tance fun
ction between them
, described usi
ng the c
onfigur
ation of the n
odes in a unidimensional 
or bidimension
al space [27]. 3. Applicat
ion 3.1. Understanding Dat
a Data are collected manu
ally from 
300 firms par
ticipating
 in an in
ternational firms marketi
ng chann
el by one-t
o-one surveys.
 The co
llected surveys are co
nverted to a d
ata matrix wh
ich includes 
20 features f
or each firm includi
ng edu
cation, sex, age, mar
ital status, pa
yment a
rrangements, the f
acility 
preferen
ces provided by the firm, 
profession
, number of trade ma
rks, 
etc. The sex feature is 
elimina
ted ma
nually because 
all the marketi
ng chann
el firms owners
, 81 Esra Kahya Ozyirmidokuz et al.  /  Procedia Economics and Finance   27  ( 2015 )  77  84 
are male except f
or one, In this phase, the bas
ic statistical analysis and some expl
orator
y graph
ical analysis 
technique
s are used to 
understand the data m
atrix.
  Web graphs are performed to 
understand the
 relation
ships between f
eatures
. We w
ill give two examp
les. Figure 
3 illustr
ates the r
elation between edu
cational status and the exp
ectation
s of the mark
eting chann
el firms owners fro
m the man
ufacturer firm. We 
can easily s
ee that the mark
eting cha
nnel firms owners with 
a bachelors degr
ee prefe
r traveling
 and traini
ng from the man
ufac
turi
ng firm. However the owner with 
primary school edu
cation 
does not 
prefe
r traveling
 and traini
ng. We can 
also s
ee from Figure 4 th
at marketing cha
nnel firms owners with lower edu
cationa
l status 
prefe
r to co
mmunicate with
 region
al representatives
.   Fig
. 3. Web graph of educa
tional sta
tus and expectations 
from the manufactur
er  Fig. 4. Web graph of educa
tional sta
tus and co
mmunic
ation with manufacture
r 82   Esra Kahya Ozyirmidokuz et al.  /  Procedia Economics and Finance   27  ( 2015 )  77  84 
 Although 
classical s
tatistics work well in giving a p
icture of the cu
rrent over
all data matrix, they are 
unable to find hidden know
ledge wa
iting to be discovered. In this ma
nner, DM is used to find 
previously unknown and use
ful 
knowledge
. 3.2. Data pre
paration
 phase Anom
alous r
ecord
s wh
ich show different behavi
or from the 
previously m
easured values in the d
ata matrix mus
t be d
etected. Af
ter we a
pplie
d anomaly d
etection analysis, 
we detected two g
roups of r
ecord
s, one w
ith 236 and th
e other with 
64 records. Af
ter anomaly d
etection analysis, only thr
ee record
s wh
ich were gr
eater than the anomaly inde
x level (1.456) were 
elimin
ated from the d
ata matrix
.  It is 
known th
at feature s
electi
on im
proves m
odel
 accuracy in the 
knowledge discovery pro
cess. In this res
earch, Kohonen 
SOM is used to ch
oose a subset of input var
iables by 
elimin
ating f
eatures with 
little predictive inform
ation. Record
s are grouped by 
KN (Kohonen 
Network) so that r
ecord
s within a g
roup 
or cluster tend to be sim
ilar to each 
othe
r, and r
ecord
s in different g
roups are dissim
ilar. For the first 
20 cycles, the neigh
borhood size was s
et at R=2, an
d the 
learni
ng rate was s
et to decay 
linearly, s
tarting 
at  = 0.3. Then, f
or the next 
150 cycles, the n
eighborhood size 
was res
et to R=1 wh
ile the learning r
ate was 
allowed to decay 
linearly fro
m  = 0.3 to  = 0.1. The neu
rons ar
e organ
ized into two layers, namely the input layer and output laye
r. In the study, the input layer has 
45 neu
rons and th
e output layer has 
12 neu
rons.  The s
elf-organ
izing map identif
ied 12 different 
clusters. The 
clusters and number 
of record
s are illustr
ated in Tab
le1. Af
ter applying the Kohonen 
SOM model
, we 
elimin
ate 11 features from the d
ata matrix includi
ng sex and co
mmunication with the man
ufac
ture
r.       Tab
le 1. The 
SOM cluster
s x y Number of record
s 0 0 42 0 1 22 0 2 48 1 0 22 1 1 4 1 2 14 2 0 15 2 1 6 2 2 26 3 0 47 3 1 2 3 2 49 Total  297  After preprocess
ing, there are 8 f
eatures in the d
ata matrix as fo
llows
; Educationa
l status, Pr
eferences about quali
ty or pr
ice, Number of tr
ade marks, Mar
ital status, Pro
fession, Exp
ectations from the m
anufacturer, 
Different s
ectors, 
Payment a
rrangeme
nts. These f
eatures are the inputs of the D
T model. 
The output of the m
odel
 is Complaints 
about the m
anufacture
r. 3.3. Decision tree inducti
on DTs are one of the 
popular methods that have b
een used for 
knowledge discovery in d
atabase
s. Tree models can 
be defined as a recursive procedure, through which a set on statistical units are progressively divided into groups, 
according to a division of an explanatory
 variable to split and the choice of a 
splitting rule for such variable, which 
establishes how to partition the observations. The main result of a tree model is a final partition of the observations. 

To achieve this it is necessary to specify stopping criteria
 for the division process[27]. DTs provide an easy to 
understand overview for users without a DM background with high classification accuracy. They also provide a tree 

model of the problem and various alternatives in an understandable format without explanation. The acquired 

knowledge is usually quite understandable and can be easily 
used to obtain a better understanding of the problem. In 
83 Esra Kahya Ozyirmidokuz et al.  /  Procedia Economics and Finance   27  ( 2015 )  77  84 
additi
on, DTs a
ssist in making d
ecisions with existi
ng inform
ation. They hav
e satisfactory performance 
even when 
the training data are highly uncertain [28]. In this study, a C5.0 DT model is run to build a DT for predicting the 
complaints of the marketing channels firms. The aim of the classification is to find similar data items which belong 

to the same class.  
Modeling is performed with C5.0, which is one of the 
popular DT modeling algorithms and an extension of the 
earlier well known algorithm ID3. A C5.0 DT model which is acceptable with 10 tree depths is achieved. The mean 

is 91.3 and the standard error is 1.4 for the cross validation. The relations and knowledge are acquired. 
 One hundred and ninety one if-then rules are generated to express the process in English. The following examples 
illustrate some of the rules: 
 If the 
Number of trade marks
 > [ 4 ] and 
Profession
 is [ "Engineering" ] and 
Expectations from manufacturer 
are [Additional discount and term care]  then the 
firms complaint
 is [communicating problems], 
 if 
Educational status
 > [ bachelors degree ] and 
Profession
 is [ "wholesaler" ] and 
Expectations from 
manufacturer
 are [Additional discount and term care]  and
 Giro <= [15000000] then the 
firms complaint
 is [money collecting difficulties], 
 Stratified 
ten-fold cross-validation accuracy evaluation was used to train and test the data matrix. The accuracy 
rate of the model is 92.67 % which is shown in Table 2.  
      Table 2. Model accuracy 
 Number of records Percent of records 
Correct records 
278 
92.67% 
Wrong records 
22 
7.33% 
Total records 
300 
100% 
4. Conclusions 
Today, marketing channel decisions are as important as the decisions companies make about the features and prices 
of products [29]. In this research, we applied a DM fram
ework and we presented a decision tree induction from 
marketing channel data to improve the efficiency of the 
marketing system. DM techniques were implemented to 
marketing survey data. We explored the use of different pre-processing and DM techniques including anomaly 
detection analysis, Kohonen SOM, and C5.0 DTs. This research included attribute reduction using KNs. A C5.0 DT 

which was used for the classification of the data set with 10 tree depths was generated. The accuracy rate of the model 

was 92.67%. The DT model lays out the data matrix clearly so that all options can be explored. This acquired 

knowledge may be used to predict the future behaviors of
 the marketing channel firms. Data are processed into a 
manageable format, and decision rules are also generated. The DT model helps managers to understand the marketing 

channel firms. This research is also important to assess the future complaints of the firms and to plan future marketing 

developments. If we evaluate the current marketing channel and plan for capacity needs we will achieve a better 

marketing system performance. Consequently, the knowledge obtained will improve the performance of the marketing 

system. 
Alternative DM techniques using artificial intelligence methods can be studied in future research to compare 
various approaches and to implement this framework. 
Acknowledgements 
This work was supported by Erciyes University Research Fund, Project Number FBA-2014-5364, Turkey.  
This project is also supported by Bournemouth University, Faculty of Science and Technology, Software Systems 
Engineering, UK. 
84   Esra Kahya Ozyirmidokuz et al.  /  Procedia Economics and Finance   27  ( 2015 )  77  84 
References Chen, 
Y-L., Hu, 
H-W., Tang
, K., 2009. Construct
ing a De
cision Tre
e fr
om D
ata 
with Hierarchica
l Cla
ss Lab
els. Expert Syste
ms with Application
s 36, 4
8384847
. Crone, S.F
., Lessmann, S., Stahlbock, R., 2
006. The Imp
act of Preproce
ssin
g on Data Mi
ning: An Evaluation of Cla
ssifier Sensi
tivi
ty 
in Dir
ect Marketing
. European Journal of Oper
ational
 Research 173, 
781800. Lee, S
., Lee, S
., Park, Y., 
2007. A 
Prediction Mod
el for Success of Serv
ices 
in E-Co
mmerc
e Using Decision Tree
: E-custome
rs Attitude Toward
s Onl
ine Serv
ice. Exper
t Syste
ms with Applications 33, 
572581. Yada, K
., Ip, E.
, Katoh, N
., 2007. Is this Brand Ephemera
l? A Mult
ivariat
e Tree-
Based De
cision Analysis of New 
Product Sustaina
bili
ty. De
cision Suppor
t Syste
ms 44, 22
3234
. Kim, S.S., Timo
thy, D.J., Hwang, J., 2011. 
Understanding Japanese Tourists Shop
ping 
Preferences Us
ing the DT Analysis Me
thod
. Touri
sm Manageme
nt 3, 
544554. Olson, D.L
., Chae, B.K., 2012. Direct Marketing De
cision Support Through 
Predictive Customer Response Mod
elin
g. De
cision Support System
s 54,  443451
. Duche
ssi,
 P., Laur
a, E.J.M., 
2013. De
cision Tr
ee Mod
els for Profil
ing Ski Resorts 
Promo
tional and Adver
tising Strategies and the Imp
act on Sales. Exper
t Syste
ms with Appl
ications 40, 
58225829. Lei, N., Moon, 
S.K
., 2015. A De
cision Suppor
t System for Market
-Driven 
Product Positionin
g and Design
. Decision Suppor
t Syste
ms 69, 82
91. You, Z., Si, 
Y-W., Zhang, D
., Zeng, 
XX.,
 Leung, S.
C.H., L
i, T., 2015. A De
cision-Making Framework for Pr
ecision Marketing. Expert Syste
ms with Applications  42, 
33573367. Berson
, A., Smith
, S., Thearl
ing, K. Buildin
g Data Mining A
pplications for
 CRM
. New 
York: McGraw-Hi
ll; 200
1. Malone, J.
, McGarry, K
., Wermter, S
., Bowerman, 
C., 2005. Dm Us
ing Rule Extracti
on from Kohonen S
elf-Or
ganizing Maps. Neural Com
put and Applic 15,  917
. Wang, X., Abrahamb, A
., Smith, K.A
., 2005. I
ntellig
ent Web Tra
ffic
 Mining and Analysis. Journal of Network and Com
puter A
pplication
s 28,
  147165. Fessant, F., Franois, J., C
lerot, F., 2007. Characteriz
ing ADSL Cus
tomer Behaviours by Network Traff
ic Data-Mining. Anna
les de
s Tlcommuni
cations  62 (34), 3
50-368
. Maioran
a, F., 2009. Fe
ature Sele
ction w
ith Kohonen S
elf Organizing Class
ification A
lgorithm. World A
cademy of S
cience, Eng
inee
ring and Technolo
gy  27, 99
81003. Gomez-
Carrac
edo, M. P., A
ndrade, J.M., 
Carr
era, G.V.S.M.,  Aires-de
-Sou
sa, J., Car
losen
a, A., Prada, D., 2
010. Combining Kohonen Neura
l Net
works and Variab
le Selecti
on by Cla
ssification Tr
ees to Cluster Road S
oil Samp
les. Chemometr
ics and Intellig
ent Labor
atory Syste
ms 102, 20-34
. Eshghi, 
A., Haug
hton, D., Regrand
, P.,  Skaletsky, M., Woolford
, S., 20
11. Ide
ntifying Group
s: A
 Comparison of Me
thodologies
. Journal of D
ata 
Scienc
e 9, 27
1291
. Nohuddin, P.N.E
., Coenen, F
., Christley, 
R., Setzkorn, C., P
atel
, Y., Williams
, S., 2012. F
indi
ng I
nteres
ting Tre
nds in Social Net
works Us
ing 
Fre
quent Pattern Mining and 
SOMs. K
nowledge-
Based Syste
ms 29, 1
04113.
  Gmez
-Prez, G
., Martn-Gue
rrero, J.D., Sor
ia-O
livas, E., 
Balague
r-Balle
ster, E., Pa
lomares, A
., Casarie
go, N., 2
009. A
ssigning Disc
ount
s in a Marketing Camp
aign by Us
ing Rei
nforceme
nt Lear
ning and Neural Networks
. Exper
t Syste
ms with Applicati
ons 36, 80
228031. Lee, J.H
., Park, S.
C., 2005. I
ntelligent 
Profitable Customers Segme
ntation System 
Based on Busine
ss Intell
igence Too
ls. Expert Syste
ms with 
Applicatio
ns 29, 
145152. Mos
tafa, M.M., El-Masry, A.A
., 2013. Citize
ns as Consumers: 
Profil
ing E-Governme
nt Serv
ices Users 
in Egypt Via Data M
inin
g Techni
ques. Intern
ation
al Journal of Inform
ation Manageme
nt 33, 627 641
. Dragomir
, O.E., 
Dragomir, F
., Radulescu
, M., 2014
. Matlab A
ppli
cation of Kohonen S
elf-
 Organizing Map to C
lass
ify
 Consumers Load 
Profiles, 2nd I
ntern
ationa
l Conference on Information Techn
ology and Quantit
ative Manageme
nt, ITQM
, Pr
oce
dia Comp
uter S
cience 31, 
474  479
. Cuadros, J
., Domngu
ez, V.E
., 2014. Customer Segme
ntation Mod
el Based on Value Gener
ation for Marketing Str
ategies Formul
ation A
lvaro
. Estudios Geren
ciales
 30, 25
30. Larose
, D.T. Disc
overing Knowledge 
in Data: An I
ntroduction to DM. Wiley
, USA
; 2005
. Tuffery
, S. DM and Sta
tistics for Decision Making
. US
A: W
iley; 2011
. Rokach
, L. and Maimon, O
. Data Minin
g with De
cision Tr
ees. River Edge, NJ: World S
cientific; 2
007. Palomoa, E. J.
, Nor
th, J., Eliz
ondo, D. , L
uquea, R.M., W
atson, T., 
2012. Appli
cation
 of Grow
ing Hierarc
hical SOM for Visualiz
ation of Networ
k Forens
ics Tra
ffic 
Data
. Neural Networks 32, 275
284. Giudici, P. Applied DM
. England: Wiley
; 2003. Kahya-zyirm
idokuz,
 E., Gezer, A. and 
Ciflikli, C. , 2012. Characteriz
atio
n of Network Tra
ffic
 Data: A Data 
Preproce
ssin
g and Data  Min
ing 
Application. 
Data Analy
tics 2012: The First I
ntern
ation
al Conference on D
ata Analy
tics, Barc
elona, Iar
ia Think M
ind, 18-23.
  Hitt, M.,
 Black, S., Porter
, L. Manageme
nt. 2nd ed
. Upper Sadd
le Ri
ver NJ: 
Prentice Ha
l; 2009.      Sustainable Cities and Society 25 (2016) 3338Contents lists available at ScienceDirect
Sustainable
 Cities and Society
jou rnal h om epage: www.elsevier.com/locate/scsEngineering advanceAdvances and challenges in building engineering and data miningapplications for energy-efcient communitiesZhun (Jerry) Yua, Fariborz Haghighatb,, Benjamin C.M. FungcaCollege of Civil Engineering, Hunan University, Changsha 410082, Hunan, PR ChinabDepartment of Building, Civil and Environmental Engineering, Concordia University, Montreal, QC, Canada H3G 1M8cSchool of Information Studies, McGill University, Montreal, QC, Canada H3A 1X1a r t i c l e i n f oArticle history:Available online 19 December 2015Keywords:
Data MiningBuilding Energy UseOccupant BehaviorBig DataReviewa b s t r a c tThe rapidly growing and gigantic body of stored data in the building eld, coupled with the need for dataanalysis, has generated an urgent need for powerful tools that can extract hidden but useful knowledgeof building performance improvement from large data sets. As an emerging subeld of computer science,data mining technologies suit this need well and have been proposed for relevant knowledge discoveryin the past several years. Aimed to highlight recent advances, this paper provides an overview of thestudies undertaking the two main data mining tasks (i.e. predictive tasks and descriptive tasks) in thebuilding eld. Based on the overview, major challenges and future research trends are also discussed. 2015 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY-NC-NDlicense (http://creativecommons.org/licenses/by-nc-nd/4.0/).Contents1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 332. DM applications in the building eld . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 342.1. Predictive tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 342.1.1. Building energy demand prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 342.1.2. Building occupancy and occupant behavior . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 342.1.3. Fault detection diagnostics (FDD) for building systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 342.2. Descriptive tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 352.2.1. Framework development . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 352.2.2. Occupant behavior . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 352.2.3. Building modeling and optimal control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 352.2.4. Discovering and understanding energy use patterns . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 363. Future directions and challengesConcluding remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36Acknowledgements.
 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .37
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 371. IntroductionCurrently the importance of improving building energy per-formance for saving energy and enhancing building sustainabilityhas been widely recognized. One effective way of achieving thisobjective is to uncover and extract useful knowledge from build-ing operational data (e.g. temperature, ow rate, power andThis is an Engineering Advance paper.Corresponding author.E-mail address: haghi@bcee.concordia.ca (F. Haghighat).equipment states) that contains abundant valuable informationon actual building performance. The widespread use of buildingautomation systems (BASs) enables a tremendous amount of build-ing operational data to be stored in building databases that alsocontinue to expand. This rapidly growing and gigantic body ofstored data, coupled with the need for data analysis, has gener-ated an urgent need for powerful tools that can extract hidden butuseful knowledge from large building databases.As an emerging and promising technology, data mining (DM)is a powerful and versatile tool to automatically extract the valu-able knowledge embedded in huge amounts of data. It can bedened in many different ways. As dened by Cabena, Hadjinian,http://dx.doi.org/10.1016/j.scs.2015.12.0012210-6707/ 2015 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).34 Z. Yu et al. / Sustainable Cities and Society 25 (2016) 3338Stadler, Verhees, and Zanasi (1998), DM is an interdisciplinaryeld bringing together techniques from machine learning, patternrecognition, statistics, databases, and visualization to address theissue of information extraction from large databases. In the pastseveral decades, researchers have been vigorously and successfullyapplying DM in many scientic, medical, and application domainssuch as banking, bioinformatics and new materials identication.Recently it has also been introduced into the building eld that is awell-t application area for DM, since it generates and collects vastamounts of data on system operation, occupant behavior, powerconsumption, climatic conditions and etc.In general, DM includes six categories of widely accepted andimplemented techniques (Usama, Gregory, & Smyth, 1996):Data classication (e.g. the decision tree method, support vectormachine (SVM) and articial neural network (ANN)),Clustering analysis,Association Rule Mining (ARM),Regression,Summarization, andAnomaly Detection.Readers may refer to (Jia, Kamber, & Pei, 2012) for detailedinformation of these techniques. These techniques can be broadlycategorized into predictive tasks and descriptive tasks (Jia et al.,2012). This paper reports an overview of the recent studies under-taking the two tasks in the building eld.2. DM applications in the building eld2.1. Predictive tasks2.1.1. Building energy demand predictionThe prediction of building energy demand plays an importantrole in improving building performance. An accurate predictionneeds to take various signicant inuencing factors of buildingenergy demand into consideration, such as weather conditions,HVAC equipment, building envelopes and occupant behavior. Thecomplexity and uncertainty of these factors further adds difcul-ties to improve prediction accuracy. DM techniques may make abreakthrough in dealing with this complexity and uncertainty.Zhao and Magouls (2012) indicated that DM techniques arevery applicable to building energy demand prediction since theycan deal with non-linear problems. ANN and SVM are the twomost widely used DM techniques for this application (Ahmadet al., 2014). Kumar, Aggarwal, and Sharma (2013) applied vari-ous ANN methods, including back propagation, recurrent ANN, autoassociative ANN and general regression ANN. The adopted ANNarchitecture signicantly inuences the coefcient of variation thatranges from 2% to 40%. They concluded that ANN is more suitablefor the prediction of a large set of parameters than any statisticaltechniques. Li, Ren, and Meng (2010) reported that in many casesSVM shows higher prediction accuracy than ANN. However, train-ing SVM can be a very slow process due to large volume of trainingdata. The usage of parallel SVM (Zhao & Magouls, 2011) might bea feasible alternative.Both ANN and SVM models operate like a black box, mean-ing that the model can provide a prediction but cannot provide ajustication for supporting the prediction. In order to overcomethis limitation, Yu, Haghighat, Fung, and Yoshino (2010) developeda building energy demand predictive model based on the deci-sion tree method. Its competitive advantage lies in the ability togenerate accurate predictive models (92% accuracy in their study)with interpretable owchart-like tree structures that enable usersto quickly extract useful information. However, the decision treemethod is basically developed for predicting categorical variablesother than for predicting numerical variables.Considering that different DM techniques have their own spe-cialties, strengths and weaknesses, it is not easy to nd the bestcandidate for the building energy demand prediction. The recentresearch trend is towards the integration of different DM tech-niques for more accurate prediction. For example, Chou and Bui(2014) developed an ensemble model by combining ANN andSVM to predict cooling and heating demand. In order to predictthe next-day energy consumption and peak power demand, Fan,Xiao, and Wang (2014) developed ensemble models by combiningeight base DM models and assigning each DM modal an optimizedweight based on genetic algorithm (GA). The results show that theensemble models achieve higher prediction accuracy than those ofindividual base models.2.1.2. Building occupancy and occupant behaviorBuilding occupancy and occupant behavior are recognized ascrucial factors inuencing the discrepancy between practical andsimulated building energy consumption. However, it is difcult toinvestigate them analytically and then to develop reliable predic-tion models due to their complicated characteristics and stochasticnature (Yu, Haghighat, Fung, Morofsky, & Yoshino, 2011a). To meetthis challenge, different stochastic models (e.g. using probabilisticmethods (Sun, Yan, Hong, & Guo, 2014; Stoppel & Leite, 2014) andthe Markov Chain method (Muratori, Roberts, Sioshansi, Marano, &Rizzoni, 2013)) have been proposed. However, the proposed mod-els are severely restrained by the limitation: The modeling processtends to be complex and advanced mathematical knowledge isrequired.In order to remove the above limitation, researchers haveattempted to establish DM-based models. For example, Basu,Hawarah, Arghira, Joumaa, & Ploix, 2013) developed a decision-tree based model for predicting occupant behavior at the appliancelevel in residential buildings. DOca and Hong (2015) proposed aDM methodology to model ofce occupancy patterns and work-ing user proles based on big data streams. They found that thedecision tree method is suitable for predicting the occupancy pres-ence, supported by Zhao, Lasternas, Lam, Yun, and Loftness (2014),who built the occupant behavior prediction models based on appli-ance power consumption data in a medium-size ofce building. Theresults of both studies indicate that the modeling accuracy is verysatisfactory. However, the developed decision-tree based modelsare static models that are hard to simulate the dynamic nature ofbuilding occupancy and occupant behavior.The main focus of future research should be placed on testingand comparing other dynamic DM-based models and integratingthem into building energy modeling programs like TRNSYS andEnergy Plus. In addition, more research needs to be conducted sothat architects and designers will benet from bridging the gapbetween actual and predicted building energy performance.2.1.3. Fault detection diagnostics (FDD) for building systemsAutomating the process of detecting equipment and systemmalfunctions and making a proper diagnosis can help to ensurestable or optimal building operation. In terms of the approach toformulating the diagnostics, FDD methods can be categorized asmodel-based methods, which are based on prior knowledge ofunderlying system physics, and data-driven methods, which arebased on historical data (Katipamula & Brambley, 2005).The requirement of prior knowledge, together with complexmodeling processes and heavy computational burden, imposessevere constraints on the application of the model-based meth-ods. Comparatively, the data driven methods are much easier to usesince the models are normally automatically generated. Various DMtechniques have been employed as data driven methods for FDD.Z. Yu et al. / Sustainable Cities and Society 25 (2016) 3338 35For example, Magouls, Zhao, & Elizondo, 2013) developed an ANNmodel based on the recursive deterministic perceptron (RDP) ANNto implement FDD at the whole building level. Moreover, they pro-posed a new fault diagnostic procedure for identifying and rankingabnormal equipment in the order of fault risk. Other DM techniqueshave also been employed for FDD such as cluster analysis (Khan,Capozzoli, Corgnati, & Cerquitelli, 2013), fuzzy logic (Lauro et al.,2014) and SVM (Mulumba, Afshari, Yan, Shen, & Norford, 2015).In order to take advantage of different DM techniques andachieve more robust models, hybrid approaches have beenproposed for FDD such as the combination of the SVM and autore-gressive model with exogenous inputs (ARX) (Yan, Shen, Mulumba,& Afshari, 2014) and the combination of ANN and subtractive clus-tering analysis (Du, Fan, Jin, & Chi, 2014). Experimental resultssuggest these hybrid approaches have higher prediction accuracyand lower false alarm rates than the application of single models.However, the above DM-based models, both individual and hybrid,focused on historical data and cannot be used for real-time FDD.Recently, researchers have shown interest in developing on-line FDD tools that can provide real-time or near-real-time updateson model parameters and predictions (Bonvini, Sohn, Granderson,Wetter, & Piette, 2014). DM techniques, particularly the SVM,have been utilized in these studies and shown great potentialfor real-time FDD since they prevent performing numerous time-consuming simulations (Yan et al., 2014). Future research shouldfocus on verifying prediction of models based on a real world dataset.2.2. Descriptive tasks2.2.1. Framework developmentThe diversity of DM techniques and their functionalities neces-sitates establishing and providing a framework for users who maynot have much knowledge and experience with the application ofDM. Basically, such framework needs to specify which DM tech-nique/algorithm can be used for building application and whichkind of pattern can be mined correspondingly. The ultimate objec-tive is to help users to develop a set of data analysis methodologiesso that they can solve the problems based on the framework. More-over, case studies also need to be conducted in order to demonstrateits effectiveness. The framework, together with the case studies,enables users to analyze building-related data more efciently.Both general and specic DM frameworks have been proposed inthe past several years (Yu, Fung, & Haghighat, 2013). General frame-works provide a DM technique pool and different patterns will befound given different DM techniques are selected and combined.Specic frameworks are targeted at exploring a specic patternsuch as occupancy patterns and designated DM techniques areused.Regarding general frameworks, Yu et al. (2013) estab-lished a four-component framework consisting of DM tech-niques/algorithms. They also proposed a step-by-step data analysisprocess that starts from problem denition to knowledge discov-ery. Fan, Xiao, and Yan (2015) developed a framework that mainlyincludes four phases: data exploration, data partitioning, knowl-edge discovery and post mining. Regarding specic frameworks,DOca and Hong (2015) developed a framework to discover occu-pancy patterns and working user proles in ofce spaces. Thedecision-tree method and cluster analysis were combined to pro-vide insights into patterns of occupancy schedules. However, theseframeworks involve limited DM techniques and so far relativelyvery few problems has been tackled with the involved DM tech-niques.DM frameworks can deliver a set of data analysis methodologiesto practical operation, and thus efciently generalizing the usageof DM techniques to a wider population. They provide an overviewand a standardized process of the usage of DM techniques. There-fore, more frameworks, particularly specic frameworks, and casestudies with high replication potential should be developed andapplied for different applications.2.2.2. Occupant behaviorDM techniques have also been employed to conduct descriptivetasks for occupant behavior in several studies. These studies focuson either one type of specic behavior in ofce buildings or fullpatterns of behavior in residential buildings. Their main objectivesare to discover behavior patterns and to identify and improve theeffect of occupant behavior on building energy consumption. Theobtained knowledge can be integrated into building energy simu-lation programs especially when setting up behavior schedules.DOca and Hong (2014) combined clustering analysis withARM in order to discover occupants window opening/closingbehavior patterns in a natural ventilated ofce building. Basedon cluster analysis, Yu, Fung, Haghighat, Yoshino, and Morofsky(2011b) developed a new methodology for examining the inu-ences of occupant behavior on building energy consumption whileremoving the effect of other factors such as climate and build-ing envelopes. Based on cluster analysis, classication analysis,and ARM, Yu et al. (2011a) developed another methodology foridentifying energy-inefcient behavior in residential buildings andproviding occupants with feasible recommendations to improvethem. It is worth mentioning that the effective usage of DMtechniques, particularly cluster analysis, for describing occupantbehavior is heavily dependent on the size of established databases,which normally are restricted by various factors such as budgetsand effort in practice.Due to its complexity and randomness, the interactions betweenoccupant behavior and other inuencing factors are still poorlyunderstood. Further studies on the DM-based methodologies areactively encouraged, and the combination of cluster analysis withARM deserves extra attention.2.2.3. Building modeling and optimal controlBuilding operational data contains valuable information that canbe extracted for modeling and identifying the hidden relationshipbetween different variables, thereby improving building operation.Also, coupling the information into building control systems canassist in optimizing building performance. A number of studieshave already been conducted on such information extraction.With the goal of improving building performance, DM tech-niques have been employed to model different performancevariables such as furnaces cycle idle time (Li, Miao, & Shi, 2014),thermodynamic properties of refrigerants (Kc ksille, Selbas , &S encan, 2011), appliance usage (Basu, Debusschere, & Bacha, 2012)and coefcient of performance for refrigeration equipment (Chou,Hsu, & Lin, 2014). For example, Ahmed, Korres, Ploennigs, Elhadi,and Menzel (2011) used Nave Bayes, decision tree and SVM tosimplify the procedure to establish models for predicting internalnatural lighting and thermal comfort. The results show the highprediction accuracy and reliability of these techniques. Based onthe BAS database of the tallest building in Hong Kong, Xiao andFan (2014) rst applied clustering analysis to identify the typicalbuilding energy consumption patterns, and then applied ARM toeach cluster for discovering the hidden associations among powerconsumptions of major equipment such as chillers and pumps. As aresult, decit ow in the chilled water system was found and abnor-mal operation of the primary and secondary pumps was detected.DM techniques have also been employed for HVAC system opti-mal control with the aim of minimizing the energy consumptionwhile maintaining indoor comfort levels. Generally these tech-niques are integrated into controllers in order to take advantage oftheir strengths. For example, based on the fuzzy logic method, ANN36 Z. Yu et al. / Sustainable Cities and Society 25 (2016) 3338and Genetic Algorithm, fuzzy logic controllers (Hussain, Gabbar,Bondarenko, Musharavati, & Pokharel, 2014), ANN controllers(Moon, Lee, & Kim, 2014) and GA-based controllers (Congradac& Kulic, 2009) were developed. These controllers have alreadybeen widely applied in the building eld and a large numberof studies can be found in literature that is not listed in thispaper.Existing studies have already demonstrated the capabilities ofDM techniques as a means to effectively help improving and opti-mizing building performance. Potential future research includesdeveloping more accurate and robust models, particularly a com-bination of mining techniques, that can be readily usable. Also,more practical applications of DM-based controllers need to beconducted in the building eld in order to evaluate their usability.2.2.4. Discovering and understanding energy use patternsBuilding energy consumption patterns, both frequent and infre-quent, usually can be used to help identify operation abnormalitiesand faults. Effective pattern recognition and corresponding knowl-edge extraction by mining building historical data or simulationdata can lead to energy savings and performance improvement.Relevant studies at both the whole building level and sub-systemlevel have been conducted in recent years.At the whole building level, Miller, Nagy, and Schlueter (2015)applied clustering analysis to aggregate frequent energy use pat-terns so as to generate proles that may benet future simulationmodel calibration. Lange, Rodriguez, Puech, and Vasques (2013)developed a 3D visualization tool that can be integrated into BASsto visualize data in an understandable way. The work is part of theRIDER project that composed of a set of DM techniques aimed atgaining insight into building energy use patterns. Based on ARM,Yu, Haghighat, Fung, and Zhou (2012) developed a methodologyfor examining all associations and correlations between build-ing operational data, thereby pinpointing energy waste as well asequipment faults. By mining simulation data, Kim, Stumpf, and Kim(2011) developed an overall data analysis process to nd importantpatterns in order to improve the energy efciency of building designduring the early design phase.At the sub-system level, an increasing number of studies (Lai,Lai, Huang, & Chao, 2013; Beckel, Sadamori, Staake, & Santini, 2014;Rollins & Banerjee, 2014) focused on the investigations on appli-ance usage patterns. Chicco (2012) provided an overview of theprevious literature on clustering analysis for grouping and min-ing electrical load patterns, particularly residential demand proles(Rhodes, Cole, Upshaw, Edgar, & Webber, 2014). Few studies furtherfocused on more specic appliance usage patterns such as lighting(Motta Cabrera & Zareipour, 2013).Patterns extracted from building historic data and simulationdata provide a deep insight into the way building occupants con-sume energy. Utilities, policy-makers, building owners, designersand operators may benet greatly from these extracted knowledge.Most existing studies are still focused on general pattern analysisover a long time length, limited by low resolution data available. Inorder to discover in-depth knowledge of building performance, thefocus of future studies needs to be placed on more detailed patternsbased on high resolution data such as 1-min data.3. Future directions and challengesConcluding remarksThis paper reviews the advances in the eld of building engi-neering and DM applications over the last several years. It hasbeen shown that DM techniques are an effective tool for extractinghidden but useful knowledge from building-related data, therebyimproving building performance. However, some important prob-lems limiting the applications still need to be addressed and moreeffort is needed. The following described the encountered andanticipated challenges, potential future developments and recom-mendations for the application of DM techniques to the buildingeld:(1) Low-quality data leads to low-quality data mining results. Akey challenge is how to improve data quality in terms of relia-bility, completeness, consistency and resolution. There are twomajor reasons for the lack of high-quality data: the lack ofwidely-accepted benchmarks for identifying the parameters tobe monitored and the lack of adequate sensors for collectingreliable and high resolution data that tend to be costly. More-over, for residential buildings, the energy consumption data arecollected from the monthly or annual bills from utility compa-nies. These highly summarized data are insufcient to conductdetailed data analysis. The fact that end-use data on householdappliance usage is seldom monitored adds great difculties toin-depth data analysis.(2) Wide and diverse applications have been found and variousDM techniques have been employed. However, considering thatDM is a relatively young and fast-growing discipline and newtechniques emerges, clearly the potential of DM to extract use-ful knowledge from building-related data has not been fullyexploited. In this view, building researchers and engineers needto keep an eye on the current trend towards the development ofDM techniques, such as scalable and constraint-based miningmethods, as well as their applications.(3) Previous studies indicate that the applications have severalimportant limitations. For example, although a deeper under-standing of the data can be gained, the process of modelconstruction and knowledge extraction relies heavily on usersprior domain knowledge (e.g. setting a threshold for ARM anddetermining the optimal number of clusters). Also, it is not intu-itive on how to utilize the discovered knowledge. More effort isneeded for the exploration of improving the efciency and easeof the mining process and result analysis. One possible directionto take is the introduction of action mining or prot min-ing that has shown some success in the business world. TheirDM results are not just a bunch of rules, but some suggestedactions to maximize the prot or revenue. Similar concepts maybe applicable in the building eld.(4) The increasing volume, velocity and variety of building-relateddata usually result in databases with sizes and complexitybeyond the ability of traditional statistical analysis methodsto process them efciently. Such databases can be consid-ered as Big Data that is a current topic under discussion inInformation, Communication and Technology (ICT) in build-ings experts. Based on the denition and existing applicationof DM to building-related data, DM is expected to play animportant role in establishing effective Big Data managementapproaches. From this perspective, it is highly recommendedthat future research pay more attention to the following points:Passing the information on the usefulness of DM applicationsin the building eld to policy makers and building owners sothat they will, in turn, be supportive for more applications;formulating acceptable benchmarks for identifying theparameters to be monitored;lowering the cost of monitoring and accessing building-related data;developing methodologies based on new DM techniques fordiverse applications;improving the presentation and visualization of DM results;andinvestigating multiple building blocks and communitiesbesides individual buildings.Z. Yu et al. / Sustainable Cities and Society 25 (2016) 3338 37AcknowledgementsThis work was nancially supported by the National NaturalScience Foundation of China (No. 51408205), the FundamentalResearch Funds for the Central Universities, Hunan Provincial Sci-ence and Technology Department Major Project of China (No.2011FJ1007) and was also funded by ELS.ReferencesAhmad, A. S., Hassan, M. Y., Abdullah, M. P., Rahman, H. A., Hussin, F., Abdullah, H.,et al. (2014). A review on applications of ANN and SVM for building electricalenergy consumption forecasting. Renewable and Sustainable Energy Reviews, 33,102109.
This paper provides an overview of the building electrical energy predictionmethod using SVM and ANN. It suggests that combining multiple classicationmodels ensure better forecasting performance. The proposed approach ispromising.Ahmed, A., Korres, N. E., Ploennigs, J., Elhadi, H., & Menzel, K. (2011). Miningbuilding performance data for energy-efcient operation. AdvancedEngineering Informatics, 25, 341354.Basu, K., Debusschere, V., & Bacha, S. (2012). Appliance usage prediction using atime series based classication approach. In IECON 201238th AnnualConference on IEEE Industrial Electronics Society Montreal, Canada.Basu, K., Hawarah, L., Arghira, N., Joumaa, H., & Ploix, S. (2013). A prediction systemfor home appliance usage. Energy and Buildings, 67, 668679.Beckel, C., Sadamori, L., Staake, T., & Santini, S. (2014). Revealing householdcharacteristics from smart meter data. Energy, 78, 397410.Bonvini, M., Sohn, M. D., Granderson, J., Wetter, M., & Piette, M. A. (2014). Robuston-line fault detection diagnosis for HVAC components based on nonlinearstate estimation techniques. Applied Energy, 124, 156166.Cabena, P., Hadjinian, P., Stadler, R., Verhees, J., & Zanasi, A. (1998). Discovering datamining: From concept to implementation. Upper Saddle River, NJ: Prentice Hall.Chicco, G. (2012). Overview and performance assessment of the clusteringmethods for electrical load pattern grouping. Energy, 42, 6880.Chou, J.-S., & Bui, D.-K. (2014). Modeling heating and cooling loads by articialintelligence for energy-efcient building design. Energy and Buildings, 82,437446.Chou, J.-S., Hsu, Y.-C., & Lin, L.-T. (2014). Smart meter monitoring and data miningtechniques for predicting refrigeration system performance. Expert Systemswith Applications, 41, 21442156.Congradac, V., & Kulic, F. (2009). HVAC system optimization with CO2concentration control using genetic algorithms. Energy and Buildings, 41,571577.DOca, S., & Hong, T. (2014). A data-mining approach to discover patterns ofwindow opening and closing behavior in ofces. Building and Environment, 82,726739.
The authors rst implemented clustering analysis to the measured data indifferent ofces for obtaining patterns including motivations, duration andnumber of opening and typical degree of opening positions. Based on the obtainedinformation, ARM was then conducted and consequently the occupants weredivided into two typical working user proles: physical environmental driven andcontextual driven.DOca, S., & Hong, T. (2015). Occupancy schedules learning process through a datamining framework. Energy and Buildings, 88, 395408.Du, Z., Fan, B., Jin, X., & Chi, J. (2014). Fault detection and diagnosis for buildingsand HVAC systems using combined neural networks and subtractive clusteringanalysis. Building and Environment, 73, 111.Fan, C., Xiao, F., & Wang, S. (2014). Development of prediction models for next-daybuilding energy consumption and peak power demand using data miningtechniques. Applied Energy, 127, 110.Fan, C., Xiao, F., & Yan, C. (2015). A framework for knowledge discovery in massivebuilding automation data and its application in building diagnostics.Automation in Construction, 50, 8190.Hussain, S., Gabbar, H. A., Bondarenko, D., Musharavati, F., & Pokharel, S. (2014).Comfort-based fuzzy control optimization for energy conservation in HVACsystems. Control Engineering Practice, 32, 172182.Jia, Wei Han, Kamber, M., & Pei, J. (2012). Data mining concepts and techniques (3rded.). Waltham, US: Morgan Kaufmann.Katipamula, S., & Brambley, M. R. (2005). Methods for fault detection, diagnostics,and prognostics for building systemsA review, part I. HVAC&R Research, 11,325.Khan, I., Capozzoli, A., Corgnati, S. P., & Cerquitelli, T. (2013). Fault detectionanalysis of building energy consumption using data mining techniques. EnergyProcedia, 42, 557566.Kim, H., Stumpf, A., & Kim, W. (2011). Analysis of an energy efcient buildingdesign through data mining approach. Automation in Construction, 20, 3743.Kc ksille, E. U., Selbas , R., & S encan, A. (2011). Prediction of thermodynamicproperties of refrigerants using data mining. Energy Conversion andManagement, 52, 836848.Kumar, R., Aggarwal, R. K., & Sharma, J. D. (2013). Energy analysis of a buildingusing articial neural network: A review. Energy and Buildings, 65, 352358.Lai, Y.-X., Lai, C.-F., Huang, Y.-M., & Chao, H.-C. (2013). Multi-appliance recognitionsystem with hybrid SVM/GMM classier in ubiquitous smart home.Information Sciences, 230, 3955.Lange, B., Rodriguez, N., Puech, W., & Vasques, X. (2013). Discovering unexpectedinformation using a building energy visualization tool. In Proc. SPIE 8650,Three-Dimensional Image Processing (3DIP) and Applications.Lauro, F., Moretti, F., Capozzoli, A., Khan, I., Pizzuti, S., Macas, M., et al. (2014).Building Fan Coil Electric Consumption Analysis with Fuzzy Approaches forFault Detection and Diagnosis. Energy Procedia, 62, 411420.Li, Q., Ren, P., & Meng, Q. (2010). Prediction model of annual energy consumptionof residential buildings. In Proceedings of 2010 international conference onadvances in energy engineering.Li, M., Miao, L., & Shi, J. (2014). Analyzing heating equipments operations based onmeasured data. Energy and Buildings, 82, 4756.Magouls, F., Zhao, H.-x., & Elizondo, D. (2013). Development of an RDP neuralnetwork for building energy consumption fault detection and diagnosis. Energyand Buildings, 62, 133138.The authors simulated the daily electricity consumption of different electricequipment in an ofce building over a year. The parameters of the equipment areset to default and changed values, indicating normal and faulty conditions,respectively. The generated normal and faulty datasets were then used to train andtest the fault detection ability of the developed ANN model. Experimental resultsdemonstrate that this model is highly accurate in fault detection.Miller, C., Nagy, Z., & Schlueter, A. (2015). Automated daily pattern ltering ofmeasured building performance data. Automation in Construction, 49(Part A),117.Moon, J. W., Lee, J.-H., & Kim, S. (2014). Application of control logic for optimumindoor thermal environment in buildings with double skin envelope systems.Energy and Buildings, 85, 5971.Motta Cabrera, D. F., & Zareipour, H. (2013). Data association mining for identifyinglighting energy waste patterns in educational institutes. Energy and Buildings,62, 210216.This study proposed an ARM-based method to quantify and understandlighting energy waste patterns associated with occupancy patterns in classroomsin post-secondary educational institutes. Simulation results show that up to about40% savings could be achieved by implementing the discovered rules.Mulumba, T., Afshari, A., Yan, K., Shen, W., & Norford, L. K. (2015). Robustmodel-based fault diagnosis for air handling units. Energy and Buildings, 86,698707.Muratori, M., Roberts, M. C., Sioshansi, R., Marano, V., & Rizzoni, G. (2013). A highlyresolved modeling technique to simulate residential power demand. AppliedEnergy, 107, 465473.Rhodes, J. D., Cole, W. J., Upshaw, C. R., Edgar, T. F., & Webber, M. E. (2014).Clustering analysis of residential electricity demand proles. Applied Energy,135, 461471.This study clustered 103 homes in Austin by their hourly electricity usepatterns for each season. The optimal number of clusters was determined torepresent residential demand proles. The results will benet utilities forformulating policies on load shifting and efciency upgrade rebates.Rollins, S., & Banerjee, N. (2014). Using rule mining to understand appliance energyconsumption patterns. In 2014 IEEE International Conference on PervasiveComputing and Communications (PerCom).Stoppel, C. M., & Leite, F. (2014). Integrating probabilistic methods for describingoccupant presence with building energy simulation models. Energy andBuildings, 68(Part A), 99107.Sun, K., Yan, D., Hong, T., & Guo, S. (2014). Stochastic modeling of overtimeoccupancy and its application in building energy simulation and calibration.Building and Environment, 79, 112.Usama, Fayyad, Gregory, Piatetsky-Shapiro, & Smyth, P. (1996). From data miningto knowledge discovery in databases. AI Magazine, 17, 3754.Xiao, F., & Fan, C. (2014). Data mining in building automation system forimproving building operational performance. Energy and Buildings, 75,109118.Yan, K., Shen, W., Mulumba, T., & Afshari, A. (2014). ARX model based faultdetection and diagnosis for chillers using support vector machines. Energy andBuildings, 81, 287295.Yu, Z., Haghighat, F., Fung, B. C. M., & Yoshino, H. (2010). A decision tree method forbuilding energy demand modeling. Energy and Buildings, 42, 16371646.Yu, Z., Haghighat, F., Fung, B. C. M., Morofsky, E., & Yoshino, H. (2011). Amethodology for identifying and improving occupant behavior in residentialbuildings. Energy, 36, 65966608.Yu, Z., Fung, B. C. M., Haghighat, F., Yoshino, H., & Morofsky, E. (2011). A systematicprocedure to study the inuence of occupant behavior on building energyconsumption. Energy and Buildings, 43, 14091417.Yu, Z., Haghighat, F., Fung, B. C. M., & Zhou, L. (2012). A novel methodology forknowledge discovery through mining associations between buildingoperational data. Energy and Buildings, 47, 430440.Yu, Z., Fung, B. C. M., & Haghighat, F. (2013). Extracting knowledge frombuilding-related data - A data mining framework. Building Simulation, 6,207222.
This paper established a four-component framework consisting of DMtechniques/algorithms (including classication analysis, cluster analysis, andARM), potential applications, input and output. A step-by-step data analysisprocess that starts from problem denition to knowledge discovery was alsoproposed. It demonstrates the feasibility of applying the process and framework inthe building eld.38 Z. Yu et al. / Sustainable Cities and Society 25 (2016) 3338Zhao, H. X., & Magouls, F. (2011). New parallel support vector regression forpredicting building energy consumption. In Proceedings of 2011 IEEEsymposium on computational intelligence in multicriteria decision-making.Zhao, H.-x., & Magouls, F. (2012). A review on the prediction of building energyconsumption. Renewable and Sustainable Energy Reviews, 16, 35863592.Zhao, J., Lasternas, B., Lam, K. P., Yun, R., & Loftness, V. (2014). Occupant behaviorand schedule modeling for building energy simulation through ofceappliance power consumption data mining. Energy and Buildings, 82,341355. Procedia Computer Science   46  ( 2015 )  725  731 
Available online at www.sciencedirect.com
1877-0509  2015 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license 
(http://creativecommons.org/licenses/by-nc-nd/4.0/
).Peer-review under responsibility of organizing committee of the International Conference on Information and Communication Technologies (ICICT 2014)
doi: 10.1016/j.procs.2015.02.136 ScienceDirect
International Conference on Information and Communication Technologies (ICICT 2014)
 An Efficient CRM
-Data Mining Framework for the Prediction of 
Customer Behaviour
 Femina Bahari T
a,*, Sudheep Elayidom M
b aResearch Scholar, Cochin University
 of Science & Technology, Kochi
- 682022, India
 bAssociate Professor,
 Cochin University
 of Science & Technology, Kochi
- 682022, India
 Abstract
 CRM-data mining framework establishes close customer relationships and manages relationship between organizations
 and 
customers in
 todays advanced 
world of businesses. Data mining has gained popularity in various CRM applications in recent 
years and classification model is an important data mining
 technique useful in the field. The model is used to predict the 
behaviour of customers
 to enhance the decision
-making processes for retaining valued customers. An efficient CRM
-data mining 
framework is proposed in this paper and two classification models, Nave Bayes and Neural Networks are studied
 to show
 that 
the accuracy of Neural Network is comparatively better
.   201
4 The Authors. Published by Elsevier B.V.
 Peer
-review under responsibility of organizing committee of the International Conference on Information and Communication 
Technologies (ICICT 2014).
 Keywords:
 data mining framework;
 customer relationship management;
 prediction;classification.
 1. Introduction
 Data mining
15 is defined as a process that uses mathematical, statistical, arti
cial intelligence and machine
-learning techniques to extract and identify useful information and subsequently gain knowledge from databases. 
Information technology tools, advanced
 internet technologies and explosion in customer data has improved
 the 
opportunities for marketing and has changed the way relationships between organisations and their customers are 
  * Corresponding author. Tel.: +91
-9946012111; 
 E-mail address:
 feminabahari@gmail.com
  2015 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license 
(http://creativecommons.org/licenses/by-nc-nd/4.0/
).Peer-review under responsibility of organizing committee of the International Conference on Information and Communication 
Technologies (ICICT 2014)
726   T. Femina Bahari and M. Sudheep Elayidom  /  Procedia Computer Science   46  ( 2015 )  725  731 
managed
3. Customer Relationship Management helps in building
 long term and pro
table relationships with 
valuable
 customers
7. The set of processes and other useful systems in CRM help
 in developing a business strategy 
and this enterprise approach understand
s and in
uence
s the
 customer behaviour through meaningful 
communications
 so that cu
stomer acquisition, customer loyalty, customer retention and customer pro
tability
 are 
improved. The key factor in the development of a competitive CRM strategy
 is the understanding and analyzing 
 of 
customer behaviour and this helps in
 acquiring
 and retai
ning
 potential customers so as to maximize customer value
4. CRM
-data mining framework helps organizations to identify valuable customers and predict their future.
 Each CRM 
element can be supported by various data mining models based on the tasks performed.
 Data mining can be used in organizations for decision making and forecasting and one of the most common 
learning models in data mining that predicts the future customer behaviours
4 is classification. The prediction is done 
by the classification of databas
e records into a number of prede
ned classes based on certain 
criteria
1, 8
. Neural 
networks, decision trees, naive bayes, 
 logistic regression and SVM are the common to
ols used for classi
cation.
 To illustrate the performance of classification models we consider the CRM applications such as
 customer 
segmentation,
 prospecting and acquisition, affinity and cross sell,
 profitability, retention and attrition, risk analyses, 
etc9  in banking domain.
 Instead of mass campaign banks focus on direct marketing campaigns as one measure to 
improve customer development. 
The banks use the data available to retain its best customers and to identify 
opportunities to sell them additional services
11. Two classification models, the Multilayer 
Perception  
Neural  
Network  (MLPNN) which  have  their  roots  in  the  artificial intelligence
5,13
 and  Nave  Bayes (NB)  classifier, a 
simple probabilistic classifier based on applying Bayes'  theorem 
14 are used for the study.
 This paper proposes an effective CRM
-data mining framework
 and investigates the effectiveness of the two 
classification models in data mining in predicting the behaviour of customers in CRM application. An application in 
the bank direct marketing campaign is selected for the performance comparison of multilayer perception neural 
network and nave bayes classifier. The data set is well known as bank marketing
 data
 from the University of 
California at Irvine
 (UCI)
11. To assess the classifier performance, classification metrics such as accuracy rate, 
sensitivity analysis and specificity can be used
11.  The tool used for the study is Weka.
 The remaining sections of the article are organized as Section 2 which covers the data used in the work and 
Section 3 specifies the Problem Statement. Section 4 describes the CRM
-data mining framework, section 5 
describes the main concepts used in the work, section 6 illustrates the implementation logic, section 7 analyzes the 
test results and finally section 8 concludes the article.
 2. Data Sets Used
 The dataset used for experiments in this paper, contains results of direct bank marketing campaigns
11. It includes 
17 campaigns of a Portuguese bank conducted between May 2008 and November 2010. The customer was offered a 
long
-term deposit application by contact over phone. The dataset contains 45211 instances with two possible 
outcomes 
 either the client signed for the long
-term deposit or not. In our experimental dataset 10% of the pre 
processed dataset is used and it contains 16 input variables. Eight variables relate to the client, four variables relate 
to the last contact of the current campaign and another four relate to the campaign: 
  age, average yearly balance in Euros (numeric),
  job type,
 marital status, education (categorical),
  whether the client has credit in default, whether the client has a loan (binary), 
  whether the client has a personal loan (binary). 
  contact communication type, last contact month of year (categorical), 
  last contact day of the month, last contact
 duration in seconds (numeric).
  number of contacts performed before and during this campaign and for this client 
(numeric), 
  number of days that passed by after the client was last contacted from a previous campaign(numeric)
 ,  outcome of the previous marketing campaign (categorical). 
 The  output  variable  corresponds  to  campaign  output,  which  has  been  reduced  to  a binary output which 
indicates whether the customer subscribes to a deposit scheme or not.
 727 T. Femina Bahari and M. Sudheep Elayidom  /  Procedia Computer Science   46  ( 2015 )  725  731 
3. Problem Statement 
 To propose an efficient CRM
-data mining framework for the prediction of customer behaviour in the domain of 
banking applications. Within the framework proposed, two classification models are studied and evaluated.
 4. CRM-Data M
ining 
Framework
      
 
 
 
   
 
 
 
   
 
       Fig. 1.
 CRM
data mining framework
.   The proposed CRM
-data mining framework is
 shown in Fig.
 1. Understanding the business goals and 
requirements of the problem domain forms the initial phase of any problem in data mining. A close study and 
management of customer relationships and their interactions will help to identify attract and
 retain effective 
customers in the domain. The next phase of data preparation or preprocessing helps in preparing the data by the 
processes of cleaning, attribute selection, data transformation etc for further building up of models and their 

evaluation. Model construction in the CRM framework is a major step in which effective model to satisfy the 

business requirements is constructed. These models help in predicting the behaviour of the customers. Model 

evaluation and visualization measure the effectiveness
 of the model for enhancing their performance. 
 Customer 
Identification
 Customer Attraction 
and Retention
 Customer 
Development
 Business/Domain Understanding
 Data Preparation/ 
Pre
-processing
 Model Building
 Association
 Regression
 Classificatio
n Forecasting
 Clustering
 Model Evaluation
 Visualization
 728   T. Femina Bahari and M. Sudheep Elayidom  /  Procedia Computer Science   46  ( 2015 )  725  731 
5. Concepts used 
 5.1.
 Multilayer 
Perception
 Neural
 Network (MLPNN)  
 Multilayer  perception  neural  network (MLPNN) structure
5,13,16
  is organized as a layered set of neurons. Among 
the input, output and hidden layers
 of neurons
5 the actual computations of the network are performed
 in the hidden 
layer, where each neuron sums its input attributes
 xi after multiplying them by the strengths of the respective 
connection weights 
wij.  The activation function (AF) of this
 sum
 gives the output 
yj and sigmoid function
16 is the AF 
used in the experiment.
  yj = f (wij,
 xi)                                                                                                                               
                           (1)
  Back
-propagation (BP)
 learning
 is the most
 common training technique used for MLPNN
2. The sum of squared 
differences between the desired and asset value of the output neuron's E is defined as:  
  E = 
j (yd j 
- yj) 2                                                                                                                            
                       (2)
  Where 
yj is the output of a
 neuron
 j whose desired value is
 yd j 
. Weights  
wi,j
  in  equation  (1),  are  adjusted  to  
finding the minimum error 
E of equation (2) as early
 as possible. 
The difference between  the  network  out
puts  and  
the  desired  ones
 is reduced by the application of weight correction
 by BP. The neural networks helps in
 learning
, and reducing
 the future errors
5. Good learning ability, fast  real
-time operation, less memory demand, analysis of 
complex patterns are some of the advantages
 of MLPNN and
 the disadvantages include high
-quality data
 requirement of the network, careful selection of variables
 a priori and so on
16. 5.2.
 Naive Bayes (NB)
 Bayesian  classifiers  are  helpful  in  predicting  the probability  that  a  sample  belongs  to  a  particular  class. 
The technique is used for large databases because of its high accuracy and fastness to train with simple models. To 
estimate the parameters (means and variances of the variables) n
ecessary for classification, the classifier requires 
only a small amount of training data. It also handles
  real and discrete data
5.  We can use Bayes rule as the basis for designing learning algorithms, as follows: To learn some target function 
f: UV, or equivalently, 
P (V|U)
, we use the training data to learn estimates of 
P (U|V)
 and 
P (V).
 Using these 
estimated probability distributions and Bayes rule
14 new 
U examples can then be classi
ed.  From  a  given  set  of  training  instances with class labels, a
 learner  in  classification  learning  problems,  
attempts  to construct  a  classifier. The Naive Bayes 
classi
er assumes all attributes describing 
U are conditionally 
independent given 
V. The number of parameters that must be estimated to learn the classi
er is reduced dramatically 
by this assumption
. For both discrete and continuous 
U14, Naive Bayes is a widely used learning algorithm
. 5.3.
 Weka
 The Waikato Environment for Knowledge Analysis (Weka)  is a  machine  learning  toolkit
 used extensively for 
research, education and projects. Weka is
 introduced by Waikato University, New Zealand and
 is open source 
software written in Java (GNU Public License). It consists of collection of machine learning algorithms and tools for 
data mining tasks such as data pre
-processing or data preparation, classification, association rules, clustering, 
regression, forecasting and visualization and is well suited for developing new machine learning schemes
16. Weka 
3.7.4 is used for experimentation in our work and can be run on Linux, Windows and Mac.  
 729 T. Femina Bahari and M. Sudheep Elayidom  /  Procedia Computer Science   46  ( 2015 )  725  731 
6. Implementation Logic 
 6.1.
 Data Preprocessing
 Originally the dataset contained 79354 contacts and 58 attributes. Contacts with missing data or inconclusive 
results were discarded leading to 45211 instances and 17 attributes without missing values. Attributes relevant to 
contact information was obtained from the campaign reports whereas attributes relevant to clients were collected 
from the banks internal database. The output attribute is whether the client subscribes the bank term deposit or not. 

Out of 58 attributes there were several irrelevant attributes that adversely affects the data mining learning process. A 

manual feature selection procedure with the help of Rattle analysis was used to reduce the attributes to 29 input 
attributes and 1 output attribute. This was further reduced in various CRISP
-DM iterations and finally 17 attributes 
including the output attribute were selected for the test data
11. Only 10% of the instances (4521) available in the 
dataset were used for the experimental studies.
 6.2.
 Modeling
 All e
xperiments were performed using weka tool and were conducted in windows 7 with Intel Core i3 2.53 GHz 
processor. In this work, we build two distinct DM classifier models: MLPNN and NB. For all the two models test 
mode of tenfold cross validation was used. ML
PNN uses back propagation to classify the instances. The nodes are 
all sigmoid except
 for when the class is numeric. We set the number of hidden layers using the heuristic 
a=round 
(M/2)
 where 
M is the sum of attributes and classes. Other network parameters were set as follows: learning rate 0.3, 
momentum 0.2, training time 500ms and validation threshold 20. During the modeling phase we successfully tested 
the two models, MLPNN and NB using the weka tool. 
10-fold cross validation
 was used to perform our 
experiments. 
Ten disjoint subsets
 were created by partitioning the original dataset 
randomly. Nine of the subsets  
were  combined  to  form  the  training  set  and  the remaining  subset  forms  the  testing  set in each of the ten runs. 
Based on the response, two classes were obtained, those which responded positive and those responded negative. 
 7. Test Result
 The results of our experiment for automatically classifying a given dataset are summarized in
 Table 1.
 which 
shows values for two different classifiers. For each method the classification accuracy (amount of correctly 
classified instances), true positive rate
 (the proportion of actual positives which are correctly identified as such)
, false positive rate (incorrectly classified positive), ROC area (area under the ROC curve) and the time taken to build 
the classifier model is shown. 
 Table 1
. Results of comparison of classifiers, average over 10 runs.
 Classifier
 Classification Accuracy(%)
 True Positive 
Rate(TPR)
 False Positive 
Rate(FPR)
 ROC Area
 Time taken to 
build models (s)
 MLPNN
 88.63
 0.41
 0.052
 0.847
 1767.75
 NB 87.97
 0.47
 0.067
 0.858
 0.08
  MLPNN classifier model shows better accuracy (88.63%) among the two models experimented. NB gives better 
values of TPR
 (0.47)
, FPR
 (0.067) 
and ROC area
 (0.
858). The time taken to build the model is very high for 
MLPNN (1767.75s)
.  In our work three statistical measures namely classification accuracy, sensitivity and specificity
 are used to 
evaluate the performance of the
 classification model
s. This information is given in terms of instances classified as 
true positive (TP), true negative (TN), false positive (FP) and false negative (FN
)6. This information about actual 
and predicted classification defines a confusion matrix and is given in Table
 2. 
 730   T. Femina Bahari and M. Sudheep Elayidom  /  Procedia Computer Science   46  ( 2015 )  725  731 
Table 
2. Confusion Matrix for each classifier
. Classifier
 Confusion Matrix
 a                
b   Classified as
 MLPNN
 213           308 
 206           3794
 a = yes b = no
 NB 246           275
 269          3731
 a = yes b = no
  Classification accuracy as shown in equation (3) is equal to the sum of TP and TN divided by the total number of 
cases N
5 and it refers to the ratio of the number of correctly classified cases.
  Accuracy   =      TP+TN                                                                                                        
                                  
(3) 
  N   
                 
 Sensitivity
 in equation (4) is equal to ratio
 of TP to the sum of TP and FN and it refers to the rate of correctly 
classified positive (True Positive Rate).
  Sensitivity =       TP                                                                                                         
                                        
(4)
                        TP+FN
  Specificity in equation (5) is equal to TN divided
 by sum of TN and FP
6 and it refers to the rate of correctly 
classified negative (True Negative Rate).
  Specificity =        TN                                                                                                           
                                     
(5)
                          TN+FP
  MLPNN model shows the best values for accuracy (88.63%) and specificity (94.85%) for the training samples 
where as NB gives the best value for sensitivity (47.2%). The performance of the three classifiers is compared in 
terms of accuracy, sensitivity and specificity and 
 the results of the compariso
n are as shown in Table 3
. Table 
3. Performance comparison of classifiers in terms of accuracy, sensitivity and specificity
 Classifier
 Accuracy(%)
 Sensitivity(%)
 Specificity(%)
 MLPNN
 88.63
 40.9
 94.85
 NB 87.97
 47.2
 93.28
  Out of the 4521 instances in the
 dataset, the instances classified correctly and instances classified
 incorrectly for 
each model are
 as shown in Table 4. MLPNN c
lassified 4007 
instances correctly wher
eas NB classified 3977 
instances correctly.
 Table 4. Classification of 4521 instances in
 the dataset.
 Classifier
 Correctly classified instances
 Incorrectly classified instances
 MLPNN
 4007 514 NB 3977 544 8. Conclusion and Future Scope
 In this paper we propose an efficient CRM
-data mining framework for the prediction of customer behaviour. 
Two classification models were used to predict the customer behaviour. In order to arrive at authentic research 
731 T. Femina Bahari and M. Sudheep Elayidom  /  Procedia Computer Science   46  ( 2015 )  725  731 
results it is always better to use standard bench marking datasets like UCI datasets. Hence we used the same in this 
work. The best model that achieves high predictive performance was MLPNN with accuracy rate of 88.63%. We 

also compared the performance of classifiers in terms of accuracy, sensitivity and specificity. This work can be 
extended to other new models like Neuro fuzzy classifiers, Ensemble
 of classifiers and so
 on. Also the same 
experimental set up can be applied to other huge live banking datasets.
 References
 1. Berson 
 A, Smith S, Thearling K. Building data mining applications for CRM,
 McGraw
-Hill; 2000.
 2. Cortez P.
 Data  Mining  with  Neural  Networks  and Support  Vector  Machines  using  the  R/rminer  Tool,  
In Proceedings  of  the  10th
  Industrial  Conference  on  Data Mining
, Germany: Springer; 2010. p. 572
583. 3. EWT Ngai. Customer relationship management research (1992
2002): An
 academic literature review and classi
cation, 
Marketing 
Intelligence, Pl
anning;  
23, 2005.  p. 582
605. 4. EWT Ngai, L Xiu, DCK.Chau.
 Application of Data Mining Techniques in Customer Relationship Management: A Literature Review on 
Classification, 
Expert Systems with Applications; 
36- 2, 2009.  p. 2592
-2602. 5. Hany AE.  Bank Direct Marketing Analysis of Data Mining Techniques, 
International Journal of Computer Applications;  
85-7, 2014.
 6. JW Han  M Kamber.
 Data mining  concepts  and techniques,  2nd  ed.
 Morgan  Kaufmann, San Francisco, CA;
 2006. 7. Ling, R., Yen D. Customer relationship management: An analysis framework and implementation strategies, 
Journal of Computer Information
 Systems
; 41, 2001.  p. 82
97. 8. Mi
 tra  S, Pal
 SK, Mitra 
 P. Data mining
 in soft computing framework: A survey, 
IEEE Transactions on Neural Networks
; 13, 2002. p.
 314. 9. MJA Berry, G
S Linoff, Data  Mining Techniques: For  marketing, Sales  and Customer Relationship  Management,
 Indianapolis: Wiley; 2004.
 10. S Moro, P Cortez, P
 Rita
. A data
-driven approach to predict the success of bank telemarketing, 
Decision Support Systems; 
62, 2014. p. 23
-31. 11. S Moro, R Laureano, P Cortez. Using Data Mining for Bank Direct Marketing: An Application of the CRISP
-DM Methodology,             
         
Proceedings of the European Simulation and Mode
lling Conference
; Portugal, 2011 . p. 117
-121. 12. Swift RS. Accelerating customer relationships: Using CRM and relationship technologies, N.J: Prentice Hall PTR;
 2001 13. T
 Munkata. Fundamentals 
 of  new  artificial intelligence, 2nd  ed. London: Springer
-Verlag;
 2008. 14. Tom M Mitchell. Machine Learning, 2nd  ed. McGraw Hill; 2010.
 15. Turban E, Aronson JE, Liang TP,
 Sharda 
 R. Decision support and business intelligence systems,
 8th 
ed.Pearson Education; 2007
. 16. Witten I, Frank
 E. Data Mining 
 Practical Machine Learning Tools and Techniques, 2nd ed. USA: Elsevier;
 2005.       
   
    Procedia Computer Science   85  ( 2016 )  862  870 
1877-0509  2016 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license 
(http://creativecommons.org/licenses/by-nc-nd/4.0/
).Peer-review under responsibility of the Organizing Committee of CMS 2016
doi: 10.1016/j.procs.2016.05.276 ScienceDirect
Available online at 
www.sciencedirect.com
 International Conference on Computati
onal Modeling and Security (CMS 2016) 
Analysis of Data Mining Techniqu
es For Healthcare 
Decision Support System Using Liver Disorder Dataset
 Tapas Ranjan Baitharu
a, Subhendu Kumar Pani
b* aOrissa Engineering College,BPUT,Bhubaneswar,752050,India 
bOrissa Engineering College,BPUT,Bhubaneswar,752050,India 
 Abstract 
Accuracy in data classification depends on
 the dataset used for learning. Now-a-days the most important cause of 
death for both men and women is due to the Liver Problem. The hea
lthcare industry collects a huge amount of data 
which is not properly mined and not put to the optimum 
use. Discovery of these hidden patterns and relationships 
often goes unexploited. Our research focuses on this asp
ect of Medical diagnosis by learning pattern through the 
collected data of Liver disorder to dev
elop intelligent medical decision support systems to help the physicians. In 
this paper, we propose the use decision trees J48, 
Naive Bayes, ANN, ZeroR, 1BK and VFI algorithm to classify 
these diseases and compare the effectiv
eness, correction rate among them. Dete
ction of Liver disease in its early 
stage is the key of its
 cure. It leads to better performance of the 
classification models in terms of their predictive or 
descriptive accuracy, diminishing of computing time 
needed to build models as they learn faster, and better 
understanding of the models. In this paper, a comparative analysis of d
ata classification accuracy using Liver 
disorder data in different scenario
s is presented. The predictive performanc
es of popular classif
iers are compared 
quantitatively.  
  2015 The Authors. Published by Elsevier B.V. 
Peer-review under responsibility of organizing committee of th
e 2016 International Conference on Computational Modeling and 
Security (CMS 2016). 
Keywords:
 Classification; Data Mining;J48; Naiv
e Bayes; Artificial Neural Network; 1BK; VFI;
  Corresponding author. Tel.: 91-9776503280; fax: 06758-239723. 
E-mail address:
skpani.india@gmail.com 
 2016 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license 
(http://creativecommons.org/licenses/by-nc-nd/4.0/
).Peer-review under responsibility of the Organizing Committee of CMS 2016
863 Tapas Ranjan Baitharu and Subhendu Kumar Pani  /  Procedia Computer Science   85  ( 2016 )  862  870 
1. Introduction 
Data and information have become major assets for most of the organ
izations [1,4]. The success of any organization 
depends largely on the extent to whic
h the data acquired from business oper
ations is utilized. In other words, the 
data serves as an input into a strategic decision making 
process, which could put the business ahead of its 
competitors . Also, in this era, wher
e businesses are driven by the custom
ers, having a customer database would 
enable management in any organization to determine custom
er behavior and preference in order to offer better 
services and to prevent losing them resulting better busin
ess. In this research work, J
48, Naive Bayes, ANN, ZeroR, 
1BK and VFI algorithm classifier algorithms are used for 
liver disease prediction. There are various numbers of 
liver disorders that required clinical care of
 the physician [3]. The main objective of this research work is to forecast 
liver diseases such as Cirrhosis, Bile 
Duct, Chronic Hepatitis, Liver Cancer and Acute Hepatitis from Liver Function 
Test (LFT) dataset using above classification algorithms[2]. 
The liver is the second largest inside organ in the human
 body, playing a key role in metabolism and serving several 
imperative functions, e.g. Decomposition of red blood ce
lls, etc.Its weight is around three pounds. The liver does 
many essential functions related to digestion, metabolism, 
immunity, and the storage of nutrients within the body. 
These functions formulate the liver as an important organ, 
without this, body tissues would rapidly die from lack of 
energy and nutrients. There are number of fa
ctors which boost the risk of liver disease. 
Data mining is regarded as an emerging technology that has 
made radical change in the information world. The term 
`data mining' (often called as knowledge discovery) refe
rs to the method of analyzing data from different 
perspectives and summarizing it into va
luable information by means of a numb
er of analytical tools and techniques, 
which in turn may be useful to increase the performance of a system. Technically, 
data mining is the method of 
finding correlations or patterns among doz
ens of fields in large relational databases
. Therefore, data mining 
consists of key functional elements that transform data on
to data warehouse, manage data in a multidimensional 
database, facilitates data access 
to information professionals
 or analysts, analyze d
ata using application tools and 
techniques, and meaningfully present data to
 provide useful information [5,6]. 
The paper is divided into five sections. Section 2 descr
ibes the main techniques and algorithms associated with 
data mining. In Section 3, we provide the results. We provid
e descriptive results. In this 
section we also compare the 
predictive power of the classifiers. Finally, in Section 4, we 
draw some conclusions from our results and offer some 
directions for future research. 
 2. Techniques and Algorithms 
Researchers find two important goals of data mining: pred
iction and description. First, th
e Prediction is possible by 
use of existing variables in the database in order to 
predict unknown or future values of interest. Second the 
description mainly focuses on finding patterns descr
ibing the data the subseq
uent presentation for user 
interpretation. The relative emphasis of both prediction and 
description differs with respect to the underlying 
application and technique. 
Classification: Classification is the most commonly app
lied data mining technique, which employs a set of pre-
classified examples to develop a model that can classify th
e population of records at large. 
Fraud detection and credit 
risk applications are particularly well suited 
to this type of analysis. This approach frequently employs decision tree 
or neural network-based classification algorithms. 
The data classification process involves learning and 
classification. In Learning the training data are analyzed 
by classification algorithm. In classification test data are 
used to estimate the accuracy of the classification rules. If th
e accuracy is acceptable the 
rules can be applied to the 
new data tuples. For a fraud detection application, this wo
uld include complete records of
 both fraudulent and valid 
activities determined on a record-by-
record basis. The classifier-training 
algorithm uses these pre-classified 
examples to determine the set of parameters required fo
r proper discrimination. The algorithm then encodes these 
parameters into a model called a classifier. 
Some well-known classification models are 
  864   Tapas Ranjan Baitharu and Subhendu Kumar Pani  /  Procedia Computer Science   85  ( 2016 )  862  870 
2.1 .Decision trees J48  
 
 J48 [14] is an important de
cision tree classifier. Decision tree is a 
predictive machine-learning representation that 
makes decisions the target value (dependent variable) of 
a fresh sample based on various attribute values of the 
available data. The internal 
nodes of a decision tree indicate the differe
nt attributes, the branches between the nodes 
gives the probable values that these attributes can have in 
the observed samples, while th
e terminal nodes generates 
the final value (classification) of the dependent variable. 
The attribute that is to be 
predicted is recognized as the 
dependent variable, since its value depe
nds upon, or is chosen by, the valu
es of all the other attributes. The other 
attributes, which help in predicting the value of the depend
ent variable, are known as th
e independent variables in 
the dataset.Figure1 shows the decision tree J48 
implementation using Liver Disorder Dataset. 
   Figure 1 :
 J48 Tree visualizer
  2.2. Naive Bayes  

[13]. A Nave Bayesian model is very simple to build and can be implemen
ted for very huge datasets. Naive 
Bayesian classifier often achieves well than more s
ophisticated classification techniques. The posterior probability, 
P (c|x) is computed from P(c), P(x), and P (x|c). The effe
ct of the value of a predictor (x) on a given class (c) is 
independent of the values of other predictors. This ass
umption is named class conditional independence [9,10]. 
    2.3. Multilayer perceptron 
 A multilayer perceptron (MLP) is a feed forward artificial 
neural network model that charts sets of input data onto a 
set of appropriate outputs. An MLP composes of multiple layers
 of nodes in a directed graph, with every layer fully 
865 Tapas Ranjan Baitharu and Subhendu Kumar Pani  /  Procedia Computer Science   85  ( 2016 )  862  870 
connected to the next one. Except for the input nodes, 
every node is a neuron (or processing element) with a 
nonlinear activation function. MLP uses a supervised lear
ning technique called back propagation for training the 
network. MLP is a alteration of the 
standard linear perception and can di
fferentiate data that are not linearly 
separable [11, 12]. 
If a multilayer perceptron has a linear activation functio
n in all neurons, that is, a linear function that charts 
the weighted inputs to the output of each neuron, then it is simply proved with linear algebra that any number of 

layers can be reduced to th
e standard two-layer input-output model. What 
makes a multilayer perceptron different is 
that some neurons use a nonlinear activation function wh
ich was developed to model the frequency of action 
potentials, or firing, of biological 
neurons in the brain. This functi
on is modeled in several ways. 
 This function is modelled in several ways. 
 2.4 .ZeroR 
 ZeroR is the easiest classification me
thod which depends on the target and 
ignores all predictors. ZeroR classifier 
basically predicts the majority category (
class). Even though there is no predictab
ility power in ZeroR, it is helpful 
for determining a baseline performance as a benchmark for other classification methods. A frequency table is 
created for the target and the most frequent value is selected
. 2.5 .1BK 

KNN (K- Nearest Neighbor) classifier alternatively known as IBK, a supervised 
learning algorithm, where a given 
data set is divided into a user specif
ied number of clusters. Predict the same 
class as the adjacent instance in the 
training set. Training stage of the cla
ssifier keeps the features and the 
class label of the training sets. New objects are 
classified based on the voting criteria. It gives the maxi
mum likelihood estimation of the class. Euclidean distance 
metrics is applied for assigning objects to the most fr
equently labeled class. Distances are computed from all 
training objects to test object using appr
opriate K value. It constructs the decisi
on tree from labeled training data set 
using information gain and it observes the same that result
s from choosing an attribut
e for splitting the data. To 
build the decision the attribute with maximum normalized in
formation gain is used. Then
 the algorithm recurs on 
smaller subsets. The splitting procedur
e ends if all instances in a subs
et belong to the same class. Then the leaf node 
is built in a decision tree
 telling to choose that class
. 2.6. VFI algorithm 
It is a very easy algorithm. Let us suppose there are 'm' no
. of features and 'n' no. of classes involved. As every other 
classification algorithm, this classifier predicts a class as its fin
al output. In this technique, each feature participates 
in the classification. Each feature presents a vote for one 
of the classes out of the n available classes. So for each 
class there are a total of m votes(1/feature
).The class with the highest votes is af
firmed to be the predicted class.  
VFI is a unexpectedly very fast algorithm
 and performs very well in most of 
the cases; even better
 than the rest of 
the classifiers in some cases
. 2.7.Margin Curve 
The margin is described as the difference between th
e probability predicted for the actual class and the highest 
probability predicted for the other classes. One hypothesis as to
 the fine performance of boosting algorithms is that 
they boost the margins on the training data and 
this gives better performance on test data. The margin 
curves for J48,ZeroR, Multilay
er Perceptron, 1BK, Naive 
Bayes, VFI are shown in  figure2,figure-3,figure-4,figure-5 , figure-6,figure-7.
          
 
 
 
   
 866   Tapas Ranjan Baitharu and Subhendu Kumar Pani  /  Procedia Computer Science   85  ( 2016 )  862  870 
 Figure2: Margin Curve of J48                                             
    
 
 Figure3 :Margin Curve of ZeroR 
 
 
 
    867 Tapas Ranjan Baitharu and Subhendu Kumar Pani  /  Procedia Computer Science   85  ( 2016 )  862  870 
  Figure 4:Margin Curve of Multilayer Perceptron               
    Figure5:Margin Curve of 1BK 
                    
  868   Tapas Ranjan Baitharu and Subhendu Kumar Pani  /  Procedia Computer Science   85  ( 2016 )  862  870 
  Figure 6 :Margin Curve of Naive Bayes 
    Figure7 :Margin Curve of VFI 
 869 Tapas Ranjan Baitharu and Subhendu Kumar Pani  /  Procedia Computer Science   85  ( 2016 )  862  870 
3.Experimental Study and Analysis 
3.1 WEKA Tool  
We use WEKA (www.cs.waikato.ac.nz/ml/weka/), an open so
urce data mining tool for our experiment. WEKA is 
developed by the University of Waik
ato in New Zealand that implements data mining algorithms using the JAVA 
language. WEKA is a state-of-the-art tool for developing machine learning (ML) techniques and their application to 

real-world data mining problems. It is a collection of 
machine learning algorithms for data mining tasks. The 
algorithms are applied directly to a dataset. WEKA implements algorithms for data pre-processing, feature 

reduction, classification, regression, clustering, and associatio
n rules. It also includes visualization tools. The new 
machine learning algorithms can be used with it and exis
ting algorithms can also be extended with this tool. 
3.2 Classifier Selection  

We select six commonly used classifier for prediction classification in our work based on their qualitative 
performance. These classifiers are briefly 
described and their performance is anal
yzed in are given below in Table 1. 
 Table 1. Performance analysis of different Classifiers using Liver Disorder Dataset. 
Data Mining Algorithms 
Kappa Statistics 
Mean absolute 
Error 
Root Mean 
squared Error  
Relative 
absolute error 
Accuracy
 J48 
0.3401 
0.3673 
0.5025 
75.3511 
68.97 
ZeroR 
0 0.4874 
0.4936 
100 
57.971 
Multilayer Perceptron 
0.4023 
0.3543 
0.4523 
72.68 
71.59 
1BK 
0.2401 
0.3718 
0.6072 
76.2906 
62.8986 
Naive Bayes 
0.153 
0.4597 
0.5083 
102.9673 
55.3623 
VFI 
0.1044 
0.4889 
0.4955 
100.3839 
60.2899 
        3.3 Results Analysis We run selected classifiers in different 
scenarios of the dataset. By analyzing 
the results, Multilayer perceptron  
gives the overall best classificatio
n result than other  classifiers. : Accuracy Comparison of different Classifiers is 
shown in figure-8. 
   
Figure8: Accuracy Comaprision of different Classifiers
  4. Conclusion  
An experiment is conducted to get the 
impact of liver disorder on the predictiv
e performance of different classifiers. 
We select six popular classifiers considering their qualitative
 performance for the experiment. After analyzing the 
870   Tapas Ranjan Baitharu and Subhendu Kumar Pani  /  Procedia Computer Science   85  ( 2016 )  862  870 
quantitative data generated from the computer simulations, we find that the gener
al concept of improved predictive 
performance of all above classifiers but Naive Bayes perf
ormance is not significant. However more experiments 
with different datasets are required to support the findings
. Classification is the major data mining technique which 
is primarily used in healthcare sectors for medical d
iagnosis and predicting diseases
. This research work used 
classification algorithms for liver disease prediction. Comparison
s of these algorithms are don
e and it is based on the 
performance factors classification accuracy and execution time.  
References 
 1. Klosgen W and Zytkow J M (eds.), Handbook of data mining and knowledge discovery, OUP, Oxford, 2002.  
2. Provost, F., & Fawcett, T., Robust Classification for Imprecise Environments. Machine Learning, Vol. 42, No.3, pp.203-231, 2
001.  
3. Larose D T, Discovering knowledge in data: an introduction to data mining, John Wiley, New York, 2005.  

4. Kantardzic M, Data mining: concepts, models, methods, and algorithms, John Wiley, New Jersey, 2003.  
5. Goldschmidt P S, Compliance monitoring for anomaly detection, Patent no. US 6983266 B1, issue date January 3, 2006, Availabl
e at: 
www.freepatentsonline.com/6983266.html  
6. Bace R, Intrusion Detection, Macmillan Technical Publishing, 2000.  
7. Smyth P, Breaking out of the BlackBox: research challenges in data mining, Paper presented at the Sixth Workshop on Research
 Issues in Data 
Mining and Knowledge Discovery (DMKD2001), held on May 20 (2001), Santra Barbara, California, USA.  
8. Agrawal R. and Srikant R. Fast Algorithms for Mining Association Rules. In M. Jarke J. Bocca and C. Zaniolo, editors, Procee
edings of the 


486, Santiago de Chile, Chile, Sept 1994 . MK  
9. J. Su, H. Zhang, C.X. Ling, S. Matwin, Discriminative parameter learning for Bayesian networks, in: Proceedings of the Twent
y-Fifth 
International Conference on Machine Learning, ACM Press, Helsinki, Finland, 2008, pp. 1016
1023. 
10.A.A. Balamurugan, R. Rajaram, S. Pramala, et al., NB+: An improved Nave Bayesian algorithm, Knowledge-Based Systems 24 (5) 
(2011) 
563
569. 
11. Stanczyk, U.. Establishing relevance of characteristic features for authorship attribution with ANN. In: Decker, H., Lhots
ka, L., Link,S., 
Basl, J., Tjoa, A., editors. Database and Expert Systems ppllications; vol. 8056 of Lecture Notes in Computer Science. Springer
 Berlin 
Heidelberg; 2013, p. 1
8. 12. Stanczyk, U.. Rough set and artificial neural network approach to computational stylistics. In: Ramanna, S., Jain, L.C., H
owlett, R.J.,editors. 
Emerging Paradigms in Machine Learning; vol. 13 of Smart Innovation,
 Systems and Technologies. Springer Berlin Heidelberg;2013, p.
 441
470. 
13.Stanczyk, U.. Decision rule length as a basis for evaluation of attribute relevance. Journal of Intelligent and Fuzzy Syste
ms 2013;24(3):429
445. 

14. Farid, D.M., Zhang, L., Rahman, C.M., Hossain, M., Strachan, 
R.. Hybrid decision tree and Naive Bayes classifiers for multi
-class 
classification tasks. Expert Systems with Applications 2014; 41(4, Part 2):1937
1946.
    Big Data Based Extraction of Fuzzy Partition Rules for Heart Arrhythmia Detection: a Semi-Automated ApproachOmar Behadada
1, Marcello 
Trovati
2, Chikh MA
1, Nik Bessis
2  1 Biomedical Engineering Laboratory, Department of Biomedical Engineering, Faculty of technology, 
University of 
Tlemcen
, Algeria
 2School of Computing and Mathematics, University of Derby, UK
 o_behadada@mail.univ
-tlemcen.dz
; M.Trovati@derby.ac.uk
; mea
_chikh@mail.univ
-tlemcen.dz
; N.Bessis@derby.ac.uk
    Abstract 
 In this paper, we i
ntroduce a novel method to 
define 
semi
-automatically
 fuzzy partition rules to provide a 
powerful and accurate insight into cardiac 
arrhythmia. In 
particular, we define a tex
t mining approach applied to a large 
data
-set consisting
 of 
the freely available 
scientific papers 
provided by PubMed
. The information extracted is then 
integrated with expert knowledge, as well as experimental 
dat
a, to provide a robust, scalable, and accurat
e system
, which 
can successfully address the challenges posed by the 
manage
ment
 and assess
ment of big d
ata in the medical sector.
 The
 evaluation we carried out shows
 an accuracy rate of
 93% 
and interpretability 
of 0.646
, which 
clearly shows
 that our 
method provides an excellent balance between accuracy and 
system transparency.  Furthermore, this
 contributes 
substantially 
to the knowledge discovery
 and offers a powerful 
tool to facilitate
 the 
decision making
 proce
ss. Keywords: 
Knowledge Discovery, 
Text Mining
, Fuzzy Logic, 
Cardiac Arrhythmia, 
Big Data, 
Data Analytics
 I. INTRODUCTION
 Cardiovascular diseases are one of the most 
worrying
 health 
issues,
 and the largest cause of mortality in
 the world, based 
on the 
World Health Report 
2013 [1]
. Thus, low
-cost and
 high
-quality c
ardiac assessment offers a very valuable
 challenge.
 Furthermore, the availability of a huge amount of 
information 
created by the c
ontinuously development of big 
data
 methods and techniques, 
provides new challenges as 
well as new opportunities in this field.
     The detection of cardia
c arrhythmia is a 
very interested
 area, 
since 
premature ventricular contraction 
(PVC) 
is a
n effective
 predictor of sudden death. 
Several
 studies, over the 
past decade have focused on methods and algorithms for 
detection and significance of cardiac arrhythmias, aiming to 
achieve a good classification rate.
     More specifically, 
many classification methods 
have been
 used in 
the field
 such
 as Bayesian classifiers
, decision trees, 
neural
, and 
rule bas
ed learners
 [2]
. However, the
 classification methods 
with
 good classification rate
s usually 
have
 a low degree of interpretability 
preventing
 the user 
(such as a 
cardiologist)
 from fully taking advantage of such 
method.
     The interpretability
 of 
any 
knowledge
 based
 system is 
crucial
, especially when dea
ling with big data applications
 [3]
. In fact, 
this ensures an effective decisi
on making 
progress 
by producing
 interpretable knowledge
, which can 
easily be maintained and assessed
. The acquisition of
 expe
rt 
knowledge is a complex
, yet essential 
task 
due to its 
inherent 
accuracy, if ca
rried out by human intervention. 
However 
this tends to be 
inefficient
 when dealing with big 
data
-sets and
 furthermore, such knowledge contains an 
unconscious component
, which is hard
 to 
formalis
e [4]
. Alternatively, 
knowledge can also be defined
 by analysing 
information extracted from 
experimenta
l big 
data, which is 
likely to provide an accurate insight into
 the different 
parameters
 [5]
. In particular
, there is a wealth 
of 
algor
ithms 
and machine
-learning 
techniques fo
r model identification
, which are based on the properties of accuracy indices, 
which can be applied to the knowledge induction process
 [6]
.      Another valuable source of knowledge i
s based on 
articles and texts published in scientific journals, which 
include the most critical knowledge
, and in which many 
experts share their results, analysis, and experiences. 
However, since such textual information is typically very 
large,
 if not hug
e, scientists are faced with great amount of 
information
, which poses a huge computational and 
implementation challenge
. In modern medicine, large 
amounts of data are generated, but there is a widening gap 
between data acquisition and data comprehension. I
t is often 
impossible to process all of the data available and to make a 
rational decision on basic trends. Thus, there is a growing 
need for intelligent data analysis
, such as data mining
, to 
facilitate the creation of knowledge to support clinicians in 
decision
 making
. In fact, data mining approaches can be 
used in such databases, to improve classification tasks
.     In this paper, we i
ntroduce a novel method to semi
-automatically identify fuzzy partition rules applied to 
cardiac arrhythmia detection, whi
ch combines an automated 
information extraction from textual sources with expert 
elicitation to create a robust, scalable, and accurate 
knowledge based system
, which provides a crucial insight 
into arrhythmia detections
 from big data information 
sources
.     The paper is structured as follows: in 
the rest of this 
section
 we discuss the relevant medical background, in 
Section 
III we introduce the text mining method to extract 
information for the generation of fuzzy partitions, which are 
subsequently analysed and evaluated in Sections
 IV, V, VI and
 VII
. Finally in Section
 VIII
, we discuss the main 
findings and future research directions
. A. Medical Context
     Electrocardiogram (ECG) reflects 
the 
activity of the 
central blood circulatory system
, wh
ich can provide 
extensive
 information on the normal and pathological 
physiology of heart activity. 
See Figure 1 for an example of 
the main features of ECGs. As a consequence
, it is an 
important non
-invasive clinical tool for the diagnosis of 
heart diseases
 [7]
. Early and quick detection and 
classification of ECG arrhythmia is important, especially for 
the treatment of patients in the intensive care unit
 [8]
. For 
over
 four decades, computer
-aided diagno
stic (CAD) 
systems have been used in the classification of the ECG 
resulting in a huge variety of techniques. Included in these 
techniques are multivariate statistics, decision trees, fuzzy 
logic, expert systems and hybrid approaches
 [8]
. In 
designing of CAD system, the most important step is the 
integration of suitable feature extractor and pattern classifier 
such that they can operate in coordination to make an 
eff
ective and efficient CAD system 
[9]
.  B. Big Data
 in Medical Information Retrieval
      The 
medical sector
 has
 always 
generated large amounts 
of data, 
based on
 patient record, regulatory requirements, 
etc. 
[10]
. All the information 
created
 by the above data 
provides a 
valuable opportunity to further improve
 the state
-of-the
-art tools available to clinicians
, as well as
 to provide 
scalable, robust and efficient methods to extract, assess and 
manage
 such informati
on [11]
.     Furthermore, the digitalisation of medical data
-sets with 
the implementation of big data techniques, has created 
further
 benefits
. These
 include 
the detection of
 diseases at 
earlier stages 
resulting to 
more 
effective and successful 
treatments, as well as the management of
 specific individual 
and population 
medical information
. Specific scenarios can 
also 
be predicted and
 estimated based on 
large
 amounts of 
historical data,
 combined with real
-time information 
to 
determine and assess their crucial properties
 [12]
.    Figure 
1: Standard ECG beat
 II. DESCRIPTION OF THE 
METHOD
   Figure 
2: Overall structure of the knowledge extraction 
process
  The method we are proposing, and in particular the 
overall structure of the extraction process from 
expert 
knowledge, data, 
and
 textual information is depicted in
 Fig
ure
 2. More specifically, f
uzzy logic as modelling platform 
allows us to merge an
d manage
 those three types of 
knowledge, where the 
fuzzy
 partition design
 aims
 to define 
the most 
influential variables, according 
to the above 
knowledge. 
An important part of this process is the 
rule
 base 
definition and integration, where
 the 
expert is 
invited to 
make a des
cription of the system behaviour, expressing 
his/her system knowled
ge as linguistic rules (
expert rules
). Furthermore
, rules are in
duced from data (
induced rules
) according
 to the common universe of
 fuzzy partition. Both 
types of rules
 use the same linguistic terms de
fined by the 
same fuzzy sets. As a consequence,
 rule comparison can 
be 
done at the linguistic level and subsequently, 
both 
types
 of 
rules are merged into a unique knowledge base.
 As part of 
the process, the expert can provi
de complete or 
partial 
information 
about the linguistic variables. Additionally, 
several algorithms 
can be used to create fuzzy partitions 
from
 data, or 
induced p
artitions
, and linguistic constraints 
are superimposed to
 the fuzzy partition definition, 
in o
rder to 
ensure their interpretability
. The result is the definition of a
 common universe for each of th
e variables, according to both 
expert knowledge and 
data distribution.
  III. AUTOMATED 
EXTRACTION OF 
FUZZY 
PARTITION 
RULES 
FROM 
TEXT
  Text mining
 (TM)
 [13]
 is a branch of computer science
, which aims to accurately extract, identif
y and 
analyse
 information and semantic properties from text sources. Even 
though there has been steady and successful progress in 
addressing the above challenges, TM research is still very 
much expanding to provide further state
-of-the
-art tools to 
improve accuracy, sc
alability and exibility
. T
he extraction 
of information from textual sources is typically a complex 
task
 due to the ambiguous nature of human la
nguage. 
In 
fact, d
epending of the general context and the given 
semantic information, a variety o
f t
ext mining t
echniques 
can be used, which in general depend
 on the type of data 
and their stru
cture
 [13]
.  In this paper, we apply a g
rammar based text extraction, 
based on
 text patterns
, which relies on a set 
of rules 
identifying sentences 
with a determined structure. More 
specifically, we consider text patt
erns of the form 
(NP, verb, NP), where 
NP refers to the noun phrases, and 
verb to the linking
 verb
 [13]
. For example, sentences such 
as 
PVCs can be related to
 electrolytes
 are identified to 
extr
act a relationship between 
PVCs
 and 
electrolytes
. The 
effectiveness of this approach is fully exploi
ted when 
syntactic properties of a sentence are investigated, by using 
suitable parsing techn
ology
 [14]
. In particular, the syntactic 
roles of the different phrasal components are essential in 
extracting the relevant information, and they can contribute 
towards a full understanding of the type of relationship. 
Furthermore, we also 
apply 
basic 
sentimen
t analysis
, which 
aims to identify the m
ood described by text fragments based 
on specific key
words
 [15]
. Table 1 shows a small selection 
of such keywords used in our approach. We 
mined
 all 
the 
articles in 
journals 
freely available from
 PubMed
 [16]
, a very 
large data base containing biomedical literature
, as follows:
   We identified articles from the above journals 
containing the 
following keywords:
 o Premature ventricular contractions
, or 
PVCs
 o Prem
ature ventricular complexes
 o Ventricular premature 
beats
 o Extrasystoles
  The identified articles were f
irst lexically and 
syntact
ically 
anal
ysed via the Stanford Parser 
[14]
.  Subsequently, a
 grammar
-based extraction 
ident
ifies the relevant information based on the 
above keywords as well as on sentiment ana
lysis 
[14]
. More speci
fically, only sentences with one or 
more of the above keywords, including those in 
Table 1, in the 
NPs will be extracted
.      Table 
1: A selection of keywords used
 Negative 
Keywords
 Positive 
Keywords
 Uncertain 
Keywords
 Bad
 Satisfactory
 Unpredictable
 Negative
 Enhancing
 Possible
 Underestimate
 Advantage
 Somewhat
 Unsafe
 Benefit
 Precautions
 Unwelcome
 Good
 Speculative
 Tragic
 Excellent
 Confusing
 Problematic
 Great
 Fluctuation
    A. Text 
Mining Extraction Results
 Table 
2: Example of relation extraction
 Keywords in Relation 
Extraction
 Sentences iden
tified
 'PVC', 'PVCs', 'imbalances'
 'PVCs can be related to 
electrolyte or other metabolic 
imbalances'
 'PVC', 'death
' '70 
mmol
/L and T2DM 
significantly increases risk of PVC 
and sudden cardiac death, the 
association between sMg and PVC 
may be modified by diabetic status'
 'premature ventricular 
complexes', 'PVC', 'PVCs', 'atrial', 
'ventricular', 'beat', 'missed', 
'premature'
 'The
 system 
recognises 
ventricular escape beats, premature 
ventricular complexes (PVC), 
premature supraventricular 
complexes, pauses of 1 or 2 missed 
beats, ventricular bigeminy, 
ventricular couplets (2 PVCs), 
ventricular runs (&#x0003e; 2 
PVCs), ventricular t
achycardia, 
atrial fibrillation/flutter, ventricular 
fibrillation and asystole'
          The output of the extraction consists of the set of 
keywords found in each text fragment (usually a sentence)
, which was also extracted, see Table 2 for an example. 
A full 
assessment of this type of inf
ormation extraction from text 
goes
 beyond the scope of this paper,
 since it specifically 
addresses issues that are not directly relevant i
n this context. 
However, we 
considered a small evaluation consisting of two 
random
ly chosen p
aper
s [17]
 [18]
, from those identified 
above. The automatic extraction was then compared with a 
manual o
ne, which produced a recall of 71% and a precision 
of 84
%. In future research, we will investigate a
 more 
specialised set of keywords, as well as a larger set of text 
patterns compared to those utilised in this paper, to improve 
the above measures
.  IV.
 DATA PREPARATION
     The 
patients
 who have been
 considered in 
the 
experiments, 
tak
en from MI
T-BIH
 [19]
, are shown in
 Table 
3. The R peaks of the ECG signals were detected using the 
Tompkins algo
rithm 
[20]
, which is an online real time QRS 
detection algorithm. Th
is algorithm reliably detects QRS 
complex using slop, amplitude, and width information. This 
algorithm automatically adjust thresholds and parameters 
periodically to the standard 24
h MIT
-BIH arrhythmia 
database, this algorithm correctly detects 99.3 percen
t of 
QRS complex
.     From patients with cardiac arrhythmia, taken from MIT
-BIT database, we chose only patients with 
three
 conditions
, namely
 premature ventricular 
contraction beats (PV
C) 
premature arterial 
contraction beats (PA
C) and premature 
junctional
 contraction beats (
PJC), since they provide the 
best quality of records, 
and more specifically PVC 
is a
 predictive
 element
 of the 
CA sudden 
death
.     Table 
3: Evaluation data taken from the MIT
-BIH database.
 Record
 N A J V 101 1860 3 - - 103 2082 2 - - 104 163 - - 2 105 2526 - - 41 106 1507 - - 520 107 - - - 59 108 1739 4 - 17 109 - - - 38 111 - - - 1 112 2537 2 - - 113 1789 - - - 114 1820 10 2 43 115 1953 - - - 116 2302 1 - 109 117 1534 1 - - 118 - 96 - 16 119 1543 - - 444 121 1861 1 - 1 122 2476 - - - 123 1515 - - 3 124 - 2 29 47 200 1743 30 - 826 201 1625 30 1 198 202 2061 36 - 19 203 2529 - - 444 205 2571 3 - 71 207 - 107 - 105 208 1586 - - 992 209 2621 383 - 1   Figure 
3: Standard ECG beat
 Dataset:
 Class 
 Normal 
 PVC PAC PJC
 Number 
of  
samples 
 60190 6709 2130 83  A. Feature Selection
 The feature vector
, which is used for recognition of beats, 
has been selected as follows: 
  the R
-R interval
 of the
 beat 
RRp (calculated as 
the 
difference between the QRS peak of the present 
and previous beat), 
  the ratio 
!" RR1
-to-RRo (RRn is calc
ulated as the 
difference between the QRS peak of the present
 and following beat see Figure 3
), and
  the QRS width 
 (calculated according to the 
Tompkins algo
rithm 
[20]
).  In this way, each beat is sto
red as 3
-element vector. Table 4 
provides the most relevant parameters used in this type of 
analysis.
  Table 
4: The 
various descriptors
 Attributes
 Meaning
 RR precedent: RR0                                                  The distance between the peak of this beat R and R of the peak beat precedent RR next : RRn                                                          RRn between the peak the present R and beat the peak of R beat following QRS complex                                                           Beginning of the Q-wave and the end of the S wave Comp  The ratio RR0/RRs PP Pic to pic of the R wave of the QRS complex Energy  Energy of the QRS complex    V. FUZZY 
PARTITION 
DESIGN
 This 
sec
tion covers the left most parts, as depicted in
 Figure 2, i.e.
 those related to membership
 functions 
extraction from both 
the 
experts input
 and 
experimental 
data. 
Note that t
he initial step 
consider
s the extraction from 
the former
, and subsequently
 some approaches for 
membership function design from data
 are introduced
. When 
defining expert knowledge, we make the 
assumption
 that
 the linguistic var
iables of the system are 
sufficiently known.
 More specifically, experts may identify 
specific properties, such as
 a d
omain of interest within a 
physical range,
 who would be subsequently facilitated in the 
decision process
 by
 a g
iven, and 
possibly 
small, nu
mber of 
linguistic terms
.  In 
this paper
, as discussed above, 
we 
assume 
that a 
minimum information 
on membership 
funct
ion definition
 is available
, which includes
 the 
defini
tion of universes, number of terms, and, sometimes, 
prototypes of linguistic labels 
(modal po
ints)
 [21]
. Note that 
this is a reduced version of the interval estimation m
ethod
 [22]
, as 
the interval
 is reduced
 to a single point. 
Furthermore
, i
f additional information is provided by the
 expert
, this can be integrated into the system.
 The knowledge base is 
split
 into two main parts, the 
data 
base
 (DB) and the 
rule base
 (RB). 
In particu
lar, t
he 
former
 is defined by 
the description of the linguistic variab
les such 
as 
number, r
ange, gr
anularity, membership functions
, as 
well as 
their
 normalis
ation function
s. As discussed in
 [21]
, in most of the existing approach
es, which focus on the
 generation from data of the fuzzy partitions, the automatic 
design of the data base 
is one of the most important
 steps in 
the definition
 of the overall knowledge base
. However, the 
method we are proposing in this paper consists of r
ules that 
integrate
 expert as well as
 data
-based knowledge.
 The automatic generation of fuzzy partitions 
is based on
 the definition
 of
 the 
best
 shapes 
of the membership 
functions, 
in terms of the optimal
 number of linguisti
c terms 
in the fuzzy p
artitions, a
nd the location of the fuzzy sets 
within
 the universe of the variable.
 As discuss
ed in 
[23]
, in th
is paper we follow an approach
, which includes
: 1. A non
-supervised clustering
 process is performed 
to address the extraction of
 the DB from the 
available data set
 as part of the pre
liminary design,
 2. An 
embedded
 basic learning method
, which
 derives the 
DB.
      Simultaneous d
esign
 [23]
 is another method
, which 
cannot be successfully applied in this context.
 In fact,
 it 
usually 
generates
 a 
far
 more complex process 
in which
 the 
computational effort 
involved 
is 
not fully exploite
d since 
only the fuzzy partitions are considered. In addition, our 
interpretability 
requirements
 require
 some specific 
properties for the partitions. 
In fact,
 techniques generatin
g multidimensional clusters
 cannot be
 successfully
 applied, 
since 
only
 one
-dimensional membership functions
 are 
required. On the other hand
, it is 
feasible
 to
 apply
 a one
-dimens
ional 
optimis
ation technique if it includes some 
semantic constraints.
 In the case of embedded design, wh
en search techniques 
are used in the
 design 
of 
the 
DB, 
it is essential
 to 
include 
appropriate 
interpretability measures in the objective 
function, 
to provide suitable and optimal solutions
. These 
include
 measures o
f completeness, consistency
, compactness, or similarity
. Finally, a
t the end of the 
embedded 
design process, only fuzzy partitions are 
considered
, which will subse
quently lead to the creation of
 fuzzy rules
. A. Criteria for the Evaluation of Fuzzy Partitions
     The
 evaluation of fuzzy partitions 
is based on both
 linguistic properties and partitioning prope
rties
 [21]
. The 
former
 influences
 the shape of the fuzzy sets, 
as well as
 the 
relations between fuzzy set
s corresponding to
 the same 
variable. T
he 
latter
, on the other hand
, focus on 
the data 
from the partitions that have been generated, and more 
importantly, on 
the
ir leve
l of matching with
 the partitions 
derived from data
. This 
is not the case for linguistic 
properties since t
heir assessment 
does not involve data.
     In the r
emaining
 section, we will give a brief overview of 
some features 
of partitioning
, as introduc
ed in 
[21]
. In 
particular, similar to the above mentioned paper, we only 
focus on
 1) Methods based
 on the 
data 
distribution
, which excludes
 methods
 bas
ed on an input
-output relation.
 2) Methods 
typically 
applied in unsupervised clustering, 
and not in supervised clustering
. 3) The assignment of 
data
-elements
 to each of the 
item
 of 
the partition. 
 Using the same notation 
as in 
[21]
, let 
 be 
the degree of 
membership of 
the 
k-th element of the data set to the 
i-th 
eleme
nt of the fuzzy partition
. The partition coefficien
t is 
      and the partition entropy i
s   where 
 is the number of elements of the fuzzy partition, 
and 
n is the cardinality of the set of data. Furthermore,
 Chen 
[24]
 recently introduced the following index measure
        An efficient partition should mi
nimis
e the entropy and 
maximis
e the coefficient partition and the Chen index
 [21]
.   Table 
5: RRo fuzzy partition quality (3 labels)
 Partition 
 Partition 
Coefficient(max)
 Partition 
Entropy(min)
 Chen 
Index(max)
  HFP 
 0.77513
 0.33406
 0.77094
  Regular 
 0.69936
 0.47483
 0.74665
  K-means
 0.79948
 0.30842
 0.80794
  Expert 
&TM 
 0.78262
 0.32504
 0.78647
    Figure 
4: Fuzzy partition RR0 from K
-means algorithm
   Table 
6: RRs fuzzy partition quality (3 labels)
 Partition 
 Partition 
Coefficient(max)
 Partition 
Entropy(min)
 Chen 
Index
  HFP 
 0.77378
 0.33614
 0.76980
  Regular 
 0.69705
 0.47751
 0.74360
  K-means
 0.77121
 0.34723
 0.77941
  Expert 
&TM 
 0.78300
 0.32441
 0.78668
    Figure 
5: Fuzzy pa
rtition RRs from Expert & TM
   Table 
7: QRS fuzzy partition quality (3 labels)
 Partition 
 Partition 
Coefficient(max)
 Partition 
Entropy(min)
 Chen 
Index(max)
  HFP 
 0.76812
 0.34495
 0.76312
  Regular 
 0.66966
 0.50649
 0.70024
  K-means
 0.82214
 0.26975
 0.82707
  Expert 
&TM 
 0.84540
 0.22046
 0.82512
    Figure 
6: Fuzzy partition QRS from Expert & TM
   Table 
8: COMP 
fuzzy partition quality (3 labels)
 Partition 
 Partition 
Coefficient(max)
 Partition 
Entropy(min)
 Chen 
Index(max)
  HFP 
 0.68847
 0.45294
 0.67092
  Regular 
 0.71224
 0.45990
 0.75945
  K-means
 0.89473
 0.18398
 0.90993
  Expert 
&TM 
 0.81362
 0.29566
 0.82971
    Figure 
7: Fuzzy partition COMP from K
-means algorithm
  Table 
9: PP fuzzy partition quality (3 labels)
 Partition 
 Partition 
Coefficient(max)
 Partition 
Entropy(min)
 Chen 
Index(max)
  HFP 
 0.68847
 0.45294
 0.67092
  Regular 
 0.71224
 0.45990
 0.75945
  K-means
 0.89473
 0.18398
 0.90993
  Expert 
&TM 
 0.81362
 0.29566
 0.82971
    Figure 
8: Fuzzy partition PP
 from 
kmeans algorithm (x10
3)   Table 
10: Energy fuzzy 
partition quality (2 labels)
 Partition 
 Partition 
Coefficient(max)
 Partition 
Entropy(min)
 Chen 
Index(max)
  HFP 
 0.78374
 0.31852
 0.66240
  Regular 
 0.52352
 0.66896
 0.17696
  K-means
 0.77454
 0.33891
 0.67045
    Figure 
9: Fuzzy 
partition energy from HFP algorithm
  The most important part of the knowledge
-based s
ystems is 
the reasoning mechani
sm that induces decision rules. Since 
the fuzzy partition is 
at the c
ore
 of induction methods in 
fuzzy logic, 
in this paper 
we proposed to 
initialize the fuzzy 
partition
s by two approaches: the first, a purely automatic 
method of induction (
K-means, 
HFP
 and 
regular
) an
d the 
approach resulting from information extraction from textual 
sources, as discussed in Se
ction 
III, to compare b
etween 
the 
different
 methods
.      Subsequently
, we have established linguistic terms to 
construct
 the rules and modal point
. VI.
 RULE 
BASE 
GENERATION
  The process of generating rules from data is called 
induction
, which
 aims 
to produce general statements, 
expressed as fuzzy rules in our case, valid for the whole set, 
from partial observations. The observed output, for each 
sample item, is part of the training set
 allowing supervised 
training procedures. Many methods are availa
ble in the fuzzy 
logic
 liter
ature
 [25]
, but we are only interested in those ones
, which generate rules sharing the same fuzzy sets. Thus, we 
have chosen
 the 
methods
, which 
are implement
ed in 
Fispro 
[25]
 and they are used by KBCT
 [1]
, as they
 can be run with 
previously defined partitioning
.  A. Knowledge Base Accuracy
      In order to 
obtain
 an accuracy measure, 
we need to
 compare the inferred output 
with
 the observed one in 
a real 
system. In classification systems, the most common index is 
defined as the number of misclassified cases. We 
will consider the three following indices:
  Unclassified cases (
UC): Number of cases from 
data set that do not fire at least one 
rule with a 
certain degree
.  Ambiguity cases (
AC): Number of remaining cases 
for which the difference between the two highest 
output confidence levels is smaller than an 
established thres
hold (
AmbThres
). More 
specifically, 
we 
also have
:  o AC (Total
): All detected ambigui
ty cases. 
 o AC (Error):
 Only those ambiguity cases 
related to error cases (observed and 
inferred outputs are different). 
 o EC:
 Error cases. Number of remaining 
cases for which the observed and inferred 
output classes are different. 
  Data (TOTAL):
 The total number of instances in 
the dataset.
  Error cases (
EC): Number of remaining cases for 
which observed and inferred values are different.
     A good KB s
hould
 minimis
e all of them by offering an 
accurate (reducing EC), consistent (reducing AC) and 
complete (reducing UC) set of rules. They can be combined 
to 
define the next accuracy in
dex:
      Table 
11: Quality Measurements
 KB cv% ac acons  abt=0
 acfd
 micfd
 macfd
 me msce
 FDT1
 100 0.914
 0.908
 0.914
 0.865
 0 1 3 0.007
 FDT4
 100 0.424
 0.303
 0.424
 0.516
 0 1 3 0.125
 FDT
7 100 0.873
 0.87
 0.873
 0.448
 0 0.842
 3 0.054
 FDT
15 100 0.717
 0.704
 0.717
 0.704
 0.013
 1 3 0.013
 FDT14
 99.794
 0.939
 0.934
 0.939
 0.925
 0 1 3 0.007
 FDT2
 100 0.935
 0.928
 0.935
 0.701
 0 1 3 0.024
 FDT3
 100 0.318
 0.277
 0.318
 0.373
 0.005
 0.742
 3 0.068
 FDT5
 100 0.424
 0.303
 0.424
 0.468
 0 1 3 0.147
 FDT6
 100 0.930
 0.924
 0.930
 0.645
 0.002
 1 3 0.035
 FDT
9 100 0.618
 0.549
 0.618
 0.361
 0 1 3 0.075
                     VII.
 EVALUATION
 The evaluation was carried out over a variety of 
experimentations. More specifically, 
the 
suitable 
fuzzy 
partition
 was investigated
, and subsequently we induced the 
corresponding decision rules and calculated and assessed the 
quality criteria
 to measure the accuracy of each approach. 
Table 11 clearly shows that
 the best results 
corre
spond to 
 FDT
1, FDT2, FDT3, FDT5,
 FDT6, FDT9
, FDT4, 
FDT7, 
FDT14, and FDT15,
  which are based on different 
algorithms for induction and partitions
, with the following 
parameters:
  Coverage (cv%
): percentage of data samples from 
the selected dataset that fire at least one rule in the 
rule base with an activation degree higher than the 
pre
-defined Blank threshold (BT). 
  Accuracy (ac): percentage of data samples properly 
classified
  Accu
racy (
acons
): percentage of data samples 
properly classified. 
  Average Confidence Firing De
gree (
acfd
): mean 
value of the firing degree related to the winner rule 
for the whole dataset. 
  Minimum Confidence Firing De
gree (
micfd
): 
minimum value of the firing degree related to the 
winner rule for the whole dataset. 
  Maximum Confidence Firing De
gree (
macfd
): 
maximum value of the firing degree related to the 
winner rule for the whole dataset. 
  Max Error (
me): maximum difference betw
een the 
observed class and the inferred one.
  Mean Square Classification E
rror (
msce
)      FDT14 was
 generated by attributes resulting from the 
text mining extraction (RRs and QRS) integrated with K
-means and HFP algorithms, gave the best result w
ith a 
clas
sification rate 
of 
93.9
% and a 
0.646 interpretability 
value
. On the other hand, FDT15
, created by expert fuzzy 
partition with fuzzy decision tree induction algorithm, 
and it 
gave a lower classification rate and interpretability value of 
71.7% and 0.025 res
pectively.
     Considering FDT1 (regular fuzzy partition with fuzzy 
decision tree induction algorithm), FDT4 (K
-means fuzzy 
partition
 and
 fuzzy decision tree induction algorithm)
, and 
FDT7
 (regular fuzzy partition
 and
 fuzzy decision with 
Wang and Mendel in
duction algorithm)
, we noticed that
 the 
interpretability 
value 
was zero, this is clearly explained by 
the 
very large 
number of rules
.    A. Ana
lysis
 of 
Rules:
   Figure 
10: Rules of 
FDT14 (expert & text mining) and fuzzy decision 
tree
 algorithm.
  In this section
, we 
further 
discuss
 the 
FDT
14 results, which 
have been shown to be more accurate,
 with a knowledge 
base consisting of 
the 
data & expert
 parts
 as a system 
partition
.      More specifically, we successfully built
 a 
simplified 
and 
optimis
ed based knowledge, with 11 decision rules and 9 
induced rules from the database and 2 of the expert
. Figure 
10 depicts an
 example 
based on
 one sample from the data
-set
, which 
activates 
both rule
 8 and 
rule 
9 with 0.553 and 
0.447 distributed as 
Class 2.
                                 We also 
had an improvement of the
 interpretability a
s shown by
 the 
corresponding
 index
 of 0.646
, providing
 a very good compromise between th
e accuracy
, with value of 
0.939,
 and interpretability. Furthermore, this al
so y
ields
 Nauck's
 index
 [26]
  of 99.796
, defined by the product 
  Nauck
! s =  comp
 x part 
x cov
      where
  Comp
 represents the complexity of a classifier 
measured as the number of classes divided by the 
total number of premises. 
  Part
 stands for the average normalized partition 
index overall input variables. It is computed as the 
inverse of the number of labels min
us one (two is 
the minimum number of linguistic terms in a 
partition) for each input varia
ble. 
  Cov
 is the averag
e normalized coverage degree of 
the fuzzy partition. 
     Figure 
11: Inference rules 
      Tabl
e 12: 
Fingrams Measurement
  INDICES
 Values 
 Converge (%)
 99.796
 Accuracy 
 0.939
 Average Confidence Firing Degree
 0.925
 Total Rule Length
 36 Inferential Fired Rules (training)(Max)
 5 Inferential Fired Rules (training)(Average)
  2.342
 Inferential Fired Rules (training)(Min)
 1 Accumulated Rule 
Complexity
 11.213
 Interpretability I
ndex (
Fingrams)
 0.646
 Interpretability Index (HLIK)
 0.123 Naucks Index
 99.796
     VIII.
 CONCLUSION
      In this paper we have discussed a method to 
build a
 system
 based on
 rules
, using 
three
 sources
 of knowledge.
  The use
 of fuzzy logic
, as a platform
 for communication 
between
 the different sources of
 knowledge
, proves 
to be
 a 
successful
 solution
 to manage the
 fusion
 of knowledge
 in a 
database
 of common
 rules.
 The application of
 specific 
text
 mining
 methods
 in the extracti
on of knowledge
 from
 the
 large
 textual data
-set
s provided by
 PubMed
, has enabled
 an accuracy of
 93.9%, 
and interpretability index
 0.646
. This is 
clearly a marked improvement
 compared to the 
existing
 algorithm
s, which 
may obtain
 high 
accuracy
 but lacking
 in 
interpretability
.      Furthermore, 
our method
 offers more flexibility
 and 
transparency
 in the system of
 detection
, allowing
 experts 
contribution 
to facilitate and guide
 the
 process of 
medical
 decision
 making
. REFERENCES
 [1]
 World Health Report 2013, 
Retrieved 06 10, 2014, from 
http://www.who.int/whr/en/
 [2]
 Alonso J.M. and Luis Magdalen
a L., 
An Experimental Study 
on the Interpretability of Fuzzy Systems, 
2009. [3]
 Gacto M. J., Alcala R., and Herrera F., Interpretabilit
y of 
linguistic fuzzy rule
-based systems: An overview of 
interpretability measures, Information Sciences, 
181(20):4340
-4360, 2011 (DOI:10.1016/j.ins.2011.02.021).
 [4]
 Alonso J. M., I
nterpretable fuzzy systems modeling with 
cooperation between expert and induce
d knowledge
, PhD 
Thesis, 2007
. [5]
 Zhang J., and Huang M.L. Density Approach: a New Mode
l for 
BigData Analysis and V
isualization
, C
oncurrency and 
Computation: Practice and Experience
, 2014, 1
532-0634 [6]
 Rumelhart. D., Hinton. G & Willams. R. Learning internal 
rep
resentations by error propagation, in parallel distribution 
proceeding: exploration in the Microstructure of Cognition, 
foundations edited 
by D. 
Rumelhart and J. McClemlland, 
MIT Press, Cambridge, M.A. 1986; 1: 318
-362. [7]
 Meau, Y. P., Ibrahim, F., 
Naroinasamy, 
S. A. L., and Omar, 
R. Intelligent classication of electrocardiogram (ECG) signal 
using ext
ended 
Kalman lter 441
 (EKF) based neuro fuzzy 
system. Computer Methods and Programs in Biomedicine
, 2006. [8]
 Yu S. and 
Chou
 T.
, Integration of independen
t component 
analysis and neural networks for ECG beat classification, 
Expert Systems with Applications, Volume 34, Issue 4,
 2008 [9]
 Hosseini
 H.G.,
 Luo
 D. and 
Reynolds
 K.J., 
The comparison of 
different feed forward neural network architectures for ECG 
signal d
iagnosis
, Medical engineering & physics 28 (4), 372
-378. [10]
 Raghupathi W.,
 Data Mining in Health Care. In Healthcare 
Informatics: Improving Efficiency and Productivity. Edited 
by Kudyba S. Taylor & Francis; 2010:211
-223. [11]
 Casado R., and Younas M. Emerging Tren
ds and 
Technologies in Big Data P
rocessing
. Concurrency and 
Computation: Practice and Experience
 2014, 
1532-0634 [12]
 Raghupathi W, and Raghupathi V., Big data analytics in 
healthcare: promise and potential, Health Information Science 
and Systems 2014, 2:3
. [13]
 Man
ning, C. D. Foundations of Statistical Natural Language 
Processing. MIT Press, 199
9. [14]
 De 
Marneffe, M. F., MacCartney, B., and Manning, C. D. 
Generating Typed Dependency Parse
s from Phrase Structure 
Parses,
 LREC
, 2006
. [15]
 Liu B. Sentiment Analysis and Opinion 
Mining, Morgan & 
Claypool Publishers, 2012
 [16]
 PubMed. Retrieved 06 10, 2014, from 
http://www.ncbi.nlm.nih.gov/pub
med/
 [17]
 Schnbauer, R., Sommers, P., Misfeld, M., Dinov, B., Fiedler, 
F., Huo, Y. and Arya, A. Rele
vant ventricular septal defect 
caused by steam pop during ablation of premature ventricular 
contraction. Circulation,
 2013 [18]
 Soheilykhah, S., Sheikhani, A., Sharif, A. G. and Daevaeiha, 
M. M. Localization of premature ventricular contraction foci 
in normal i
ndividuals based on multichannel 
electrocardiogram signals proces
sing. 
Springerplus, 486, 
2013. [19]
 Moody GB, Mark RG. The impact of the MIT
-BIH 
Arrhythmia Database.
 IEEE 
Eng in Med and Biol 20(3):45
-50, 2001
. [20]
 Pan J and Tompkins W
.J.,
 A Real
-Time QRS Detection
 Algorithm IEEE T
ransactions
 ON 
Biomedical
 Engineering
, Vol
. BM
E-32, 
NO. 3, 1985
. [21]
 Guillaume
 S. and 
Magdalena
 L., 
Expert guided integration of 
induced knowledge into a fuzzy knowledge base
, Soft 
Computing
, 2006, 
773-784, Vol
. 10
. [22]
 Piotrkiewicz M., Kudina
 L., Mierzejewska J., Jakubiec M. 
and Hausmanowa
-Petrusewicz I., Age
-related change in 
duration of after hyperpolarization of human motoneurones, 
The Journal of Physiology, 585, 483
-490, 2007
. [23]
 Casillas J., Accuracy Improvements in Linguistic Fuzzy 
Modellin
g, Springer Science & Business Media, 2003
 [24]
 Chen T., An effective fuzzy collaborative forecasting 
approach for predicting the job cycle time in wafer 
fabrication, Computers & Industrial Engineering, Volume 66, 
Issue 4, 2013
 [25]
 Guillaume, S.
, and 
Charnomordic, 
B. Fuzzy inference 
systems: An integrated modeling environment for 
collaboration between expert knowledge and data using 
FisPro. Expert Systems with Applications, 8744
-8755, 2012.
 [26]
 D.D. 
Nauck, 
Measuring interpretability in rule
-based 
classification systems
, In Proceedings of the FUZZ
-IEEE, St. 
Louis, Missouri, USA, pp. 196
201, 2003 
(DOI:10.1109/FUZZ.2003.1209361
).   Procedia Computer Science   57  ( 2015 )  500  508 
1877-0509  2015 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license 
(http://creativecommons.org/licenses/by-nc-nd/4.0/
).Peer-review under responsibility of organizing committee of the 3rd International Conference on Recent Trends in Computing 2015 (ICRTC-2015)
doi: 10.1016/j.procs.2015.07.372 ScienceDirect
Available online at 
www.sciencedirect.com
3rd International Conference on Recent Trends in Computing 
2015(ICRTC
-2015) 
 Classification and prediction based 
data mining algorithms to 
predict slow learners in 
education 
sector 
  Parneet Kaura,Manpreet Singh
b,Gurpreet Singh Josan
c  aScholar,
 Department
 of CSE, Punjab Technical University,Jalandhar
 144603,India
 bAssistant Professor, Department CSE&IT, GNDEC
, Ludhiana,
 Punjab, India
 cAssistant Professor, Department of CSE & IT, Punjabi University, Patiala, Punjab, India.
 Abstract
 Educational Data Mining field concentrate on Prediction more often as compare to generate exact results for future purpose. 
In order to keep a check on the changes occurring in cu
rriculum patterns, a regular analysis is must of educational 
databases. This paper focus on identifying the slow learners among students and displaying it by a
 predictive data mining 
model using classification based algorithms. Real World data set from a high school is taken and filtration of desired 
potential variables is done using WEKA an Open Source Tool. The dataset of student academic records is tested and 
applied on various
 classification algorithms such as Multilayer Perception, Nave Bayes, SMO, J48 and REPTree using 
WEKA
 an Open source tool
. As a result,
 statistics 
are generated based
 on all classification algorithms and comparison of all 
five classifiers is also done in order to predict the accuracy and to find the best performing classification algorithm
 among 
all. In this paper,
 a knowledge flow model is also shown among all five classifiers. This paper showcases the importance of 
Prediction and Classification based data mining algorithms in the field of education and also presents
 some promising 
future lines
.   201
5 The Authors. Published by Elsevier B
.V.  Peer
-review under responsibility of organizing committee of the 3rd International Conference on Recent Trends in 
Computing 2015 (ICRTC
-2015). 
Keywords
: Educational Data Mining; Knowledge Discovery; Classification;
 Attribute Evaluator.
  1. Introduction
 Data mining has attracted lot of attention in
 the research industry and in society as a whole in recent years, due 
to enormous availability of large amount
 of data and the need for turning such data into useful information and 
knowledge. Data mining, also called Knowledge Discovery in Databases (KDD), is the field of discovering 
new and potentially useful information from 
huge databases
 [12]. 
  * Corresponding author. Tel.: +91
-946668883
1; fax:
 0171-2822002
. E-mail address: kaur.parneet
@gmail.com
  2015 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license 
(http://creativecommons.org/licenses/by-nc-nd/4.0/
).Peer-review under responsibility of organizing committee of the 3rd International Conference on Recent Trends in Computing 2015 
(ICRTC-2015)
501 Parneet Kaur et al.  /  Procedia Computer Science   57  ( 2015 )  500  508 
Educational Data Mining (EDM) is the application of Data Mining techniques on educational data. The 
objective of EDM is to analyze such data and to resolve educational research issues. EDM deals with 

developing new methods to explore the educational data, and using Data Mining methods to better understand 
student
 learning environment
 [1-4]. The EDM process converts raw data coming from educational systems into 
useful information that could potentially have a great impact on educational research and practice. Educational 
Data Mining researchers study a variety of areas, including individual learning from educational software, 
computer supported collaborative learning ,computer
-adaptive testing (and testing more broadly), and the 
factors that are associated with student failure or non
-retention in courses
[6,8]. Some other key areas include 
improvement of student models; application of EDM methods has been in discovering or improving models of 
a domains knowledge structure and studying pedagogical support (both in learning software, and in other 
domains, such as collaborative learning behaviours). There are increasing research interests in using data 
mining in education. This new emerging field, called educational data mining, concerns with developing 

methods that discover knowledge from data originating from educational environments. Educational data 

mining uses many techniques such as Decision Trees, Neural Networks, Nave Bayes, K
-nearest neighbour
 and 
many others. Prediction and analysis of student performance is an important milestone in educational 
environment. 


[4,5]. Academic 
performance of student is not a result of only one deciding factor besides it heavily hinges on various factors 
like personal, socio
-economic, psychological and other environmental variables. This paper identifies
 the 
factors associated with students whose academic performance is not good and to improve the quality of 
education by identifying slow learners so that teachers can assist them individually to improve their
 performance. Through this paper, the accuracy of some classification techniques for predicting performance of 
a student
 is also investigated. The main
 objectives of this work
 are: to generate  data source of predictive 
variables, Data mining methodologies
 to study student performance at high school level, 
identification of the 
slow learners performance, identification of the highly influencing predictive variables on the academic 
performance of high school students and to find the best classification algorithm
.  Nomenclature
 EDM
     Education Data Mining
  SMO     Sequential minimal optimization
 J48             Decision Tree Algorithm
 REPTree    Reduced Error Pruning Decision Tree
 WEKA
      Waikato Environment for Knowledge Analysis
 2. Background and prior work
 Data mining, also called Knowledge Discovery in Databases (KDD), is the field of discovering novel and 
potentially useful information from large amounts of data. Educational Data Mining (EDM) is still in its 
infancy
 [15]. The field of EDM is new and emerging in the field of education sector which can also be applied 
in other areas like sports, accounts, transportation etc.
 The international working group in EDM established the 
Journal of Educational Data Mining (2009) and a yearly international conference that began in 2008. Han and 
Kamber [17] describes data mining software that allow the users to analyze data from different dimensions, 
Categorize it and summarize the relationships which are identified during the mining process. M.Ramaswami 
and R.Bhaskaran
 [12] applied CHAID prediction model to analyze the interrelation between variables that are 
used to predict the outcome of the performance at higher secondary school education.
 The CHAID prediction 
model of student performance was constructed with seven class predictor variable. Nguyen Thai
-Nghe, Andre 
Busche, and Lars Schmidt
-Thieme [13] used
 machine learning techniques to improve the prediction results of 
502   Parneet Kaur et al.  /  Procedia Computer Science   57  ( 2015 )  500  508 
academic performances in real case studies. Three methods have been used by them to deal with the class 
imbalance problem and all of them show satisfactory results. They first balanced the datasets and used both 

cost
-insensitive and sensitive learning with 
SVM for the small d
atasets with Decision Tree for the larger 
datasets. Arockiam et al. [14] implemented FP Tree and K
-means clustering technique for finding the similarity 
between urban and rural students programming skills. FP Tree mining was
 applied to sieve the patterns from 
the dataset. K
-means clustering was used to determine the programming skills
 of the students. The study clearly 
indicates that the rural and the urban students differ in their programming skills
 and found that huge 
proportions of urban students were
 good in programming skill compared to rural students. It divulges that 
academicians provide extra training to urban students in the programming subject.  Cortez and Silva [15] 
attempted to predict failure in the two core classes (Mathematics and Portuguese) of two secondary school 
students from the Alentejo region of Portugal by utilizing 29 predictive variables. Four data mining algorithms 
such as Decision Tree (DT), Random Forest (RF), Ne
ural Network (NN) and Support Vector Machine (SVM) 
were applied on a data set of 788 students, who appeared in 2006 examination. It was reported that DT and NN 

algorithms had the predictive accuracy of 93% and 91% for two
-class dataset (pass/fail) respectively. It was 
also reported that both DT and NN algorithms had the predictive accuracy of 72% for a four
-class dataset. Galit 
[18] gave a case study that use
s 
 data to analyze their learning behaviour
 to predict the results and to
 warn
 students at risk before their final exams. V.Ramesh et al [16] tries to identify the factors influencing the 
performance of students in final examination. They 
adopted survey cum experimental methodology to generate 
the database. The algorithms which were used by them for implementation were Nave Bayes, Multi Layer 
Perception, SMO, J48, and REPTree. The obtained results from hypothesis testing reveals that type of
 school is 
not influence student performance but 

  2.1 Proposed Methodology
  A survey cum experimental methodology is used. Through extensive search of the literature and discussion 

with experts on student performance, a number of factors that are considered to have influence on the 
performance of a student are identified. These influencing factors are categorized as input variables. For this 

work,
 recent real world data is collected from high school. This data is then filtered out
 using manual 
techniques. Then data is
 transformed into a standard format
 required by the WEKA tool. After that
, features 
and parameters
 selection is identified. 
    
   
 
     
   
 
                      Fig 1. Flowchart 
of proposed 
work
 High School Data Set is taken
 Domain values and related variables are decided
 Conversion of data into desired File format is done
 Formatted file is inserted into the Evaluator
 Identification of High Potential Variables are done
 Classification Algorithm are applied and comparison of output is done
 503 Parneet Kaur et al.  /  Procedia Computer Science   57  ( 2015 )  500  508 
Then analysis of identified parameters and implementation is performed on the tool. After implementation 
results are produced and analyzed. Stepwise description of methodology used is represented with
 the help of 
flowchart as
 shown in Figure 1.A record 
152 
students of high school
 is used as dataset and student related 
variables are defined in the Table 1 along with their domain values
. Table 1. Student related variables
 Variable Name
 Description
 Domain
 SEX
 
 {M,F}
 INS-HIGH Institution at high level
 {Private, Government}
 TOB
 Type of board
 {State Board, CBSE}
 MOI
 Medium of instruction
 {Hindi, English}
 TOS
 Type of school
 {Co-ed, Boys, Girls}
 PTUI
 Private tuition
 {Yes, No}
 S-AREA
 Area at school level
 {Urban, Rural}
 MOB
 Student having mobile
 {Yes, No}
 COM-HM Computer at home
 {Yes, No}
 NETACS
 Student having net access
 {Yes, No}
 ROLL NO.
 
 Given by school authority
 INT-GR Internal grade of student
 {A+, A, B, C}
 ATDN
 Attendance count
 Based on School Attendance 
count
 CLASS( Response Variable)
 Whether qualified or not
 {NQ, Q}
  2.2 
Tool
s and 
Techniques used
  In this paper variety of Data Mining techniques are used for prediction of slow learners in Educational Data 
Mining. The techniques
 are Classification, Regression and Density Estimation. 
 During this work, 
Classification technique
s for prediction
 are used. The output dataset is tested and analyze
d with five 
Classification algorithms which are Multilayer Perception, Nave Bayes, SMO, J48 and REPTree. For 
implementation of all these classification tasks we have used WEKA workbench.
  3. Simulation Case Study
 A total of 152 records are taken for the analysis of this research. In this paper,
 selected high potential variables 
using select attributes facility of WEKA
 is done. For attribute evaluation
, Chi Squared attribute, Info Gain 
attribute
, Symmeterical
 Uncert  attribute 
and ReliefF attribute evaluator
 are used. To rank variables Ranker 
Search method
 technique of WEKA is also applied. Final ranks 
are generated
 (using cross validation) manually 
by taking average. High potential variables are listed below along with their ranks in Table 2.
  504   Parneet Kaur et al.  /  Procedia Computer Science   57  ( 2015 )  500  508 
 Table 2. High potential variables
 Name of the 
Variable
 Rank 
Values
 INT-GR 1.65
 ATDN
 2.225
 SEX
 3.6
 PTUI
 3.525
 MOB
 5.375
 INS-HIGH 5.925
 COM-HM 8.325
 NET
-ACS
 9.2
 4. Results
 The dataset during this work is tested and analyze with five Classification algorithms those are Multilayer 
Perception, Nave Bayes, SMO, J48 and REPTree( using cross v
alidation). All the statistics results are provided 
in 
Table 3. Also a comparison of accuracy of all classifiers is done and
 finally it has been investigated that 
Multi 
Layer 
Perception technique
 performs best with accuracy 75%. The accuracy level of all the algorithms 
are given below in Table 4.
  Table 3. Statistical Analysis of Classifiers with Cross Validation
  Name of Classification
 Algorithm
 Class
 TP Rate
 FP Rate
 Precision
 Recall
 F-Measure
 ROC Area
 Multilayer Perception
 NQ 0.83
 0.44
 0.807
 0.83
 0.822
 0.77
 Q 0.55
 0.162
 0.605
 0.553
 0.57
8 0.773
 Naive Bayes
 NQ 0.76
 0.596
 0.741
 0.762
 0.751
 0.648
 Q 0.40
 0.238
 0.432
 0.404
 0.418
 0.648
 SMO NQ 0.88
 0.766
 0.721
 0.886
 0.795
 0.56
 Q 0.23
 0.114
 0.478
 0.234
 0.314
 0.56
 J48
 NQ 0.81
 0.574
 0.761
 0.819
 0.789
 0.713
 Q 0.42
 0.181
 0.513
 0.426
 0.465
 0.713
 REPTree
 NQ 0.83
 0.681
 0.733
 0.838
 0.762
 0.667
 Q 0.31
 0.162
 0.469
 0.319
 0.38
 0.667
   Table 4. Comparison of Classifiers on the basis of Correctly Classified Instances with Cross Validation 
  Mining Technique
 Accuracy
 Multilayer Perception
 75% Nave Bayes
 65.13%
 SMO 68.42%
 J48
 69.73%
 REPTree
 67.76%
  505 Parneet Kaur et al.  /  Procedia Computer Science   57  ( 2015 )  500  508 
  Fig.2. Comparison of Classifiers with use of WEKA Experimenter 
 Comparison of all classifiers with the help of WEKA Experimenter is shown in fig. 2. In this case also Multi 
Layer Perception performs best among all classifiers with F-Measure 82%. A model performance chart is also 
created using GUI of WEKA. The performance comparison on the basis of accuracy among algorithms is 

shown in the figure 3. Also, knowledge flow model shown in fig.4 which shows membership tree structure. 

   Fig.3. Accuracy Comparison of Classifiers  
 506   Parneet Kaur et al.  /  Procedia Computer Science   57  ( 2015 )  500  508 
  Fig. 
4. 
Knowledge flow model
  After loading the data file run the model and 
a model performance chart as shown in figure 5 for multiple 
classifiers such as
 Naive Bayes, Multilayer Perceptron,
 SMO,
 J48 and REPTree
. Figure 5 shows region of 
convergence curve (ROC) for each classifier. 
    Fig.
5. 
Model performance chart
 507 Parneet Kaur et al.  /  Procedia Computer Science   57  ( 2015 )  500  508 
5. Conclusion and Future Scope
 In this paper, classification techniques are used for prediction on the dataset of
 152 students, to predict and 

 as well slow learners among them. In this study, a model was developed based 
on some selected student related input variables collected from real world (high schools). Among all data 
mining classifiers Multi 
Layer 
Perception
 performs best with 75% accuracy and therefore MLP proves to be 
potentially effective and efficient
 classifier algorithm. 
Also comparison of all 5 classifiers with the help of 
WEKA experimenter
 is also done, in this case also MLP proves to be best with F
-measure of 82%. Therefore, 
performance of MLP is relatively higher than other classifiers. A model performance chart is also plotted. This 
research help the
 institution
s to identify students who are slow learners which further provide base for deciding 
special aid to them. EDM is in its infancy and it has lot of potential for education.EDM opens promising and 
exciting
 avenues for future research. In future, Integration of data mining techniques with DBMS and E
- learning techniques
 is merged together on different datasets to find accuracy and predictions of desired results. 
Also, EDM tools are easy to understand and interface
d with various techniques. E
ducators with no expertise in 
data mining
 can also apply their hands in these fields. Also some new factors can be applied to improve the 

 performance, learning and retention
 capabilities among them
. Hence the future of EDM is promising 
for further research and can be applied in other areas like medicine, sports, and share
 market due to the 
availability of huge databases.
  Acknowledgements
  This research
 paper is truly contribution and guidance who belong to the list of author. The authors pay homage 
to  Punjab Technical University for continuous encouragement, guidance and support during their work.
  References
  [1]
 Cristobal Romero
 (2010)

-of-the
-
 systems
 , man and cybernetics
- Part C: Applications and Reviews
 vol. 40 issue 6, pp 601 
 618. [2]
 Zaane, O. (2001

-
Technology For Education,
 60-64. [3]
 
-
 Proceedings of the International Conference on 
Computers in Education,55
59. [4]
 

Proceedings of the 
7th International Conference on Intelligent Tutoring Systems, 531
-540. [5]
 Tang, T., McCalla, G. 

-
International 
Journal on
 E-Learning, vol. 
4, issue1, 105
129. [6]
 
-based tutoring tool with mining facilities to improve learni

 Proceedings of 
the 11
th International Conference on Artificial Intelligence in Education, 201
208 [7]
 

Proceedings of the 
International
 Conference on User Modelling, 25
34. [8]
 
-
Proceedings of the 5th International Conference 
on Intelligent
 Tutoring Systems, 584
593.
 [9]
 

Computer and 
Education
 Journal , 45, 141
160.
 [10]
 M.Ramaswami and R.Bhaskaran(2010)


Journal of Computer Science Issues Vol. 7, Issue 1, pp 10
-18. [11]
 Nguyen Thai
-Nghe, Andre Busche, and Lars Schmidt
-Thieme(2009)


 Ninth International Conference on Intelligent Systems Design and Applications, 
 [12]
 L.Arockiam, S.Charles,Arulkumar
 et.al(2010)


International Journal on Computer Science and Engineering Vol. 02, No. 03, pp 687
-690. 508   Parneet Kaur et al.  /  Procedia Computer Science   57  ( 2015 )  500  508 
[13]
 P. Cortez, and A. Silva(2008)

 

Teixeira (Eds.), 
 pp 
5-12. [14]
 V.Ramesh, P.Parkavi, K.Ramar(2013)


of computer 
applications , Volume 63
- no. 8, pp 35
-39. [15]
 Jiawei Han Michelin Kamber(2011)

-Concepts and Techniqu

.   Survey of Clustering Data Mining Techniques Pavel Berkhin Accrue Software, Inc.Clusteringis a division of data into groups of similar objects. Representing the data by fewer clusters necessarily loses certain fine details, but achieves simplification. It models data by its clusters. Data modeling puts clustering in a historical perspective rooted in mathematics, statistics, and numerical analysis.From a machine learning perspective clusters correspond to hidden patterns, thesearch for clusters is unsupervised learning, and the resulting system represents a data concept. From a practical perspective clustering plays an outstanding role in data mining applications such as scientific data exploration, information retrievaland text mining, spatial database applications, Web analysis, CRM, marketing, medical diagnostics, computational biology, and many others.Clustering is the subject of active research in several fields such as statistics,pattern recognition, and machine learning. This survey focuses on clustering in data mining. Data mining adds to clustering the complications of very large datasets with very many attributes of different types. This imposes unique computational requirements on relevant clustering algorithms. A variety of algorithms have recently emerged that meet these requirements and were successfully applied to real-life data mining problems. They are subject of the survey.Categories and Subject Descriptors: I.2.6. [Artificial Intelligence]: Learning  Concept learning; I.4.6 [Image Processing]: Segmentation; I.5.1 [PatternRecognition]: Models; I.5.3 [Pattern Recognition]: Clustering. General Terms: Algorithms, Design Additional Key Words and Phrases: Clustering, partitioning, data mining,unsupervised learning, descriptive learning, exploratory data analysis, hierarchical clustering, probabilistic clustering, k-meansContent:1. Introduction1.1. Notations 1.2.Clustering Bibliography at Glance1.3.Classification of Clustering Algorithms1.4.Plan of Further PresentationAuthors address: Pavel Berkhin, Accrue Software, 1045 Forest Knoll Dr., San Jose, CA, 95129; e-mail: pavelb@accrue.com12. Hierarchical Clustering2.1. Linkage Metrics 2.2. Hierarchical Clusters of Arbitrary Shapes 
2.3. Binary Divisive Partitioning 2.4. Other Developments3. Partitioning Relocation Clustering 3.1. Probabilistic Clustering 3.2.K-Medoids Methods 3.3.K-Means Methods 4. Density-Based Partitioning 4.1. Density-Based Connectivity 4.5. Density Functions5.Grid-Based Methods 6. Co-Occurrence of Categorical Data
7. Other Clustering Techniques7.1.Constraint-Based Clustering7.2. Relation to Supervised Learning7.3.Gradient Descent and Artificial Neural Networks 7.4. Evolutionary Methods 7.5. Other Developments9.Scalability and VLDB Extensions10.Clustering High Dimensional Data10.1.Dimensionality Reduction 10.2.Subspace Clustering10.3.Co-Clustering10.General Algorithmic Issues10.1.Assessment of Results10.2. How Many Clusters?10.3. Data Preparation10.4.Proximity Measures 10.5. Handling OutliersAcknowledgementsReferences1. IntroductionThe goal of this survey is to provide a comprehensive review of different clustering techniques in data mining.Clusteringis a division of data into groups of similar objects. Each group, called cluster, consists of objects that are similar between themselves and dissimilar to objects of other groups. Representing data by fewerclusters necessarilyloses certain fine details (akin to lossy data compression), but achieves simplification. It represents many data objects by few clusters, and hence, it models data by its clusters. Data modeling puts clustering in a historical perspective rooted in mathematics, statistics,and numerical analysis. From a machine learning perspective clusters correspond to hidden patterns, the search for clusters isunsupervised learning, and the resulting systemrepresents a data concept. Therefore, clustering is unsupervised learning of a hidden data 2concept. Data mining deals with large databases that impose on clustering analysis additional severe computational requirements. These challenges led to the emergence of powerful broadly applicable data mining clustering methods surveyed below. 1.1. Notations 
To fix the context and to clarify prolific terminology, we consider a dataset Xconsistingof data points (or synonymously,objects, instances, cases,patterns,tuples,transactions) in attribute space A, where i, and each component is anumerical or nominal categoricalattribute(or synonymously, feature,variable,dimension,component,field). For a discussion of attributes data types see [Han & Kamber 2001]. Such point-by-attribute data format conceptually corresponds to a matrix and is used by the majority of algorithms reviewed below. However, data of other formats, such as variable length sequences and heterogeneous data, is becoming more and more popular. The simplest attribute space subset is a direct Cartesian product of sub-ranges called a segment (also cube, cell, region). A unitis an elementary segment whose sub-ranges consist of a single category value, or of asmall numerical bin. Describing the numbers of data points per every unit represents anextreme case of clustering, a histogram, where no actual clustering takes place. This is a very expensive representation, and not a very revealing one. User driven segmentation isanother commonly used practice in data exploration that utilizes expert knowledge regarding the importance of certain sub-domains. We distinguish clustering fromsegmentation to emphasize the importanceof the automatic learning process.Axxxidii=),...,(1CCl=:N1=lilAxNd,:1,,dlACAll=The ultimate goal of clustering is to assign points to a finite system ofksubsets, clusters. Usually subsets do not intersect (this assumption is sometimes violated), and their union is equal to a full dataset with possible exception of outliers.oCCCCCXjjouliersk/==21,...11.2. Clustering Bibliography at Glance 
General references regarding clustering include [Hartigan 1975; Spath 1980; Jain & Dubes 1988; Kaufman &  Rousseeuw 1990; Dubes 1993; Everitt 1993; Mirkin 1996; Jainet al. 1999; Fasulo 1999; Kolatch 2001; Han et al. 2001; Ghosh 2002]. A very good introduction to contemporary data miningclustering techniques can be found in the textbook [Han & Kamber 2001]. There is a close relationship between clustering techniques and many other disciplines.Clustering has always been used in statistics [Arabie & Hubert 1996] and science [Massart & Kaufman 1983]. The classic introduction into pattern recognition frameworkis given in [Duda & Hart 1973]. Typical applications include speechandcharacterrecognition. Machine learning clustering algorithms were applied to image segmentation andcomputer vision [Jain & Flynn 1996]. For statistical approaches to pattern recognition see [Dempster et al. 1977] and [Fukunaga 1990]. Clustering can be viewed as a density estimation problem. This is the subject of traditional multivariate statisticalestimation [Scott 1992]. Clustering is also widely used for data compression in imageprocessing, which is also known as vector quantization [Gersho & Gray 1992]. Data 3fitting in numerical analysis provides still another venue in data modeling [Daniel & Wood 1980]. This surveys emphasis is on clustering in data mining. Such clustering is characterized by large datasets with many attributes of different types. Though we do not even try to review particular applications, many important ideas are related to the specific fields.Clustering in data mining was brought to life by intense developments in informationretrieval and text mining [Cutting et al. 1992; Steinbach et al. 2000; Dhillon et al. 2001],spatial database applications, for example,GIS or astronomicaldata, [Xu et al. 1998; Sander et al. 1998; Ester et al. 2000], sequence and heterogeneous data analysis [Cadez et al. 2001], Web applications [Cooley et al. 1999; Heer & Chi 2001; Foss et al. 2001], DNA analysis in computational biology [Ben-Dor & Yakhini 1999], and many others.They resulted in a large amount of application-specific developments that are beyond our scope, but also in some general techniques. These techniques and classic clustering algorithms that relate to them surveyed below. 1.3. Classification of Clustering Algorithms Categorization of clustering algorithms is neither straightforward, nor canonical. In reality, groups below overlap. For readers convenience we provide a classification closely followed by this survey. Corresponding terms are explained below.Clustering Algorithms Hierarchical Methods Agglomerative AlgorithmsDivisive AlgorithmsPartitioning Methods Relocation AlgorithmsProbabilistic Clustering K-medoids Methods K-means Methods Density-Based AlgorithmsDensity-Based Connectivity Clustering Density Functions Clustering Grid-Based Methods Methods Based on Co-Occurrence of Categorical Data Constraint-Based Clustering Clustering Algorithms Used in Machine LearningGradient Descent and Artificial Neural NetworksEvolutionary Methods Scalable Clustering AlgorithmsAlgorithms For High Dimensional Data Subspace Clustering Projection TechniquesCo-Clustering Techniques 41.4. Plan of Further Presentation Traditionally clustering techniques are broadly divided in hierarchicalandpartitioning.Hierarchical clustering is further subdivided into agglomerative and divisive. The basicsof hierarchical clustering include Lance-Williams formula, idea ofconceptual clustering,now classic algorithms SLINK, COBWEB, as well as newer algorithms CURE andCHAMELEON. We survey them in the section Hierarchical Clustering.While hierarchical algorithms build clusters gradually (as crystals are grown), partitioning algorithms learn clusters directly. In doing so, they either try to discoverclusters by iteratively relocating points between subsets, or try to identify clusters asareas highly populated with data. Algorithms of the first kind are surveyed in the section Partitioning Relocation Methods. They are further categorized into probabilisticclustering(EM framework, algorithms SNOB, AUTOCLASS, MCLUST), k-medoidsmethods(algorithms PAM, CLARA, CLARANS, and its extension), and k-meansmethods (different schemes, initialization,optimization, harmonic means, extensions).Such methods concentrate on how well points fit into their clusters and tend to build clusters of proper convex shapes.Partitioning algorithms of the second type are surveyed in the section Density-BasedPartitioning. They try to discover dense connected components of data, which are flexible in terms of their shape. Density-based connectivity is used in the algorithmsDBSCAN, OPTICS, DBCLASD, while the algorithm DENCLUE exploits space densityfunctions. These algorithms are less sensitive to outliers and can discover clusters ofirregular shapes. They usually work with low-dimensional data of numerical attributes, known as spatialdata. Spatial objects could include not only points, but also extended objects (algorithm GDBSCAN). Some algorithms work with data indirectly by constructing summaries of data over the attribute space subsets.They perform space segmentation and then aggregate appropriatesegments. We discuss them in the section Grid-BasedMethods. They frequently use hierarchical agglomeration as one phase of processing. Algorithms BANG, STING,WaveCluster, and an idea of fractal dimensionare discussed in this section. Grid-basedmethods are fast and handle outliers well. Grid-based methodology is also used as an intermadiate step in many other algorithms (for example, CLIQUE, MAFIA).Categorical data is intimately connected with transactional databases. The concept of asimilarity alone is not sufficient for clusteringsuch data. The idea of categorical data co-occurrence comes to rescue. The algorithmsROCK, SNN, and CACTUS are surveyed in the sectionCo-Occurrence of Categorical Data. The situation gets even more aggravatedwith the growth of the numberof items involved. To help with this problem an effort is shifted from data clustering to pre-clustering of items or categorical attribute values.Development based on hyper-graphpartitioning and the algorithm STIRR exemplify thisapproach.Many other clustering techniques are developed, primarilyin machine learning, that either have theoretical significance, are used traditionally outside the data miningcommunity, or do not fit in previously outlined categories. The boundary is blurred. In the sectionOther Clustering Techniques we discuss relationship to supervised learning,gradient descent and ANN (LKMA, SOM), evolutionary methods (simulated annealing,5genetic algorithms (GA)), and the algorithm AMOEBA. We start, however, with the emerging field of constraint-based clustering that is influenced by requirements of real-world data mining applications.Data Mining primarily works with large databases. Clustering large datasets presentsscalability problems reviewed in the section Scalability andVLDB Extensions. Here we talk about algorithms like DIGNET, about BIRCH and other data squashing techniques, and about Hoffding or Chernoff bounds.Another trait of real-life data is its high dimensionality. Corresponding developments are surveyed in the sectionClustering High Dimensional Data. The trouble comes from adecrease in metric separation when the dimension grows. One approach to dimensionalityreductionuses attributes transformations (DFT, PCA, wavelets). Another way to address the problem is through subspace clustering (algorithms CLIQUE, MAFIA, ENCLUS,OPTIGRID, PROCLUS, ORCLUS). Still another approach clusters attributes in groups and uses their derived proxies to cluster objects. This double clustering is known as co-clustering.Issues that are common to different clustering methods are overviewed in the section General Algorithmic Issues. We talk about assessment of results, determination of appropriate number of clusters to build, data preprocessing (attribute selection, data scaling, special data indices),proximity measures, and handling outliers.1.5. Important Issues What are the properties of clustering algorithms we are concerned with in data mining?These properties include:Type of attributes algorithm can handleScalability to large datasetsAbility to work with high dimensional data Ability to find clusters of irregular shapeHandling outliersTime complexity (when there is no confusion, we use the term complexity)Data order dependency Labeling or assignment (hard or strict vs. soft of fuzzy) Reliance on a priori knowledge and user defined parametersInterpretability of resultsWhile we try to keep these issues in mind, realistically, we mention only few with every algorithm we discuss. The above list is in no way exhaustive. For example, we also discuss such properties as ability to work in pre-defined memory buffer, ability to restartand ability to provide an intermediate solution. 2. Hierarchical Clustering Hierarchicalclustering builds a cluster hierarchy or, in other words, a tree of clusters,also known as a dendrogram. Every cluster node contains child clusters; sibling clusterspartition the points covered by their common parent. Such an approach allows exploring6data on different levels of granularity. Hierarchical clustering methods are categorized intoagglomerative (bottom-up) and divisive(top-down) [Jain & Dubes 1988; Kaufman& Rousseeuw 1990]. An agglomerative clustering starts with one-point (singleton) clusters and recursively merges two or more most appropriate clusters. A divisiveclustering starts with one cluster of all data points and recursively splits the most appropriate cluster. The process continues until a stopping criterion (frequently, therequested numberkof clusters) is achieved. Advantages of hierarchical clusteringinclude:Embedded flexibility regarding the level of granularityEase of handling of any forms of similarity or distanceConsequently, applicability to any attribute types Disadvantages of hierarchical clustering are related to: Vagueness of termination criteria The fact that most hierarchical algorithms do not revisit once constructed (intermediate) clusters with the purpose of their improvementThe classic approaches to hierarchical clustering are presented in the sub-section LinkageMetrics. Hierarchical clustering based on linkage metrics results in clusters of proper (convex) shapes. Active contemporary efforts to build cluster systems that incorporate our intuitive concept of clusters as connected components of arbitrary shape, including the algorithms CURE and CHAMELEON, are surveyed in the sub-sectionHierarchicalClusters of Arbitrary Shapes. Divisive techniques based on binary taxonomies are presented in the sub-section Binary Divisive Partitioning. The sub-section OtherDevelopments contains information related to incremental learning, model-basedclustering, and cluster refinement.In hierarchical clustering our regular point-by-attribute data representation is sometimesof secondary importance. Instead, hierarchical clustering frequently deals with the matrix of distances (dissimilarities) or similarities between training points. It issometimes called connectivitymatrix. Linkage metrics are constructed (see below) fromelements of this matrix. The requirement of keeping such a large matrix in memory is unrealistic. To relax this limitation different devices are used to introduce into the connectivity matrix some sparsity. This can be done by omitting entries smaller than a certain threshold, by using only a certain subset of data representatives, or by keeping with each point only a certain number of its nearest neighbors. For example, nearestneighbor chains have decisive impact on memory consumption [Olson 1995]. A sparse matrix can be further used to represent intuitive concepts of closeness and connectivity.Notice that the way we process original (dis)similarity matrix and construct a linkagemetric reflects our a priori ideas about the data model.NNWith the (sparsified) connectivity matrix we can associate the connectivity graph whose vertices Xare data points, and edges Eand their weights are pairs of points and the corresponding positive matrix entries. This establishes a connection between hierarchical clustering and graph partitioning. ),(EXG=One of the most striking developments in hierarchical clustering is the algorithm BIRCH. Since scalability is the major achievement of this blend strategy, this algorithm is discussed in the sectionScalable VLDB Extensions. However, data squashing used by 7BIRCH to achieve scalability, has independent importance. Hierarchical clustering of large datasets can be very sub-optimal, even if data fits in memory. Compressing data may improve performance of hierarchical algorithms.2.1. Linkage Metrics Hierarchical clustering initializes a cluster system as a set of singleton clusters (agglomerative case) or a single cluster of all points (divisive case) and proceeds iteratively with merging or splitting of the most appropriate cluster(s) until the stoppingcriterion is achieved. The appropriateness of a cluster(s) for merging/splitting depends on the (dis)similarity of cluster(s) elements. This reflects a general presumption that clustersconsist of similar points. An important example of dissimilarity between two points is the distance between them. Other proximity measures are discussed in the section GeneralAlgorithm Issues.To merge or split subsets of points ratherthan individual points, the distance betweenindividual points has to be generalized to the distance between subsets. Such derived proximity measure is called a linkage metric. The type of the linkage metric used significantly affects hierarchical algorithms, since it reflects the particular concept ofcloseness and connectivity. Major inter-cluster linkage metrics [Murtagh 1985, Olson 1995] include single link,average link, and complete link. The underlying dissimilaritymeasure (usually, distance) is computed for every pair of points with one point in the first set and another point in the second set. A specific operation such as minimum (single link), average (average link), or maximum(complete link) is applied to pair-wisedissimilarity measures:.},|),({),(2121CyCxyxdoperationCCd=Early examples include the algorithm SLINK [Sibson 1973], which implements single link, Voorhees method [Voorhees 1986], which implements average link, and the algorithm CLINK [Defays 1977], which implements complete link. Of these SLINK is referenced the most. It is related to the problem of finding the Euclidean minimalspanning tree [Yao 1982] and has Ocomplexity. The methods using inter-clusterdistances defined in terms of pairs with points in two respective clusters (subsets) arecalledgraph methods. They do not use any cluster representation other than a set of points. This name naturally relates to the connectivity graphG introducedabove, since every data partition corresponds to a graph partition. Such methods can be appended by so-called geometric methods in which a cluster is represented by its central point. It results in centroid,median, and minimumvariance linkage metrics. Under the assumption of numerical attributes, the center point is defined as a centroid or an average of two cluster centroids subject to agglomeration.)(2N),(EX=All of the above linkage metrics can be derived as instances of the Lance-Williamsupdating formula [Lance & Williams 1967] ),(),(),(),()(),()(),(kjkijikjkikjiCCdCCdcCCbdCCdkaCCdiaCCCd+++=.Herea,b,care coefficients corresponding to a particular linkage. This formula expresses a linkage metric between the union of the two clusters and the third cluster in terms of 8underlying components. The Lance-Williams formula has an utmost importance since it makes manipulation with dis(similarity) computationally feasible. Survey of linkage metrics can be found in [Murtagh 1983; Day & Edelsbrunner 1984]. When the base measure is distance, these methods capture inter-cluster closeness. However, a similarity-based view that results in intra-cluster connectivity considerations is also possible. This is how original average link agglomeration (Group-Average Method) [Jain & Dubes 1988] was introduced.Linkage metrics-based hierarchical clustering suffers from time complexity. Under reasonable assumptions, such as reducibility condition (graph methods satisfy thiscondition), linkage metrics methods have complexity [Olson 1995]. Despite the unfavorable time complexity, these algorithmsare widely used. An example is algorithmAGNES (AGlomerative NESting) [Kaufman & Rousseeuw 1990] used in S-Plus. )(2NOWhen the connectivity matrix is sparsified, graph methods directly dealing withthe connectivity graphGcan be used. In particular, hierarchical divisive MST (MinimumSpanning Tree) algorithm is based on graph partitioning [Jain & Dubes 1988]. NN2.2. Hierarchical Clusters of Arbitrary Shapes Linkage metrics based on Euclidean distance for hierarchical clustering of spatial datanaturally predispose to clusters of proper convex shapes. Meanwhile, visual scanning of spatial images frequently attests clusters with curvy appearance.Guha et al. [1998] introduced the hierarchical agglomerative clustering algorithm CURE (Clustering Using REpresentatives). This algorithm has a number of novel features of general significance. It takes special care with outliers and with label assignment stage. It also uses two devices to achieve scalability. The first one is data sampling (section Scalability and VLDB Extensions). The second device is data partitioning in p partitions,so that fine granularity clusters are constructed in partitions first. A major feature ofCURE is that it represents a cluster by a fixed numberc of points scattered around it. Thedistance between two clusters used in the agglomerative process is equal to the minimumof distances between two scattered representatives. Therefore, CURE takes a middle-ground approach between the graph (all-points) methods and the geometric (one centroid) methods. Single and average link closeness is replaced by representatives aggregate closeness. Selecting representatives scattered around a cluster makes it possible to cover non-spherical shapes. As before, agglomeration continues until requested numberk of clusters is achieved. CURE employs one additional device: originally selected scatteredpoints are shrunk to the geometric centroid of the cluster by user-specified factor .Shrinkage suppresses the affect of the outliers since outliers happen to be located further from the cluster centroid than the other scattered representatives. CURE is capable of finding clusters of different shapes and sizes,and it is insensitive to outliers. Since CUREuses sampling, estimation of its complexity is not straightforward. For low-dimensionaldata authors provide a complexity estimate of defined in terms of sample size.More exact bounds depend on input parameters: shrink factor , number of representative pointsc, number of partitions, and sample size. Figure 1 illustrates agglomeration inCure. Three clusters, each with three representatives, are shown before and after themerge and shrinkage. Two closest representatives are connected by arrow.)(2sampleNO9While the algorithm CURE works with numerical attributes (particularly low dimensionalspatial data), the algorithm ROCK developed by the same researchers [Guha et al. 1999] targets hierarchical agglomerative clustering for categorical attributes. It is surveyed in the sectionCo-Occurrence of Categorical Data.The hierarchical agglomerative algorithm CHAMELEON [Karypis et al. 1999a] utilizesdynamic modeling in cluster aggregation. It uses the connectivity graph G corresponding to the K-nearest neighbor model sparsification of the connectivity matrix: the edges of Kmost similar points to any given pointare preserved, the rest are pruned. CHAMELEON has two stages. In the first stage small tight clusters are built to ignite the second stage. Thisinvolves a graph partitioning [Karypis& Kumar 1999]. In the second stage agglomerativeprocess is performed. It utilizes measures of relative inter-connectivity  and relative closeness; both are locally normalized by quantities related to clusters. In this sense the modeling is dynamic. Normalization involves certain non-obvious graph operations [Karypis& Kumar 1999]. CHAMELEON strongly relies on graph partitioning implemented in the library HMETIS (see the section Co-Occurrence of Categorical Data). Agglomerative process depends on user provided thresholds. A decision to merge is made based on the combination),(jiCCRI),(jiCCRCjiCC,),(),(jijiCCRCCCRIof local relative measures. The algorithm does not depend on assumptions about the data model. This algorithm is proven to find clusters of different shapes, densities, and sizes in 2D (two-dimensional) space. It has a complexity of ,wherem is number of sub-clusters built during first initialization phase. Figure 2(analogous to the one in [Karypis& Kumar 1999]) presents a choice of four clusters (a)-(d) for a merge. While Cure would merge clusters (a) and (b), CHAMELEON makes intuitively better choice of merging (c) and (d).))log()log((2mmNNNmO++  Figure 1: Agglomeration in Cure.Figure 2: CHAMELEONmerges (c) and (d).BeforeAfter(a)(b)(c)(d)2.3. Binary Divisive Partitioning In linguistics, information retrieval, and document clustering applications binarytaxonomies are very useful. Linear algebra methods, based on singular value decomposition (SVD) are used for this purpose in collaborative filtering and informationretrieval [Berry & Browne 1999]. SVD application to hierarchical divisive clustering of document collections resulted in the PDDP (Principal Direction Divisive Partitioning)10algorithm [Boley 1998]. In our notations, object xis a document,lth attribute corresponds to a word (index term), and matrix entryis a measure (as TF-IDF) of l-term frequencyin a documentx. PDDP constructs SVD decomposition of the matrixilxdTNiiRexNxxeXC====)1,...1(,1),(:1.This algorithm bisects data in Euclidean space by a hyperplane that passes through data centroid orthogonally to eigenvector with the largest singular value. Thek-way splitting is also possible if the klargest singular values are considered. Bisecting is a good way to categorize documents and it results in a binary tree. Whenk-means (2-means) is used for bisecting, the dividing hyperplane is orthogonal to a line connecting two centroids. The comparative study of both approaches [Savaresi & Boley 2001] can be used for further references. Hierarchical divisive bisecting k-means was proven [Steinbach et al. 2000] to be preferable for document clustering.While PDDP or 2-means are concerned with how to split a cluster, the problem of which cluster to split is also important. Casual strategies are: (1) split each node at a given level,(2) split the cluster with highest cardinality, and, (3) split the cluster with the largestintra-cluster variance. All three strategieshave problems. For analysis regarding thissubject and better alternatives, see [Savaresi et al. 2002]. 2.4. Other DevelopmentsWards method [Ward 1963] implements agglomerative clustering based not on linkage metric, but on an objective function used in k-means (sub-section K-Means Methods).The merger decision is viewed in termsof its effect on the objective function. The popular hierarchical clustering algorithm for categorical data COBWEB [Fisher1987] has two very important qualities. First, it utilizesincremental learning. Instead of following divisive or agglomerative approaches, it dynamically builds a dendrogram by processing one data point at a time. Second, COBWEB belongs to conceptual or model-based learning. This means that each cluster is considered as a model that can bedescribed intrinsically, rather than as a collection of points assigned to it. COBWEBsdendrogram is called a classification tree. Each tree node C, a cluster, is associated with the conditional probabilities for categorical attribute-values pairs, llplApdlCvx:1,:1),|Pr(===.This easily can be recognized as a C-specific Nave Bayes classifier. During the classification treeconstruction, every new point is descended along the tree and the tree is potentially updated (by an insert/split/merge/create operation). Decisions are based on an analysis of a category utility [Corter & Gluck 1992]()))(Pr()|(Pr()(,/)(},...,{22,:11lpljpllpljkjjkvxCvxCCUkCCUCCCU=====similar to GINI index. It rewards clusters C for increases in predictability of the categorical attribute values . Being incremental, COBWEB is fast with a complexityjlpv11of, though it depends non-linearly on tree characteristics packed into a constant t.There is the similar incremental hierarchical algorithm for all numerical attributes calledCLASSIT [Gennari et al. 1989]. CLASSIT associates normal distributions with cluster nodes. Both algorithms can result in highly unbalanced trees. )(tNOCl(dChiu et al. [2001] proposed another conceptualormodel-based approach to hierarchical clustering. This development contains several different useful features, such as the extension of BIRCH-like preprocessing to categorical attributes, outliers handling, and a two-step strategy for monitoring the numberof clusters including BIC (defined below). The model associated with a cluster covers both numerical and categorical attributes and constitutes a blend of Gaussian and multinomial models. Denote correspondingmultivariate parameters by . With every clusterC,we associate a logarithm of its(classification) likelihood)|(logiCxxpi=The algorithm uses maximum likelihood estimates for parameter. The distance betweentwo clusters is defined (instead of linkage metric) as a decrease in log-likelihood 2121),21CCCClllCC+=caused by merging of the two clusters under consideration. The agglomerative process continues until the stopping criterion is satisfied. As such, determination of the best kisautomatic. This algorithm has the commercial implementation (in SPSS Clementine). The complexity of the algorithm is linear in N for the summarization phase. Traditional hierarchical clustering is inflexible due to its greedy approach: after a mergeor a split is selected it is not refined. Though COBWEB does reconsider its decisions, it is so inexpensive that the resulting classification tree can also have sub-par quality. Fisher [1996] studied iterative hierarchical cluster redistribution to improve once constructed dendrogram. Karypis et al. [1999b] also researched refinement for hierarchical clustering.In particular, they brought attention to a relationof such a refinement to a well-studied refinement of k-way graph partitioning [Kernighan & Lin 1970].For references related to parallel implementation of hierarchical clustering see [Olson1995].3. Partitioning Relocation ClusteringIn this section we survey data partitioning algorithms, which divide data into severalsubsets. Because checking all possible subset systems is computationally infeasible,certain greedy heuristics are used in the form of iterative optimization. Specifically, this means different relocation schemes that iteratively reassign points between the kclusters.Unlike traditional hierarchical methods, in which clusters are not revisited after being constructed, relocation algorithms gradually improve clusters. With appropriate data, this results in high quality clusters. One approach to data partitioning is to take a conceptual point of view that identifies thecluster with a certain model whose unknown parameters have to be found. More 12specifically,probabilisticmodels assume that the data comes from a mixture of several populations whose distributions and priors we want to find. Corresponding algorithms aredescribed in the sub-section Probabilistic Clustering. One clear advantage of probabilistic methods is the interpretability of the constructed clusters. Having concise cluster representation also allows inexpensivecomputation of intra-clusters measures of fit that give rise to a globalobjective function (see log-likelihood below). Another approach starts with the definition of objective function depending on apartition. As we have seen (sub-section Linkage Metrics), pair-wise distances or similarities can be used to compute measures of iter- and intra-cluster relations. In iterative improvements such pair-wise computations would be too expensive. Using unique cluster representatives resolves the problem: now computation of objective function becomes linear in N (and in a number of clusters ). Depending on how representatives are constructed, iterative optimization partitioning algorithms aresubdivided into k-medoids and k-means methods.K-medoid is the most appropriate data point within a cluster that represents it. Representation by k-medoids has two advantages. First, it presents no limitations on attributes types, and, second, the choice of medoids isdictated by the location of a predominant fraction of points inside a cluster and, therefore, it is lesser sensitive to the presence of outliers. Ink-means case a cluster is represented by its centroid, which is a mean (usually weighted average) of points within a cluster. This works conveniently only with numerical attributes and can be negatively affected by a single outlier. On the other hand, centroids have the advantage of clear geometric and statistical meaning. The corresponding algorithms are reviewed in the sub-sections K-MedoidsMethods and K-Means Methods.Nk<<3.1. Probabilistic ClusteringIn the probabilistic approach, data is considered to be a sample independently drawn from a mixture model of several probability distributions [McLachlan & Basford 1988]. The main assumption is that data points are generated by, first, randomly picking a model j with probabilitykjj:1,=, and, second, by drawing a point xfrom a corresponding distribution. The area around the mean of each (supposedly unimodal) distribution constitutes a natural cluster. So we associate the cluster with the correspondingdistributions parameters such as mean, variance, etc. Each data point carries not only its(observable) attributes, but also a (hidden) cluster ID (class in pattern recognition). Each pointxis assumed to belong to one and only one cluster, and we can estimate the probabilities of the assignment to j)|Pr(xCjthmodel. The overall likelihood of the training data is its probability to be drawn from a given mixture model===NikjjijCxCXL:1:1)|Pr()|(Log-likelihood serves as an objective function, which gives rise to the Expectation-Maximization (EM) method. For a quick introduction to EM, see [Mitchell 1997]. Detailed descriptions and numerous references regarding this topic can be found in [Dempster et al. 1977; McLachlan & Krishnan 1997]. EM is a two-step iterative optimization. Step (E) estimates probabilities, which is equivalent to a soft))|(log(CXL)|Pr(jCx13(fuzzy) reassignment. Step (M) finds an approximation to a mixture model, given current soft assignments. This boils down to finding mixture model parameters that maximizelog-likelihood. The process continues until log-likelihood convergence is achieved.Restarting and other tricks are used to facilitate finding better local optimum. Moore[1999] suggested acceleration of EM methodbased on a special data index, KD-tree.Data is divided at each node into two descendents by splitting the widest attribute at the center of its range. Each node stores sufficient statistics (including covariance matrix)similar to BIRCH. Approximate computing over a pruned tree accelerates EM iterations.Probabilistic clustering has some important features:It can be modified to handle recodes of complex structure It can be stopped and resumed with consecutive batches of data, since clusters have representation totally different from sets of points At any stage of iterative process the intermediate mixture model can be used to assign cases (on-line property) It results in easily interpretable cluster systemBecause the mixture model has clear probabilistic foundation, the determination of the most suitable number of clusters kbecomes a more tractable task. From a data miningperspective, excessive parameter set causesoverfitting, while from a probabilisticperspective, number of parameters can be addressed within the Bayesian framework. See the sub-section How Many Clusters? for more details including terms MML and BIC used in the next paragraph. The algorithm SNOB [Wallace & Dowe 1994] uses a mixture model in conjunction with the MML principle. Algorithm AUTOCLASS[Cheeseman & Stutz 1996] utilizes a mixturemodel and covers a broad variety of distributions, including Bernoulli, Poisson, Gaussian, and log-normal distributions. Beyond fitting a particular fixed mixture model,AUTOCLASS extends the search to different models and different k. To do this AUTOCLASSheavily relies on Bayesian methodology, in which a model complexity is reflected through certain coefficients (priors) in the expression for the likelihood previously dependent only on parameters values. This algorithm has a history of industrial usage. The algorithm MCLUST [Fraley & Raftery 1999] is a software package (commerciallylinked with S-PLUS) for hierarchical, mixture model clustering, and discriminantanalysis using BIC for estimation of goodness of fit. MCLUST uses Gaussian models with ellipsoids of different volumes, shapes, and orientations.An important property of probabilistic clustering is that mixture model can be naturally generalized to clustering heterogeneous data. This is important in practice, where an individual (data object) has multivariate static data (demographics) in combination withvariable length dynamic data (customer profile) [Smyth 1999]. The dynamic data can consist of finite sequences subject to a first-order Markov model with a transition matrixdependent on a cluster. This frameworkalso covers data objects consisting of severalsequences, where number n of sequences per  is subject to geometric distribution [Cadez et al. 2000]. To emulate sessions of different lengths, finite-state Markov model(transitional probabilities between Web site pages) has to be augmented with a specialend state. Cadez et al. [2001] used mixture model for customer profiling based on transactional information.iix14Model-based clustering is also used in a hierarchical framework: COBWEB, CLASSIT and development by Chiu et al. [2001] were already presented above. Another early example of conceptual clustering is algorithm CLUSTER/2 [Michalski & Stepp 1983]. 3.2.K-Medoids MethodsInk-medoids methods a cluster is represented by one of its points. We have already mentioned that this is an easy solution since it covers any attribute types and that medoidshave embedded resistance against outliers since peripheral cluster points do not affectthem. When medoids are selected, clustersare defined as subsets of points close to respective medoids, and the objective function is defined as the averaged distance or another dissimilarity measure between a point and its medoid.Two early versions of k-medoid methods are the algorithm PAM (Partitioning Around Medoids) and the algorithmCLARA (Clustering LARge Applications) [Kaufman &Rousseeuw 1990]. PAM is iterative optimization that combines relocation of points between perspective clusters with re-nominating the points as potential medoids. The guiding principle for the process is the effect on an objective function, which, obviously, is a costly strategy. CLARA uses several (five) samples, each with 40+2k points, whichare each subjected to PAM. The whole dataset is assigned to resulting medoids, the objective function is computed, and the best system of medoids is retained.Further progress is associated with Ng & Han [1994] who introduced the algorithmCLARANS (Clustering Large Applications based upon RANdomized Search) in the context of clustering in spatial databases. Authors considered a graph whose nodes are the sets ofkmedoids and an edge connects two nodes if they differ by exactly onemedoid. While CLARA compares very few neighbors corresponding to a fixed smallsample, CLARANS uses random search to generate neighbors by starting with an arbitrary node and randomly checking maxneighborneighbors. If a neighbor represents a better partition, the process continues with this new node. Otherwise a local minimum is found, and the algorithm restarts until numlocal local minima are found (value numlocal=2 is recommended). The best node (set of medoids) is returned for the formation of a resulting partition. The complexity of CLARANS is O in terms of number of points. Ester et al. [1995] extended CLARANS to spatial VLDB. They used R*-trees [Beckmann 1990] to relax the original requirement that all the data resides incore memory, which allowedfocusing exploration on the relevant part of the database that resides at a branch of the whole data tree.)(2N3.3.K-Means MethodsThek-means algorithm [Hartigan 1975; Hartigan & Wong 1979] is by far the mostpopular clustering tool used in scientific and industrial applications. The name comesfrom representing each of k clusters C by the mean (or weighted average)cof its points, the so-called centroid. While this obviously does not work well with categoricalattributes, it has the good geometric and statistical sense for numerical attributes. The sum of discrepancies between a point and its centroid expressed through appropriatedistance is used as the objective function. For example, the -normbased objectivejj2L15function, the sum of the squares of errors between the points and the corresponding centroids, is equal to the total intra-cluster variance2:1)(==kjCxjijicxCE.The sum of the squares of errors can be rationalized as (a negative of) log-likelihood for normally distributed mixture model and is widely used in statistics (SSE). Therefore, k-means algorithm can be derived from general probabilistic framework (see sub-section Probabilistic Clustering) [Mitchell 1997]. Note that only means are estimated. A simple modification would normalize individual errors by cluster radii (cluster standarddeviation), which makes a lot of sense when clusters have different dispersions. An objective function based on -normhas many unique algebraic properties. For example,it coincides with pair-wise errors2L2:1,21)(==kjCyxiijiiyxCE,and with the difference between the total data variance and the inter-cluster variance.Therefore, the cluster separation is achieved simultaneously with the cluster tightness.Two versions of k-means iterative optimization are known. The first version is similar to EM algorithm and consists of two-step majoriterations that (1) reassign all the points totheir nearest centroids, and (2) recompute centroids of newly assembled groups.Iterations continue until a stopping criterion is achieved (for example, no reassignmentshappen). This version is known as Forgys algorithm [Forgy 1965] and has manyadvantages:It easily works with any-normpLIt allows straightforward parallelization [Dhillon & Modha 1999] It is insensitive with respect to data ordering.The second (classic in iterative optimization) version of k-means iterative optimizationreassigns points based on more detailed analysis of effects on the objective function caused by moving a point from its current cluster to a potentially new one. If a move has a positive effect, the point is relocated andthe two centroids are recomputed. It is not clear that this version is computationally feasible, because the outlined analysis requiresan inner loop over all member points of involved clusters affected by centroids shifts. However, in case it is known [Duda & Hart 1973; Berkhin & Becher 2002] that all computations can be algebraically reduced to simply computing a single distance! Therefore, in this case both versionshave the same computational complexity.2LThere is experimental evidence that compared with Forgys algorithm, the second (classic) version frequently yields better results [Larsen & Aone 1999; Steinbach et al. 2000].  In particular, Dhillon et al. [2002] noticed that a Forgysspherical k-means (usingcosine similarity instead of Euclidean distance) has a tendency to get stuck when applied to document collections. They noticed that a version reassigning points and immediately recomputing centroids works much better. Figure 3 illustrates both implementations.16Besides these two versions, there have been other attempts to find minimum ofk-meansobjective function. For example, the early algorithm ISODATA [Ball & Hall 1965] used merges and splits of intermediate clusters.The wide popularity of k-means algorithm is well deserved.It is simple, straightforward,and is based on the firm foundation of analysis of variances. The k-means algorithm also suffers from all the usual suspects: The result strongly depends on the initial guess of centroids (or assignments) Computed local optimum is known to be a far cry from the global one It is not obvious what is a good kto use The process is sensitive with respect to outliersThe algorithm lacks scalability Only numerical attributes are coveredResulting clusters can be unbalanced (in Forgys version, even empty)A simple way to mitigatethe affects of clusters initialization wassuggested by Bradley & Fayyad [1998]. First, k-means is performed on several small samples of data with a random initial guess. Each ofthese constructedsystems is then used as a potential initializationfor a union of all the samples. Centroids of the best systemconstructed this way are suggested as an intelligent initial guesses to ignite the k-means algorithm on the full data. Another interesting attempt [Babu & Murty 1993] is based on GA (see below). No initialization actually guarantees global minimum for k-means. As is common to any combinatorial optimization, a logical attemptto cure this problem is to use simulatedannealing [Brown & Huntley 1991]. Zhang [2001] suggested another way to rectifyoptimization process by soft assignment of points to different clusters with appropriate weights (as EM does), rather than by moving them decisively from one cluster to another. The weights take into account how well a point fits into recipient clusters. This process involves so-called harmonic means.111111222222312312333333333111212222323123133233313112122223233133233ReassignpointsRecomputecentroids1111112222223333333311112122222233333333One pointassignmentis changed.Two-step majoriterations (Forgys algorithm)Iterative optimization(with centroidrecomputation)We discuss scalability issues in the section Scalability and VLDB Extensions. For a comprehensive approach in relation to k-means see an excellent study [Bradley et al.1998]. A generic method to achieve scalability is to preprocess or squash the data. Suchpreprocessing usually also takes care of outliers. Preprocessing has its drawbacks. It results in approximations that sometimes negatively affect final cluster quality. Pelleg &Moore [1999] suggested how to directly (without any squashing) accelerate k-meansiterative process by utilizing KD-trees [Moore 1999]. The algorithmX-means [Pelleg &17Moore 2000] goes a step further: in addition to accelerating the iterative process it tries to incorporate a search for the best k in the process itself. While more comprehensivecriteria discussed in the sub-section How Many Clusters? require running independent k-means and then comparing the results (costly experementation),X-means tries to split a part of already constructed cluster based on outcome of BIC criterion. This gives a much better initial guess for the next iteration and covers a user specified range of admissiblek.The tremendous popularity of k-means algorithm has brought to life many other extensions and modifications. Mahalanobis distance can be used to cover hyper-ellipsoidal clusters [Mao & Jain 1996]. Maximum of intra-cluster variances, instead of the sum, can serve as an objective function [Gonzales 1985]. Generalizations that incorporate categorical attributes are known. Sometimes the termk-prototypes is used in this context [Huang 1998]. Modifications which constructs clusters of balanced size are discussed in the sub-section Constrained-Based Clustering.4. Density-Based PartitioningAn open set in the Euclidean space can be divided into a set of its connected components.The implementation of this idea for partitioningof a finite set of points requires conceptsof density, connectivity and boundary. They are closely related to a points nearest neighbors. A cluster, defined as a connected dense component, grows in any direction that density leads. Therefore, density-basedalgorithms are capable of discovering clusters of arbitrary shapes. Also this provides a natural protection against outliers. Figure 4illustrates some cluster shapes that present a problem for partitioningrelocation clustering (e.g.,k-means), but are handled properly by density-based algorithms. They also have good scalability. These outstanding properties are tempered with certain inconveniencies.From a very general data description point of view, a single dense cluster consisting of two adjacent areas with significantlydifferent densities (both higher than a threshold) is not very informative. Another drawback is a lack of interpretability. Anexcellent introduction to density-based methods is contained in the textbook [Han & Kamber 2001].Figure 4. Irregular shapes difficult for k-means areSince density-based algorithms require a metric space, the natural setting for them is spatial data clustering [Han et al. 2001; Kolatch 2001]. To make computations feasible, some index of data is constructed (such as R*-tree). This is a topic of active research.Classic indices were effective only with reasonably low-dimensional data. The algorithmDENCLUE that, in fact, is a blend of a density-based clustering and a grid-based preprocessing is lesser affected by data dimensionality.There are two major approaches for density-based methods. The first approach pins density to a training data point and is reviewed in the sub-section Density-BasedConnectivity. Representative algorithms include DBSCAN, GDBSCAN, OPTICS, and 18DBCLASD. The second approach pins density to a point in the attribute space and isexplained in the sub-section Density Functions. It includes the algorithm DENCLUE. 4.1. Density-Based Connectivity 
Crucial concepts of this section aredensityandconnectivity both measured in terms of local distribution of nearest neighbors.The algorithm DBSCAN (Density Based Spatial Clustering of Applications with Noise) [Ester et al. 1996] targeting low-dimensional spatial data is the major representative in this category. Two input parametersand MinPts are used to define:1) An -neighborhood}),(|{)(=yxdXyxN of the point x,2) A coreobject (a point with a neighborhood consisting of more than MinPtspoints)3)A concept of a point ydensity-reachable from a core object x (a finite sequence of core objects betweenx and y exists such that each next belongs to an -neighborhood of its predecessor)4) A density-connectivity of two points x, y (they should be density-reachable from acommon core object).So defined density-connectivity is a symmetric relation and all the points reachable fromcore objects can be factorized into maximal connected components serving as clusters. The points that are not connected to any core point are declared to be outliers (they are not covered by any cluster). The non-core points inside a cluster represent its boundary.Finally, core objects are internal points. Processing is independent of data ordering. So far, nothing requires any limitations on the dimension or attribute types. Obviously, an effective computing of -neighborhoods presents a problem. However, in the case of low-dimensionalspatialdata, different effective indexation schemes exist (meaningrather than Ofetches per search). DBSCAN relies on R*-tree indexation [Kriegel et al. 1990]. Therefore, on low-dimensional spatial data theoretical complexityof DBSCAN is . Experiments confirm slight super-linear runtime.))(log(NO)(N))log(N(NONotice that DBSCAN relies on -neighborhoods and on frequency count within such neighborhoods to define a concept of a core object. Many spatial databases contain extended objects such as polygons instead of points. Any reflexive and symmetric predicate (for example, two polygons have a non-empty intersection) suffice to define a neighborhood. Additional measures (as intensity of a point) can be used instead of a simple count as well. These two generalizations lead to the algorithm GDBSCAN [Sander et al. 1998], which uses the same two parameters as algorithm DBSCAN. With regard to these two parametersand MinPts, there is no straightforward way to fitthem to data. Moreover, different parts of data could require different parameters  the problem discussed earlier in conjunction with CHAMELEON. The algorithm OPTICS (Ordering Points To Identify the Clustering Structure) [Ankerst et al. 1999] adjusts DBSCAN to this challenge. It builds an augmented ordering of data which is consistentwith DBSCAN, but goes a step further: keeping the same two parameters,MinPts,OPTICS covers a spectrum of all different . The constructed ordering can be used automatically or interactively. With each point, OPTICS stores only two additional fields,19the so-called core- and reachability-distances. For example, the core-distance is the distance to MinPts nearest neighbor when it does not exceeds , or undefined otherwise. Experimentally, OPTICS exhibits runtimeroughly equal to 1.6 of DBSCAN runtime.While OPTICS can be considered as a DBSCAN extension in direction of different local densities, a more mathematically sound approach is to consider a random variable equal to the distance from a point to its nearest neighbor, and to learn its probability distribution. Instead of relying on user-defined parameters,a possible conjuncture is that each cluster has its own typical distance-to-nearest-neighbor scale. The goal is to discover such scales. Such nonparametricapproach is implemented in the algorithm DBCLASD (Distribution Based Clustering of Large Spatial Databases) [Xu et al. 1998].Assuming that points inside each cluster are uniformly distributed which may or may notbe realistic, DBSCLAD defines a cluster as a non-empty arbitrary shape subset in X that has the expected distribution of distance to the nearest neighbor with a required confidence, and is the maximalconnected set with this quality. This algorithm handlesspatial data (minefield example is used).-test is used to check distribution requirement(standard consequence is a requirement for each cluster to have at least 30 points). Regarding connectivity, DBCLASD relies on grid-based approach to generate cluster-approximating polygons. The algorithm contains devices for handling real databases with noise and implementsincremental unsupervised learning. Two venues are used. First, assignments are not final: points can change cluster membership. Second, certain points (noise) are not assigned, but are tried later. Therefore, once incrementally fetched pointscan be revisited internally. DBCLASD is known to run faster than CLARANS by a factorof 60 on some examples. In comparison with much more efficient DBSCAN, it can be 2-3 times slower. However, DBCLASD requires no user input, while empirical search for appropriate parameter requires several DBSCAN runs. In addition, DBCLASD discovers clusters of different densities. 24.2. Density Functions Hinneburg & Keim [1998] shifted the emphasisfrom computing densities pinned to data points to computing density functions defined over the underlying attribute space. Theyproposed the algorithm DENCLUE (DENsity-basedCLUstEring). Along with DBCLASD, it has a firm mathematical foundation. DENCLUE uses a density function=DyDyxfxf),()(that is the superposition of several influence functions. When the f-term depends on x-y, the formula can be recognized as a convolution with a kernel. Examples include a square wave function)/(),(yxyxf=equal to 1, if distance between x andyisless than or equal to , and a Gaussian influence function 222/),(yxeyxf=. This provides a high level of generality: the first example leads to DBSCAN, the second one tok-means clusters! Both examples depend on parameter. Restricting the summation toXkyxyD<=}:{ enables a practical implementation. DENCLUE concentrates on local maxima of density functions called density-attractors and uses a flavor of gradient hill-climbing technique for finding them. In addition to center-definedclusters,20arbitrary-shape clusters are defined as continuations along sequences of points whose local densities are no less than prescribed threshold . The algorithm is stable with respect to outliers and authors show how to choose parameters and. DENCLUEscales well, since at its initial stage it builds a map of hyper-rectangle cubes with edge length 2. For this reason, the algorithm can be classified as a grid-based method.Applications include high dimensional multimedia and molecular biology data. While no clustering algorithm could have less than O complexity, the runtime of DENCLUE scales withN sub-linearly! The explanation is that though all the points are fetched, the bulk of analysis (in clustering stage) involves only points in highly populated areas. )(N5. Grid-Based Methods In the previous section crucial concepts of density, connectivity, and boundary were used which required elaborate definitions. Another way of dealing with them is to inherit thetopology from the underlying attribute space. To limit the search combinations, multi-rectangular segments are considered. Recall that a segment (alsocube, cell, region). is a direct Cartesian product of individual attribute sub-ranges (contiguous in case of numerical attributes). Since some binning is usually adopted for numerical attributes, methods partitioning space are frequently called grid-based methods. The elementarysegment corresponding to single-bin or single-value sub-ranges is called a unit.Overall, we shift our attention from data to space partitioning. Data partitioning isinduced by points membership in segments resulted from space partitioning, while space partitioning is based on grid-characteristics accumulated from input data. One advantageof this indirect handling (data grid-data  space-partitioning data-partitioning) isthat accumulation of grid-data makes grid-based clustering techniques independent of data ordering. In contrast, relocation methods and all incremental algorithms are very sensitive with respect to data ordering. While density-based partitioning methods work best with numerical attributes, grid-based methods work with attributes of different types.To some extent, the grid-based methodology reflects a technical point of view. The category is eclectic: it contains both partitioning and hierarchical algorithms. The algorithm DENCLUE from the previous section uses grids at its initial stage. The very important grid-based algorithm CLIQUE and its descendent, algorithm MAFIA, are presented in the sectionClustering High Dimensional Data. In this section we survey algorithms that use grid-based technique as their major principle instrument.BANG-clustering [Schikuta & Erhart 1997] improves the similar hierarchical algorithm GRIDCLUST [Schikuta 1996]. Grid-based segments are used to summarize data. Thesegments are stored in a special BANG-structure that is a grid-directory incorporatingdifferent scales. Adjacent segments are neighbors. If a common face has maximumdimension they are called nearest neighbors. More generally, neighbors of degree between 0 and d-1 can be defined. The density of a segment is defined as a ratio betweennumber of points in it and its volume. From the grid-directory, a dendrogram is directly calculated.The algorithm STING (STatistical INformation Grid-based method) [Wang et al. 97] works with numerical attributes (spatial data) and is designed to facilitate region 21oriented queries. In doing so, STING constructs data summaries in a way similar toBIRCH. It, however, assembles statistics in a hierarchical tree of nodes that are grid-cells. Figure 5 presents the proliferation of cells in 2-dimensional space and the construction of the corresponding tree. Each cell has four (default) children and stores a point count, and attribute-dependent measures: mean, standard deviation, minimum, maximum, and distribution type. Measures are accumulated starting frombottom level cells, and furtherpropagate to higher-level cells (e.g., minimum is equal to a minimum among the children-minimums). Only distribution type presents a problem  -test is used afterbottom cell distribution types are handpicked. When the cell-tree is constructed (in time), certain cells are identified and connected in clusters similar to DBSCAN. If thenumber of leaves is K, the cluster construction phase depends on K and not on N. Thisalgorithm has a simple structure suitable for parallelization and allows for multi-resolution, though defining appropriate granularity is not straightforward. STING hasbeen further enhanced to algorithm STING+ [Wang et al. 1999] that targets dynamicallyevolving spatial databases, and uses similar hierarchical cell organization as its predecessor. In addition, STING+ enables activedata mining.2)(NOFigure 5. Cell generation and treeconstruction in STING. To do so, it supports user defined trigger conditions (e.g., there is a region where at least10 cellular phones are in use per square mile with total area of at least 10 square miles, or usage drops by 20% in a described region). The related measures, sub-triggers, are stored and updated over the hierarchical cell tree. They are suspended until the trigger fires with user-defined action. Four types of conditions are supported: absolute and relative conditions on regions (a set of adjacent cells), absolute and relative conditions on certain attributes.The algorithm WaveCluster [Sheikholeslami et al. 1998] works with numerical attributes and has an advanced multi-resolution. It is also known for other outstanding properties:High quality of clusters Ability to work well in relatively high dimensional spatial data Successful handling of outliers  complexity)(NOWaveCluster is based on ideas of signal processing. It applies wavelet transforms to filterthe data. Notice that high-frequency parts of a signal correspond to boundaries, while low 22frequency high amplitude parts of a signal correspond to clusters interiors. Wavelettransform provides us with useful filters. For example, hat-shape filter forces dense areas to serve as attractors and simultaneouslysuppresses lesser dense boundary areas. Aftergetting back from signal to attribute space this makes clusters more sharp and eliminatesoutliers. WaveClustergoes in stages. It: 1)Bins every dimension and assigns points to corresponding units 2)Applies discrete Wavelet transform to so accumulated units3)Finds connected components (clusters) in a transformed attribute space (corresponding to a certain level of resolution) 4) Assigns pointsThe algorithms complexity is  for low dimensions, but exponentially grows with the dimension.)(NOThe hierarchy of grids allows definition of the HausdorffFractal Dimension (HFD) [Schalkoff 1991]. HFD of a set is the negative slope of a log-log plot of the number of cells (occupied by a set) as a function of a grid size r. A fast algorithm (boxcounting) to compute HFD was introduced in [Liebovitch & Toth 1989]. The concept of HFD is fundamental to the FC (Fractal Clustering) algorithm [Barbara & Chen 2000] for numeric attributes, which works with several layers of grids (cardinality of each dimension is increased 4 times with each nextlayer). Although only occupied cells are kept to save memory, memory usage is still a significant problem. FC starts withinitializing ofkclusters. Initialization threshold and a data sample are used at this stage to come up with the appropriate k. Then FC scans full data incrementally. It tries to add an incoming point to each cluster that results in certain increase of HFD. If the smallestincrease exceeds a threshold, a point is declared an outlier; otherwise a point is assigned so that HFD would be minimally impacted. The FC algorithm has few appealing properties:)(rCellIncremental structure (batches of data are fetched into core memory)Suspendable nature always ready for on-line assignmentsAbility to discover clusters of irregular shapes complexity)(NOIt also has a few problems: Data order dependency Strong dependency on clusters initialization Dependency on parameters (threshold used in initialization, and)6. Co-Occurrence of Categorical Data In this section we talk about categorical data, which frequently relates to the concept of avariable sizetransaction that is a finite set of elements calleditems from a common itemuniverse. For example, market basket data has this form. Every transaction can bepresented in a point-by-attribute format, by enumerating all itemsj, and by associatingwith a transaction the binary attributes that indicate whether j-items belong to a transaction or not. Such representation is sparse and two random transactions have very few items in common. This is why similarity (sub-sectionProximity Measures) between 23them is usually measured by Jaccard coefficient 212121),(TTTTTTsim=. Commonto this and others examples of point-by-attribute format for categorical data, is highdimensionality, significant amount of zero values, and small number of common values between two objects. Conventional clustering methods, based on similarity measures, do not work well. Since categorical/transactional data is important in customer profiling, assortment planning, Web analysis, and other applications, different clustering methodsfounded on the idea of co-occurrence of categorical data have been developed.The algorithm ROCK (Robust Clustering algorithm for Categorical Data) [Guha et al.1999] deals with categorical data and has many common features with the algorithm CURE (section Hierarchical Clustering): (1) it is a hierarchical clustering, (2) agglomeration continues until specified numberkof clusters is constructed, and (3) it uses data sampling in the same way as CURE does. ROCK defines a neighbor of a point xas a pointysuch that),(yxsim for some threshold , and proceeds to a definition of linkslink(x,y)between two points x, y equal to number of their common neighbors. Clusters consist of points with a high degree of connectivity  pair-wise points inside a cluster have on average a high number oflinks. ROCK utilizes the objective function)(21,:1/),(fjCyxkjjCyxlinkCEj+==,wheref() is a data dependent function. Erepresents specifically normalized intra-connectivity measure.To put this formula into perspective, noticethat linkage metrics normalize the aggregate measures by the number of edges. For example, the average link metric is the sum of distances between each point C and each point in C divided by the factor ijjiCCL=.The value L can be rationalized on a more general level. If the expected number of edgesper cluster is ]2,1[,C, then the aggregate inter-cluster similarity has to be normalized by the factor ()jijCCCiC+ representing the number of inter-cluster edges. The average link normalization factor Lcorresponds to 2=, the highest expected connectivity indeed. The ROCK objective function uses the same idea, but fits it with parameters. Whether a model fits particular data is an open question. Frequently, different regions of data have different properties, and therefore, global fit is impossible.ROCK relies on an input parameter and on a function f() that have to fit data. It has acomplexity ofO, where coefficient c is a product of average and maximum number of neighbors.))log(samplesampleN+(samplemNc2NmThe algorithm SNN (Shared Nearest Neighbors) [Ertoz et al. 2002] blends a density-based approach with the idea of ROCK. SNN sparsifies similarity matrix (therefore,unfortunately resulting in  complexity) by only keeping K-nearest neighbors, and thus derives the total strength of links for each x.)(2NOFor this matter, the idea to use shared nearest neighbors in clustering was suggested by Jarvis & Patrick [1973] long ago. See also [Gowda & Krishna 1978]. 24The algorithm CACTUS (Clustering Categorical Data Using Summaries) [Ganti et al.1999a] looks for hyper-rectangular clusters (called interval regions) in point-by-attribute data with categorical attributes. In our terminology such clusters are segments. CACTUS is based on the idea of co-occurrence for attribute-value pairs. (Implicitly uniformdistribution within the range of values for each attribute is assumed). Two values a, b of two different attributes are strongly connected if the number of data points having both aandb is larger than the frequency expected under independency assumption by a user-defined margin. This definition is extended to subsets A,Bof two different attributes(each value pair a has to be strongly connected), to segments (each 2D projection is strongly connected), and to the similarity of pair of values of a singleattribute via connectivity to other attributes. The cluster is defined as the maximalstrongly connected segment having at least times more elements than expected fromthe segment under attributes independency assumption. CACTUS uses data summaries to generate all the strongly connected and similar attribute value pairs. As a second step, a heuristic is used to generate maximum segments. The complexity of the summarizationphase is O, where the constantc depends on whether all the attribute-valuesummaries fit in memory (one data scan), or not (multiple data scans).BbA,)(cNThe situation with clustering transactional data becomes more aggravated when size of item universe grows. Here we have a classic case of low separation in high-dimensionalspace (sectionClustering High Dimensional Data). With categorical data, the idea of auxiliary clustering of items, or more generally of categorical attribute values, gained popularity. It is very similar to the idea of co-clustering (sub-section Co-Clustering).This, formally speaking, preprocessing step becomes the major concern, while the following data clustering remains a lesser issue. We start with the development of Han et al. [1997] that exemplifies this approach. After items are clustered (major step), a very simple method to cluster transactions themselvesis used: each transactionTis assigned to a cluster of items having most in commonwithT, as defined by a function jCjjCCT/}. Other choices come to mind, but again the primary objective is to find item groups. To achieve this association rules and hyper-graphmachineries are used. First, frequent item-sets are generated from transactional data. A hyper-graph  can be associated with item universe, so that vertices Vare items. In a common graph, pairs of vertices are connected by edges, but in a hyper-graph several vertices are connected by hyper-edges. Hyper-edge inHcorrespondsto a frequent item-set{ and has a weightequal to an average of confidences among all association rules involving this item-set. A solution to the problem of k-waypartitioning of a hyper-graphH is provided by algorithm HMETIS [Karypis et al. 1997].),(EVH=svv,...,1EeThe algorithm STIRR (Sieving Through Iterated Reinfircement) [Gibson et al. 1998] deals with co-occurrence for d-dimensional categorical objects, tuples. Extension totransactional data is obvious. It uses beautiful technique from functional analysis. Define configurations as weights  over all different values v for all dattributes.Consider, for example, a value vof the first attribute. The tuples }{vww=),...,,(11=duuvx25containingvresult in a weight update , where termsdepend on a combining operator. An example of a combining operator is. So the weight is redistributed among different values. The major iteration scans the data X and results in the propagation of weights betweendifferent nodes  equal to a described update followed by the normalization of weights among the values of each attribute. Functionfcan be considered as a dynamicsystem (non-linear, ifis non-linear). STIRR relies on a deep analogy with the spectralgraph partitioning. For linear dynamic systemdefined over the graph, a re-orthogonalization Gram-Schmidt process can be engaged to compute its eigenvectors that introduces negative weights. The few first non-principal eigenvectors (non-principle basins) define graph partitioning corresponding to positive/negative weights. The process works like this: few weights (configurations)  are initialized. A major iterationupdates them, , and new weights are re-orthogonalized. The process continues untilfixed point of a dynamic system is achieved. Non-principle basins are analyzed. In STIRR a dynamic system instead of association rules formalizes co-occurrence.Additional references related to spectral graph partitioning can be found in[Gibson et al. 1998]. As the convergence of the process can cause a problem, the further progress is related to the modification of the dynamic system that guarantees it [Zhang et al. 2000]. =xvxxvzw,{qvqww=),...,(11=duuxwwz1111...),...,(++=ddwwww)(wfwnew=)(qqnewwfw=}7. Other Clustering Techniques A number of other clustering algorithms have been developed. Some deal with the specific application requirements.Constraint-based clustering belongs to this category. Others have theoretical significance or are mostly used in other than data miningapplications. We briefly discuss these developments in the sub-sectionsRelation toSupervised Learning,Gradient Descent and ANN, and Evolutionary Methods. Finally, in the sub-section Other Developments we very briefly mention developments that simply do not fit well in our classification.7.1. Constraint-Based Clustering In real-world applications customers are rarely interested in unconstrained solutions.Clusters are frequently subjected to some problem-specific limitations that make themsuitable for particular business actions. Building of so conditioned cluster partitions is the subject of active research; for example, see survey [Han et al. 2001].The framework for the constrained-based clustering is introduced in [Tung et al. 2001].The taxonomy of clustering constraints includes constraints on individual objects (e.g., customer who recently purchased) and parameter constraints (e.g., number of clusters) that can be addressed through preprocessing or external cluster parameters. Thetaxonomy also includes constraints on individual clusters that can be described in termsof bounds on aggregate functions (min, avg, etc.) over each cluster. These constrains are essential, since they require a new methodology. In particular, an existentialconstraint is 26a bound from below on a count of objects of a certain subset (i.e. frequent customers) in each cluster. Iterative optimization used in partitioning clustering relies on movingobjects to their nearest cluster representatives. This may violate such constraint. A methodology of how to resolve this conflict is developed in [Tung et al. 2001]. The most frequent requirement is to bound number of cluster points from below. Unfortunately,k-means algorithm, which is used most frequently, sometimes provides a number of very small (in certain implementations empty) clusters. The modification of thek-means objective function and of k-means updates that incorporate lower limits on cluster volumes is suggested in [Bradley et al. 2000]. This includes soft assignments of data points with coefficients subject to linear program requirements. Banerjee & Ghosh [2002] presented another modification to k-means algorithm. Their objective functioncorresponds to an isotropic Gaussian mixture with widths inversely proportional to numbers of points in the clusters. The result is the frequency sensitive k-means. Still another approach to building balanced clusters is to convert a task into a graph-partitioning problem [Strehl & Ghosh 2000].Important constraint-based clustering application is to cluster 2D spatial data in the presence of obstacles. Instead of regular Euclidean distance, a length of the shortest path between two points can be used as an obstacle distance. The COD (Clustering withObstructed Distance) algorithm [Tung et al. 2001] deals with this problem. It is best illustrated by the figure 6, showing the difference in constructing three clusters in absenceof obstacle (left) and in presence of a river with a bridge (right). Figure 6. Obstacle (river with the bridge) makes a difference.7.2. Relation to Supervised Learning Both Forgys k-means implementation and EM algorithms are iterative optimizations.Both initializek models and then engage in a series of two-step iterations that: (1) reassign (hardorsoft) data points, (2) update a combined model. This process can be generalized to a frameworkrelating clustering with predictive mining [Kalton et al. 2001]. The model update is considered as the training of a predictive classifier based on current assignments serving as the target attribute values supervising the learning. Points reassignments correspond to the forecasting using the recently trained classifier. Liu et al. [2000] suggested another elegant connection to supervised learning. They considered binary target attribute defined as Yes on points subject to clustering, and defined as Noon non-existent artificial points uniformly distributed in a whole attribute space. A decision tree classifier is applied to the full synthetic data. Yeslabeled leavescorrespond to clusters of input data. The new technique CLTree (CLustering based on 27decision Trees) resolves the challenges of populating the input data with artificial Nopoints such as: (1) adding points gradually following the tree construction; (2) makingthis process virtual (without physical additions to input data); (3) problems with uniformdistribution in higher dimensions.7.3. Gradient Descent and Artificial Neural NetworksSoft reassignments make a lot of sense, if k-means objective function is slightly modifiedto incorporate (similar to EM) fuzzy errors, that is if it accounts for distances not only to the closest, but also to the less fit centroids:22:1:1)(ijNikjjicxCE===Exponential probabilities are defined based on Gaussian models. This makes the objective function differentiable with respect to means and allows application of general gradient decent method. Marroquin & Girosi [1993] presented a detailed introduction to this subject in the context ofvector quantization.  Gradient decent method in k-means isknown as LKMA (Local K-Means Algorithm). At each iteration t,it modifies meanstjc=++=Niijtjittjtjwcxacc:121)(  or21)(ijtjittjtjwcxacc+=+in the direction of gradient decent. In the second case one x is selected randomly. Scalars  satisfy certain monotone asymptotic behavior and converge to zero, coefficients waredefined trough  [Bottou & Bengio 1995]. Such updates are also used in a different context of artificialneural network (ANN) clustering, namely SOM (Self-Organized Map) [Kohonen 1990]. SOM is popular in vector quantization. Bibliography related to this dynamic field can be found in the monograph [Kohonen 2001]. Wewill not elaborate here about SOM except for two important features: (1) SOM uses incremental approach  points (patterns) are processed one-by-one; (2) SOM allows to map centroids into 2D plane that provides for a straightforward visualization. In addition to SOM, other ANN developments, such as adaptive resonance theory [Carpenter et al. 1991], have relation to clustering. For further discussion see [Jain & Mao 1996]. ta7.4. Evolutionary MethodsSubstantial information on simulated annealing in the context of partitioning (main focus) or hierarchical clustering is accumulated, including the algorithm SINICC(SImulation of Near-optima for InternalClustering Criteria) [Brown & Huntley 1991].The perturbation operator used in general annealing has a simple meaning in clustering: it amounts to a relocation of a point from its current to a new randomly chosen cluster (very similar to k-means scheme). SINICC also tries to address the interesting problem of choosing the most appropriate objective function. It has a real application  surveillance monitoring of ground-based entities by airborne and ground-based sensors. Similar to simulating annealing is the so-calledtabusearch [Al-Sultan 1995]. Genetic Algorithms(GA) [Goldberg 1989] are also used in cluster analysis. An example is the GGA (Genetically Guided Algorithm) for fuzzy and hard k-means [Hall et al.281999]. This article can be used for further references. Sarafis et al. [2002] applied GA in the context of k-means objective function. A population is a set of k-means systemsrepresented by grid segments instead of centroids. Every segment is described by drules(genes), one per attribute range. The population is improved through mutation and crossover specifically devised for these rules. Unlike in normalk-means, clusters can have different size and elongation; however, shapes are restricted to segments, a far cry from density-based methods. GA were also applied to clustering of categorical data using so-called generalized entropy to define the dissimilarity [Cristofor and Simovici 2002].Evolutionary techniques rely on certain parameters to empirically fit data and have high computational costs that limit their application in data mining. However, usage of combined strategies (e.g., generation of initial guess fork-means) has been attempted[Babu & Murty 1993; Babu & Murty 1994]. Usage of GA with variable length genome to simultaneously improvek-means centroids and k itself [Lee & Antonsson 2000] also has a merit in comparison with running multiple k-means to determine a k, since changes in khappen before full convergence is achieved. 7.5. Other Developments There are other developments that in terms of their performance qualify for data mining.For 2D spatial data (for example, GIS database) the algorithm AMOEBA [Estivill-Castro& Lee 2000] uses Delaunay diagram (the dual of Voronoi diagram) to represent data proximity and has  complexity.))log((NNOHarel & Koren [2001]suggested an approach related to agglomerative hierarchical graph methodology that they showed to successfully find local clusters in 2D. As above, consider a connectivity graph G. Using Delaunay diagram or keeping with any point only its K-nearest neighbors sparsifies the graph. The method relies on randomwalk to find separating edges F so that clusters become connected components of .),(EX=),(FEVG=8. Scalability and VLDB Extensions Clustering algorithms face problems of scalability both in terms of computing time and memory requirements. In data mining reasonable runtime and ability to use certainlimited core memory become especially important. There have been many interesting attempts to extend clustering to very large databases (VLDB), which can be divided into: - Incrementalmining,- Data squashing,- Reliable sampling.The algorithm DIGNET [Thomopoulos et al. 1995; Wann & Thomopoulos 1997] (compare with the leaderclustering algorithm in [Hartigan 1975]) is an example of incremental unsupervised learning. This meansthat it handles one data point at a time,and then discards it. DIGNET usesk-means cluster representation without iterative optimization. Centroids are insteadpushed or pulled depending on whether they loose or win each next coming point. Such on-line clustering needs only one data pass, but 29strongly depends on data ordering, and it can result in sub-quality clusters. However, it handles outliers, clusters can be dynamically born or discarded, and the training process is resumable. This makes it very appealing for dynamic VLDB.  Some further tools can be used to improve obtained clusters. Data squashing techniques scan data to compute certain data summaries (sufficientstatistics) [DuMouchel et al. 1999]. The obtained summaries are then used instead of the original data for further clustering. The pivotal role here belongs to the algorithm BIRCH (Balanced Iterative Reduction and Clustering using Hierarchies) [Zhang et al. 1996; Zhang et al. 1997]. This work had a significant impact on overall direction of scalability research in clustering. BIRCH creates a height-balanced tree of nodes that summarizedata by accumulating its zero, first, and second moments. A node, called Cluster Feature(CF), is a tight small cluster of numerical data. The construction of a tree residing in core memory is controlled by some parameters.A new data point descends along the tree to the closest CF leaf. If it fits the leaf well and if the leaf is not overcrowded, CF statisticsare incremented for all nodes from the leaf to the root. Otherwise a new CF is constructed. Since the maximum number of children per node (branching factor) is limited, one or several splits can happen.When the tree reaches the assigned memorysize, it is rebuilt and a threshold controllingwhether a new point is assigned to a leaf or starts a new leaf is updated to a coarser one. The outliers are sent to disk, and refitted gradually during tree rebuilds. The final leaves constitute input to any algorithm of choice. The fact that a CF-tree is balanced allows the log-efficient search. BIRCH depends on parameters that control CF tree construction (branching factor, maximum of points per leaf, leaf threshold), and it also depends on data ordering. When the tree is constructed (one data pass), it can be additionally condensed in the optional 2nd phase to further fit desired input cardinality of post-processing clustering algorithm. Next, in the 3rd phase a global clustering of CF (considered as individual points) happens. Finally, certain irregularities (for example, identical points getting to different CFs) can be resolved in an optional 4th phase. It makes one or more passes through data reassigning points to best possible clusters, as k-means does. The overall complexity is O.Summarization phase of BIRCH was extended to mixed numerical and categoricalattributes [Chiu et al. 2001]. )(NA full interface between VLDB and relocation clustering (as k-means) includes followingrequirements [Bradley et al. 1998]. Algorithm has to: Take one (or less  early termination) data scan Provide on-line solution: some solution in-progress should always be available Be suspendable, stoppable, resumableBe able to incorporate additional data incrementallyBe able to work in prescribed memory bufferUtilize different scanning modes (sequential, index, sample)Be able to operate in forward-only cursor over a view of databaseThe article suggests data compression that accumulates sufficient statistics like BIRCH does, but makes it in phases. Points that are compressed over the primary stage are discarded.  They can be attributed to their clusters with very high confidence even if other points would shift. The rest is taken care of in the secondary phase, which tries to find30dense subsets by k-means method with higher than requested k. Violators of this stage are still kept in retained set (RT) of singletons to be analyzed later.BIRCH-like preprocessing substantially relies on vector-space operations. Meanwhile, in many applications, objects (for example, strings) belong to a metric space. In other words, all we can do with data points is to compute distances between them. Ganti et al.[1999b] proposed BIRCH-type data squashing BUBBLE for VLDB in metric spaces.Each leaf of the BUBBLE-tree is characterized by:1)Number of its points 2) Medoid (calledclustroid) that delivers a minimum to an error  a squared distance between it and all other points belonging to the leaf 3)Radius equal to the square root of an average error per a pointThe problem to overcome is how to insert new points in the absence of a vector structure. BUBBLE uses a heuristic that relates to a distance preserving embedding of leaf points into a low-dimensional Euclidean vector space. Such embedding is known as isometricmap in geometry and as multidimensional scaling in statistics. Certain analogy can also be made with embeddings used in support vector machines. While Euclidean distance (used in BIRCH) is cheap, the computation of a distance in a metric space (for example, edit distance for strings) can be expensive. Meanwhile, every insertion requires tocompute distances to all the nodes descending to a leaf. The similar algorithm BUBBLE-FM handles this difficulty. It relaxes the computations by using approximate isometricembedding. This is possible due to the algorithm FastMap [Faloutsos & Lin 1995].In the context of hierarchical density-based clustering in VLDB, Breunig et al. [2001] analyzed data reduction techniques such as sampling and BIRCH summarization, and noticed that they result in deterioration of cluster quality. To cure this, they approacheddata reduction through accumulation of data bubbles that are summaries of localinformation about distances and nearest neighbors. A data bubble contains an extent, thedistance from a bubbles representative to most points in X, and the array of distances to each of MinPtsnearest neighbors. Data bubbles are then used in conjunction with the algorithm OPTICS (see sub-section Density-Based Connectivity).Grid-methods also generate data summaries, though their summarization phase relates to units and segments and not to CFs. Therefore, they are scalable.Many algorithms use old-fashioned samplingwith or without rigorous statistical reasoning. It is especially handy for different initializations as in CLARANS (sub-section K-Medoids Methods), Fractal Clustering (section Grid-Based Methods), or k-means[Bradley & Fayyad 98]. Notice that when clusters are constructed using whatever sample, assigning the whole data to the most appropriate clusters minimally adds the termto the overall complexity.)(NOSampling has got a new life with the adoption by the data mining community of a special uniform check to control its adequacy. This check is based on Hoffding or Chernoffbounds [Motwani & Raghavan 1995] and says that, independent of the distribution of a real-valued random variable Y,the average of n independent observations lies within of the actual mean,0RY31=njjYnY:11with probability 1- as soon as nR2/)/1ln(2=.These bounds were used in the clustering algorithm CURE [Guha et al. 1998] and in the development of scalable decision trees in predictive mining [Hulten et al. 2001]. In thecontext of balanced clustering, a statistical estimation of a sample size is provided in [Banerjee & Ghosh 2002]. Due to their nonparametric nature, the bounds have a ubiquitous significance. 9. Clustering High Dimensional Data The objects in data mining could have hundreds of attributes. Clustering in such high dimensional spaces presents tremendous difficulty, much more so than in predictive learning. In decision trees, for example, irrelevant attributes simply will not be picked fornode splitting, and it is known that they do not affect Nave Bayes as well. In clustering,however, high dimensionality presents a dual problem. First, under whatever definition of similarity, the presence of irrelevant attributes eliminates any hope on clusteringtendency. After all, searching for clusters where there are no clusters is a hopeless enterprise. While this could also happen with low dimensional data, the likelihood of presence and number of irrelevant attributes grows with dimension.The second problem is the dimensionality curse that is a loose way of speaking about a lack of data separation in high dimensional space. Mathematically, nearest neighborquery becomesunstable: the distance to the nearest neighbor becomes indistinguishable from the distance to the majority of points [Beyer et al. 1999]. This effect starts to be severe for dimensions greater than 15. Therefore, construction of clusters founded on the concept of proximity is doubtful in such situations. For interesting insights into complications of high dimensionaldata, see [Aggarwal et al. 2000]. Basic exploratory data analysis (attribute selection) preceding the clustering step is thebest way to address the first problem of irrelevant attributes. We consider this topic in the sectionGeneral Algorithmic Issues. Below we present sometechniques dealing with asituation when the number of already pre-selected attributes d is still high.In the sub-sectionDimensionality Reduction we talk briefly about traditional methods ofdimensionality reduction. In the sub-section Subspace Clustering we review algorithmsthat try to circumvent high dimensionalityby building clusters in appropriate subspaces of original attribute space. Such approach has a perfect sense in applications, since it isonly better if we can describe data by fewer attributes. Still another approach that divides attributes into similar groups and comes up with good new derived attributes representing each group is discussed in the sub-section Co-Clustering.Important source of high dimensional categorical data comes from transactional (market basket) analysis. Idea to group items very similar to co-clustering has already been discussed in the section Co-Occurrence of Categorical Data.329.1. Dimensionality Reduction 
When talking about high dimensionality,how high is high?Many spatial clustering algorithms depend on indices in spatial datasets (sub-sectionData Preparation) to facilitate quick search of the nearest neighbors. Therefore, indices can serve as good proxies with respect to dimensionality curse performance impact. Indices used in clustering algorithms are known to work effectively for dimensions below 16. For a dimensiond> 20 their performance degrades to the level of sequential search(though newer indices achieve significantly higher limits). Therefore, we can arguably claim that data with more than 16 attributes is high dimensional.How large is the gap? If we are dealing with a retail application, 52-weeks sales volumesrepresent a typical set of features, which is a special example of more general class of time series data. In customer profiling dozens of generalized item categories plus basicdemographics result in at the least 50-100 attributes. Web clustering based on site contents results in 200-1000 attributes (pages/contents) for modest Web sites. Biology and genomic data can have dimensions that easily surpass 2000-5000 attributes. Finally,text mining and information retrieval also deal with many thousands of attributes (wordsorindex terms). So, the gap is significant.Two general purpose techniques are used to fight high dimensionality: (1) attributestransformations and (2) domain decomposition.Attribute transformations are simple functions of existent attributes. For sales profilesand OLAP-type data, roll-ups as sums or averages over time intervals (e.g., monthlyvolumes) can be used. Due to a fine seasonality of sales such brute force approaches rarely work. In multivariate statisticsprincipal components analysis (PCA) is popular [Mardia et al. 1980; Joliffe 1986], but this approach is problematic since it leads to clusters with poor interpretability. Singular value decomposition (SVD) technique is used to reduce dimensionality in information retrieval [Berry et al. 1995; Berry & Browne 1999] and statistics [Fukunaga 1990]. Low-frequency Fourier harmonics in conjunction with Parsevals theorem are successfully used in analysis of time series [Agrawal et al.1993], as well as wavelets and other transformations [Keogh et al. 2001]. Domain decomposition divides the data into subsets, canopies, [McCallum et al. 2000] using some inexpensive similarity measure, so that the high dimensional computationhappens over smaller datasets. Dimension stays the same, but the costs are reduced. Thisapproach targets the situation of high dimension, large data, and many clusters. 9.2. Subspace Clustering Some algorithms better adjust to high dimensions. For example, the algorithm CACTUS (sectionCo-Occurrence of Categorical Data) adjusts well since it defines a cluster onlyin terms of a clusters 2D projections. In this section we cover techniques that are specifically designed to work with high dimensional data.The algorithm CLIQUE (Clustering In QUEst) [Agrawal et al. 1998] for numericalattributes is fundamental in subspace clustering. It marries the ideas of: Density-based clustering33Grid-based clustering Induction through dimensions similar to Apriorialgorithm in association rules MDL principle to select appropriate subspaces Interpretability of clusters in terms of DNF representation CLIQUE starts with the definition of a unit  elementary rectangular cell in a subspace.Only units whose densities exceed a threshold  are retained. A bottom-up approach of finding such units is applied. First, 1-dimensional units are found by dividing intervals in  equal-width bins (a grid). Both parameters and are the algorithms inputs. The recursive step fromq-1-dimensional units toq-dimensional units involves self-join of q-1units having firstcommonq-2 dimensions (Apriori-reasoning). All the subspaces are sorted by their coverage and lesser-covered subspaces are pruned. A cut point is selected based on MDL principle. A cluster is defined as a maximal set of connected dense units. It is represented by a DNF expression that is associated with a finite set of maximalsegments (calledregions) whose union is equal to a cluster. Effectively, CLIQUE resultsin attribute selection (it selects several subspaces) and produces a view of data from different perspectives! The result is a series of cluster systems in different subspaces.This versatility goes more in vein with data description rather than with data partitioning:different clusters overlap. If qis a highest subspace dimension selected, the complexity of dense units generations is . Identification of clusters is a quadratic task interms of units. )(qNconstOq+The algorithm ENCLUS (ENtropy-based CLUStering) [Cheng et al. 1999] follows in the footsteps of CLIQUE, but uses a different criterion for subspace selection. The criterion is derived from entropy related considerations: the subspace spanned by attributes with entropy smaller than a threshold is considered good for clustering. Any subspace of a good subspace is also good, since qAA,...,1),...,(1qAAH<=),...,(),...,|(),...,(),...,(111111qqqqqAAHAAAHAAHAAH.Low entropy subspace corresponds to a skewed distribution of unit densities. The computational costs of ENCLUS are high. The algorithm MAFIA (Merging of Adaptive Finite Intervals) [Goil et al. 1999; Nagesh et al. 2001] significantly modifies CLIQUE. It starts with one data pass to construct adaptive grids in each dimension. Many (1000) bins are used to compute histograms by reading blocks of data in core memory, which are then merged together to come up with a smaller number of variable-size bins than CLIQUE does. The algorithm uses a parameter, called cluster dominance factor, to select bins that are -times more densely populated relative to their volume than on average. These are q=1 candidate dense units(CDUs). Then MAFIA proceeds recursively to higher dimensions (every time a data scan is involved). The difference between MAFIA and CLIQUE is that to construct a new q-CDU, MAFIA tries twoq-1-CDUs as soon as they share any (not onlyfirst dimensions)q-2-face. This creates an order of magnitude more candidates. Adjacent CDUs are merged into clusters and clusters that are proper subsets of the higher dimension clusters are eliminated. The parameter (default value 1.5 works fine) presents no problem in comparison with global density threshold used in CLIQUE. Reporting for a range of  in 34a single run is supported. If qis a highest dimensionality of CDU, the complexity is.)(qNconstOq+The algorithm OPTIGRID [Hinneburg & Keim 1999] uses data partitioning based on divisive recursion by multi-dimensional grids. Authors present a very good introduction into the effects of high-dimension geometry. Familiar concepts, as for example, uniformdistribution, become blurred for large d. OPTIGRID uses density estimations in the sameway the algorithm DENCLUE (by the same authors) does. It primarily focuses on separation of clusters by (hyper) planes that are not necessarily axes parallel. To find such planes consider a set of contracting linear projectors (functionals)1,,...,1jkPPP)(yxofthe attribute space Aat a 1D line. For a density kernel of the form (a tool oftrade in DENCLUE) and a contracting projection, density induced after projection ismore concentrated. A cutting plane is chosen so that it goes through the point of minimaldensity and discriminates two significantly dense half-spaces. Several cutting planes are chosen, and recursion continues with each subset of data.KThe algorithm PROCLUS (PROjected CLUstering) [Aggarwal et al. 1999a] associates with a subset C a low-dimensional subspace such that the projection of Cinto thesubspace is a tight cluster. The subset  subspace pair when exists constitutes a projectedcluster. The numberk of clusters and the average subspace dimensionlare user inputs.The iterative phase of the algorithm deals with finding k good medoids, each associated with its subspace. A sample of data is used in a greedy hill-climbing technique.Manhattan distance divided by the subspace dimension is a useful normalized metric for searching among different dimensions. An additional data pass follows after iterative stage is finished to refine clusters including subspaces associated with the medoids.The algorithm ORCLUS (ORiented projected CLUSter generation) [Aggarwal & Yu 2000] uses a similar approach of projected clustering, but employs non-axes parallel subspaces of high dimensional space. In fact, both developments address a more generic issue: even in a low dimensional space, different portions of data could exhibit  clustering tendency in different subspaces (consider several non-parallel non-intersecting cylindersin 3D space). If this is the case, any attribute selection is doomed. ORCLUS has a k-means-like transparent model that defines clusters as sets of points (partition) that have low sum-of-squares of errors (energy) in a certain subspace. More specifically, for ,and directions (specific to C), the projection is defined as {.The projection only decreasesenergy. SVD diagonalization can be used to find directions (eigenvectors) corresponding to the lowest leigenvalues of the covariance matrix. In reality, the algorithm results in Xpartitioning (the outliers excluded) intokclusterstogether with their subspace directions . The algorithm builds more than k clusters,with larger than l-dimensionalE gradually fitting the optimal subspace and requested k.Though suggestion of picking a good parameterlis provided, uniform l is a certainliability.Cx}lexjC},...,{1leeE=,...,1exjEAny other comparison aside, projected clusters provide data partitioning, while clustersystems resulted from CLIQUE overlap.359.3. Co-Clustering In OLAP attribute roll-ups can be viewed as representatives of the attribute groups. An interesting general idea of producing attribute groups in conjunction with clustering of points themselves leads to the concept of co-clustering. Co-clustering is a simultaneousclustering of both points and their attributes. This approach reverses the struggle: to improve clustering of points based on their attributes, it tries to cluster attributes based on the points. So far we were concerned with grouping only rows of a matrixX. Now we are talking about grouping its columns as well. This utilizes a canonical duality contained in thepoint-by-attribute data representation.The idea of co-clustering of data points and attributes is old [Anderberg 1973; Hartigan 1975] and is known under the namessimultaneous clustering,bi-dimensional clustering,block clustering,conjugate clustering,distributional clustering, and informationbottleneck method. The use of duality for analysis of categorical data (dual or multidimensional scaling) also has a long history in statistics [Nishisato 1980]. The similar approach ofbuilding groups of item was presentedin the section Co-Occurrence of Categorical Data.In this section weturn to numericalattributes. Assumethat the matrixXhas non-negative elements. In thiscontext it is known asincidence,relational,frequency,contingencymatrix. Inapplications it canreflect intensity ofa gene response in a tissue sample,frequency of visitation activity of a page, or the amount of a sale in a store per item category. Figure 7. Learning referring traffic on a Web site. Govaert [1995] researched simultaneous block clustering of the rows and columns ofcontingency tables. He also reviewed an earlier work on the subject. An advancedalgebraic approach to co-clustering based on bi-partite graphs and their minimal cuts inconjunction with text mining was proposed in [Dhillon 2001]. This paper contains anexcellent introduction in relations between simultaneous clustering and graph 36partitioning, as well as in connection with SVD. A simple algorithm Ping-Pong [Oyanagi et al. 2001] was suggested to find populated areas in a sparse binary matrix. It redistributes influence of columns on rows and vice versa (compare with algorithm STIRR above) by transversal connection through matrix elements and provides an example of other than co-clustering, but a related development.A series of publications deal with distributional clustering of attributes based on theinformational measures of attribute similarity. Two attributes (two columns in matrixX)with exactly the same probability distributions are identical for the purpose of data mining, and so, one can be deleted. Attributes that have probability distributions that are close in terms of their Kullback-Leibler (KL) distance [Kullback & Leibler 1951] can still be grouped together without much of an impact. In addition, a natural derived attribute, the mixed distribution (a normalized sum of two columns) is now available to represent the group. This process can be generalized. The grouping simplifies the original matrixXto the compressed formX. Attribute clustering is productive when it minimallyimpacts information reduction )()(XIXIR=, where is mutual informationcontained in X[Cover & Thomas 1990]. Such attribute grouping is intimately related to Nave Bayes classification in predictive mining [Baker & McCallum 1998].)(XIThe outlined technique is very much relevant to grouping words in text mining. In this context the technique was explored under the nameinformation bottleneck method[Tishby et al. 1999]. It was used to facilitate agglomerative co-clustering of words indocument clustering [Slonim & Tishby 2000] and classification [Slonim & Tishby 2001].Berkhin & Becher [2002] showed deep algebraic connection of distributional clustering tok-means. They used k-means adaptation to KL-distance as a major iterative step in the algorithm SIMPLIFYRELATION that gradually co-clusters points and attributes. Thisdevelopment has industrial application in Webanalysis. Figure 7 shows how an original incidence matrix of Web site traffic between 197 referrers(rows) and 203 Web site pages (columns) is clustered into 26x22 matrix with 6% information loss. While KL-distance isnot actually a distance, since it is not symmetric, it can be symmetrized to the so-calledJensen-Shanon divergence. Dhillon et al. [2002] used Jensen-Shanon divergence tocluster words in k-means fashion in text classification. Besides text and Web mining, the idea of co-clustering finds its way into other applications, as for example, clustering of gene microarrays [Busygin et al. 2002]. 10. General Algorithmic IssuesWe have presented many different clustering techniques. However, there are common issues that must be addressed to make any clustering algorithm successful. Some are so ubiquitous that they are not even specific to unsupervised learning and can be considered as a part of overall data mining framework.Others are resolved in certain algorithms we presented. In fact, many algorithms were specifically designed for this reason. Now we overview common issues, and necessarily our coverage will be very fragmented.Scalability for VLDB and high dimensional clustering were already surveyed above, but several others significant issues are discussed below:-Assessment of results37-Choice of appropriate number of clusters - Data preparation- Proximitymeasures-Handling outliers10.1. Assessment of Results The data mining clustering process starts with the assessment of whether any clustertendencyhas a place at all, and correspondingly includes, appropriate attribute selection,and in many cases feature construction. It finisheswith thevalidation and evaluation of the resulting clustering system. The clustering system can be assessed by an expert, or by a particular automated procedure. Traditionally, the first type of assessment relates to two issues: (1) cluster interpretability, (2) cluster visualization. Interpretability depends on the technique used. Model-based probabilistic and conceptual algorithms, as COBWEB, have better scores in this regard.K-means and k-medoid methods generate clusters that are interpreted as dense areas around centroids or medoids and, therefore, also score well. The review [Jain et al. 1999] extensively covers cluster validation, while a discussion of cluster visualization and related references can be found in [Kandogan 2001].Regarding automatic procedures, when two partitions are constructed (with the same or different number of subsets k), the first order of business is to compare them. Sometimesthe actual class label s of one partition is known. Still clustering is performed generating another label j. The situation is similar to testing a classifier in predictive mining when the actual target is known. Comparison of sandjlabels is similar to computing an error,confusion matrix, etc., in predictive mining. Simple criterion Rand serves this purpose [Rand 1971]. Computation of a Rand index (defined below) involves pairs of points that were assigned to the same and to the different clusters in each of two partitions. Hence ithascomplexity and is not always feasible.Conditionalentropyof a known label sgiven clustering partitioning [Cover & Thomas 1990] )(2NO=jsjsjsjpppJSH)log()|(||is another measure used. Here  are probabilities of jcluster, and conditionalprobabilities of s given j. It has O complexity. Other measures are also used, for example, the F-measure [Larsen & Aone 1999].jsjpp|,)(N10.2. How Many Clusters? In many methods numberk of clusters to construct is an input user parameter. Running an algorithm several times leads to a sequence of clustering systems. Each system consists of more granular and less-separated clusters. In the case ofk-means, the objectivefunction is monotone decreasing. Therefore, the answer to the question of which system is preferable is not trivial.Many criteria have been introduced to find an optimalk. Some industrial applications (SAS, NeoVista) report pseudo F-statistic. This only makes sense for k-means clusteringin context of ANOVA. Earlier publications on the subject analyzed cluster separation fordifferentk [Engleman & Hartigan 1969; Milligan & Cooper 1985].For instance, a 38distance between any two centroids (medoids) normalized by corresponding clustersradii (standard deviations) and averaged (with cluster weights) is a reasonable choice of coefficient of separation. This coefficient has a very low O complexity. Another popular choice for separation measure is a Silhouette coefficient[Kaufman & Rousseeuw 1990]. For example, Silhouette coefficient is used in conjunction with CLARANS in [Ng & Han 1994]. It has  complexity. Consider average distancebetween the point xof cluster Cand other points within C and compare it with averageddistance to the best fitting cluster Gother than C)(2k)(2NO=xyCyyxdCxa,),(1||1)(,=GyCGyxdGxb),(||1min)(The Silhouette coefficient of xis, values close to +1 corresponding to a perfect and values below 0 to a bad clustering choice. The overall average of individual s(x) gives a good indication of cluster system appropriateness.)}(),(max{/))()(()(xbxaxaxbxs=Still another approach to separation is to employ possible soft (or fuzzy) assignments. Ithas an intermediate complexity. Remember that assignment of a point to aparticular cluster may frequently involve certain arbitrariness. Depending on how well a point fits a particular cluster C, different probabilities or weights  can be introduced so that a hard (strict) assignment is defined as )(kNO),(Cxw),(argmin)(CxwxCC=.APartition coefficient [Bezdek 1981] is equal to the sum of squares of the weights =XxxCxwNW2))(,(1(compare with GINI index). Each of the discussed measures can be plotted as a function ofk and the graph can be used to choose the best k.The strong probabilistic foundation of the mixture model, discussed in sub-section Probabilistic Clustering, allows viewing a choice of optimalkas a problem of fitting thedata by the best model. The question is whether adding new parameters results in a bettermodel. In Bayesian learning (for example, AUTOCLASS [Cheeseman & Stutz 1995]) thelikelihood of the model is directly affected (through priors) by the model complexity(number of parameters is proportional to k). Several criteria were suggested including: Minimum Description Length (MDL) criterion [Rissanen 1978; Schwarz 1978;Rissanen 1989] Minimum Message Length (MML) criterion [Wallace & Freeman 87; Wallace &Dowe 94] Bayesian Information Criterion (BIC) [Schwarz 1978; Fraley & Raftery 1998] Akaikes Information Criterion (AIC) [Bozdogan 1983] Non-coding Information Theoretic Criterion (ICOMP) [Bozdogan 1994] 39Approximate Weight of Evidence (AWE) criterion [Banfield & Raftery 1993] Bayes Factors [Kass & Raftery 1995] All these criteria are expressed through combinations of log-likelihood L, number of clustersk, number of parameters per cluster, total number of estimated parametersp, and different flavors of Fisher information matrix. For example,,,))(argminkMDLkbest=log(2/)(ppLkMDL+=)log(2)(npLkBIC=,.)(argmaxkBICkbest=Further details and discussion can be found in [Bock 1996; Oliver et al. 1996; Fraley & Raftery 1998]. Few examples: MCLUST and X-means use BIC criterion, SNOB uses MML criterion, CLIQUE and evolutionary approach to k determination [Lee & Antonsson 2000] use MDL. Significant expertise is developed in estimation of goodness of fit based on the criteria above. For example, different ranges of BIC are suggested forweak, positive, and very strong evidence in favor of one clustering system versus another[Fraley & Raftery 1999]. Smyth [1998] suggested a likelihood cross-validation techniquefor determination the bestk.10.3. Data Preparation Irrelevant attributes make chances of a successful clustering futile, because they negatively affect proximity measures and eliminateclustering tendency. Therefore, soundexploratory data analysis (EDA) is essential. An overall framework for EDA can befound in [Becher et al. 2000]. As its first order of business, EDA eliminates inappropriate attributes and reduces the cardinality of the retained categorical attributes. Next it provides attribute selection. Different attribute selection methods exist. Inconsistency rates are utilized in [Liu & Setiono 1996]. The idea of a Markov blanket is used in [Koller & Sahami 1996]. While there are others methods (for example, [Jebara & Jaakkola 2000]), most are used primarily for predictive and not descriptive mining and thus do not address general-purpose attribute selection for clustering. We conclude that cluster-specific attribute selection yet to be invented.Attributes transformation and clustering have already been discussed in the context of dimensionality reduction. The practice of assigning different weights to attributes and/orscaling of their values (especially, standardization) is widespread and allows constructing clusters of better shapes. To some extentattribute scaling can be viewed as the continuation of attribute selection.In real-life applications it is crucial to handle attributes of different nature. For example,images are characterized by color, texture, shape, and location, resulting in four attribute subsets. Modha & Spangler [2002] suggested a very interesting approach for attribute scaling that pursues the objective of clustering in each attribute subset by computingweights (a simplex) that minimize the product of intra-cluster to inter-cluster ratios forthe attribute subset projections (called generalized Fisher ratio).In many applications data points have different significance. For example, in assortmentplanning, stores can be characterized by the profiles of sales of particular items in 40percentage. However, the overall sale volume gives additional weight to larger stores.Partitioning relocation algorithms easily handle weighted data, because centroids becomecenters of weights instead of means.The described practice is called casescaling.Some algorithms depend on the effectiveness of data access. To facilitate this processdata indices are constructed. Examples include the extension of the algorithm CLARANS [Ester et al. 1995] and the algorithm DBSCAN [Ester et al. 1996]. Index structures used for spatial data, include KD-trees [Friedman et al. 1977], R-trees [Guttman 1984], R*-trees [Kriegel et al. 1990]. A blend of attribute transformations (DFT, Polynomials) and indexing technique is presented in [Keogh et al. 2001a]. Other indices and numerousgeneralizations exist [Beckmann 1990; Faloutsos et al. 1994; Berchtold et al. 98; Wang et al. 1998; Karypis & Han 2000; Keogh et al. 2001b]. The major application of such data structures is in nearest neighbors search.Preprocessing of multimedia data that is based on its embedding in Euclidean space (thealgorithm FastMap) [Faloutsos & Lin 1995]. A fairly diverse range of preprocessing is used for variable length sequences. Instead of handling them directly (as in the sub-sectionProbabilistic Clustering), a fixed set offeatures to represent variable length sequences can be derived [Guralnik & Karypis 2001; Manilla & Rusakov 2001].10.4. Proximity MeasuresBoth hierarchical and partitioning methods use different distances and similaritymeasures [Jain & Dubes 1988]. The usual  distancepL()21:1z,,),(zzzyxyxdppdjjpp====is used for numerical data,1, in which lowerpcorresponds to a morerobustestimation (therefore, less affected by outliers). Euclidean (p=2) distance is by far the most popular choice used in k-means objective function (sum of squares of distances between points and centroids) that has a clear statistical meaning of total inter-clustersvariance. Manhattan distance corresponds to p=1. The distance that returns the maximumof absolute difference in coordinates is also used and corresponds to <p=p. In manyapplications (profile analyses) points are scaled to have a unit norm, so that the proximitymeasure is an angle between the points,()yxyxyxdT=arccos),(.It is used, in specific tools, as DIGNET (section Scalability and VLDB Extensions), and in particular applications, as text mining. All above distances assume attributes independence (diagonal covariance matrixS). Mahanalabonis distance)()(),(1yxSyxyxdT=[Mardia et al. 1980] is used in algorithms, as ORCLUS [Aggarwal & Yu 2000], that do not make this assumption.41Formula()),(11),(yxdyxs+= defines similarity for numerical attributes. Other choicesarecosine, Dice coefficients and distance exponent()22cos2,),(yxyxsyxyxyxsTDiceT+==,()yxs=expexp.Now we shift our attention to categorical data. A number of similarity measures exist forcategorical attributes [Dubes 1993; Everitt 1993]. Assuming binary attributes with values=,, let  be a number of attributes having outcomesd in x and in y.Then the RandandJaccard (also known as Tanimoto) indices R,Jare equal to ()()()()++++++++++++++=++++=ddddyxJddddddyxR),(,),(Notice that Jaccard index treats positive and negative values asymmetrically, which makes it the measure of choice for transactional data, + meaning that an item is present. It is simply the fraction of common items of two transactions relative to the number ofitems in both transactions. It is also used in collaborative filtering, sequence analysis, text mining, and pattern recognition. Extended Jaccard coefficient is advocated in [Ghosh 2002]. For construction of similarity measures for market basket analysis see [Aggarwalet al. 1999b; Baeza-Yates 1992]. Similarity canalso be constructed axiomatically basedon information-theoretical considerations [Lin 1998]. The last two references contain material related to strings similarity (biology is one application). For strings over the same alphabet, edit distance is a frequent choice [Arslan & Egecioglu 2000]. It is based on the length of a sequence of transformations (such as insertion, deletion, transposition,etc.) that are necessary to transform one string into another. A classic Hamming distance [Cover & Thomas 1990] is also used. Further references can be found in the review [Jain et al. 1999]. Historically textual mining was a source of major inspirations for a concept of similarity [Resnik 1995]. Proximity measures between two clusters that can be derived from proximities betweenpairs of their points were discussed in the sub-section Linkage Metrics.10.5. Handling Outliers Applications that derive their data from measurements have an associated amount of noise, which can be viewed as outliers. Alternately, outliers can be viewed as legitimaterecords having abnormal behavior. In general, clustering techniques do not distinguish between the two: neither noise nor abnormalities fit into clusters. Correspondingly, the preferable way to deal with outliers in partitioning the data is to keep one extra set ofoutliers, so as not to pollute factual clusters.There are multiple ways of how descriptivelearning handles outliers. If a summarizationor data preprocessing phase is present, it usually takes care of outliers. For example, thisis the case with grid-based methods. They simply rely on input thresholds to eliminatelow-populated cells. Algorithms in the section Scalability and VLDB Extensions provide further examples. The algorithm BIRCH [Zhang et al. 1996; Zhang et al. 1997] revisits outliers during the major CF tree rebuilds, but in general handles them separately. This approach is shared by other similar systems [Chiu et al. 2001]. The framework of [Bradley et al. 1998] utilizes a multiphase approach to outliers. 42Certain algorithms have specific features for outliers handling. The algorithm CURE [Guha et al. 1998] uses shrinkage of clusters representives to suppress the effects of outliers.K-medoids methods are generally more robust than k-means methods with respect to outliers: medoids do not feel outliers. The algorithm DBCSAN [Ester et al.1996] uses concepts of internal (core), boundary (reachable), and outliers (non-reachable) points. The algorithm CLIQUE [Agrawal et al. 1998] goes a step further: it eliminatessubspaces with low coverage. The algorithm WaveCluster [Sheikholeslami et al. 1998] is known to handle outliers very well through its filtering DSP foundation. The algorithmORCLUS [Aggarwal & Yu 2000] produces a partition plus a set of outliers.What is an outlier? Statistics definesan outlier as a point that does not fit a probabilitydistribution. This approach has the problem with discordance testing for unknown multivariate distribution. Classic data analysis utilizes a concept ofdepth [Tukey 1977] and defines an outlier as a point of low depth. This concept becomes computationallyinfeasible ford> 3. Data mining is gradually develops its own definitions.Consider two positive parameters,. A point can be declared an outlier if its -neighborhood contains less than 1- fraction of a whole dataset X [Knorr & Ng 1998]. Ramaswamy et al. [2000] noticed that this definition can be improved by eliminatingparameter. Rank all the points by their distance to the K-nearest neighbor and define the fraction of points with highest ranks as outliers. Both definitions are uniformly global. How to describe local outliers? In essence, different subsets of data have differentdensities and may be governed by different distributions. A point close to a tight clustercan be a more probable outlier than a point that is further away from a more dispersedcluster. The concept of local outlier factor (LOF) that specifies a degree of outlier-nesscomes to rescue [Breunig et al. 2000]. The definition is based on the distance to the k-nearest neighbor. Knorr et al. [2001] addressed a related problem of how to eliminate outliers in order to compute an appropriate covariance matrix that describes a given locality. To do so, they utilizedDonoho-Stahel estimator in two-dimensional space.Crude handling of outliers works surprisingly well in many applications, because the simple truth is that many applications are concerned with systematic patterns. An example is customer segmentation with an objective of a direct mail campaign. On the other hand, philosophically outlier is a non-typical leftover after a regular clustering and, as such, it can easily attain a prominent significance. Therefore, in addition to eliminatingnegative effects of outliers on clusters construction, there is a separate reason driving interest in outlier detection. The reason is that in some applications, the outlier is thecommodity of trade. This is so in medical diagnostics, fraud detection, network security,anomaly detection, and computer immunology. Some connections and further references can be found in [Forrest et al. 1997; Lee & Stolfo 1998; Ghosh et al. 1999]. In CRM, E-commerce, Web-site analytics outliers relate to a concept of interestingand unexpected[Piatetsky-Shapiro & Matheus 1994; Silberschatz & Tuzhilin 1996; Padmanabhan & Tuzhilin 1999; Padmanabhan & Tuzhilin 2000]. Most of the research in theseapplications is not directly related to clustering (but to pruning association rules).43AcknowledgementsCooperation with Jonathan Becher was essential for the appearance of this text. It resulted in numerous discussions and various improvements. I am grateful to Jiawei Han for reading the text and his thoughtful remarks concerning the presentation of thematerial.I am very much thankful to Sue Krouscup for her help with text preparation. ReferencesAGGARWAL, C.C., HINNEBURG, A., and KEIM, D.A. 2000. On the surprising behavior of distance metrics in high dimensional space. IBM Research report, RC 21739. AGGARWAL, C.C., PROCOPIUC, C., WOLF, J.L., YU, P.S., and PARK, J.S. 1999a. Fastalgorithms for projected clustering. In Proceedings of the ACM SIGMOD Conference,61-72, Philadelphia, PA. AGGARWAL, C.C., WOLF, J.L., and YU, P.S. 1999b. A new method for similarityindexing of market basket data. In Proceedings of the ACM SIGMOD Conference, 407-418, Philadelphia, PA. AGGARWAL, C.C. and YU, P.S. 2000. Finding generalized projected clusters in high dimensional spaces. Sigmod Record, 29, 2, 70-92. AGRAWAL, R., FALOUTSOS, C., and SWAMI, A. 1993. Efficient similarity search in sequence databases. In Proceedings of the 4th International Conference on Foundations of Data Organization and Algorithms, Chicago, IL.AGRAWAL, R., GEHRKE, J., GUNOPULOS, D., and RAGHAVAN, P. 1998. Automaticsubspace clustering of high dimensional data for data mining applications. InProceedings of the ACM SIGMOD Conference, 94-105, Seattle, WA.AL-SULTAN, K. 1995. A Tabu search approach to the clustering problem.PatternRecognition, 28, 9, 1443-1451.ANDERBERG, M. 1973. Cluster Analysis and Applications. Academic Press, New York. ANKERST, M., BREUNIG, M., KRIEGEL, H.-P., and SANDER, J. 1999. OPTICS: Orderingpoints to identify clustering structure. In Proceedings of the ACM SIGMOD Conference,49-60, Philadelphia, PA. ARABIE, P. and HUBERT, L.J. 1996. An overview of combinatorial data analysis, in: Arabie, P., Hubert, L.J., and Soete, G.D. (Eds.) Clustering and Classification, 5-63, World Scientific Publishing Co., NJ. ARSLAN, A.N. and EGECIOGLU, O. 2000. Efficient algorithms for normalized edit distance.Journal of Discrete Algorithms, 1, 1. BABU, G.P. and MURTY, M.N. 1993. A near-optimal initial seed value selection in K-means algorithm using a genetic algorithm.Pattern Recogn. Lett. 14, 10, 763-169.BABU, G.P. and MARTY, M.N. 1994. Clustering with evolution strategies. PatternRecognition, 27, 2, 321-329.44BAEZA-YATES, R. 1992. Introduction to data structures and algorithms related to information retrieval. In Frakes,W.B. and Baeza-Yates, R. (Eds.) Information Retrieval,Data Structures and Algorithms, 13-27, Prentice-Hall.BAKER, L.D. and MCCALLUM, A. K. 1998. Distributional clustering of words for text classification. In Proceedings ofthe 21st ACM SIGIR Conference, Melbourne, Australia.BALL, G. and HALL, D. 1965. ISODATA, a novel method of data analysis and classification.Technical Report AD-699616, SRI, Stanford, CA. BANERJEE, A. and GHOSH, J. 2002. On scaling up balanced clustering algorithms. In Proceedings of the 2nd SIAM ICDM, 333-349, Arlington, VA.BANFIELD, J. and RAFTERY, A. 1993. Model-based Gaussian and non-Gaussian clustering.Biometrics, 49, 803-821. BARBARA, D. and CHEN, P. 2000. Using the fractal dimension to cluster datasets. In Proceedings of the 6th ACM SIGKDD, 260-264, Boston, MA. BECHER, J., BERKHIN, P., and FREEMAN, E. 2000. Automating exploratory data analysis for efficient data mining.  In Proceedings of the 6th ACM SIGKDD, 424-429, Boston, MA.BECKMANN, N., KRIEGEL, H-P., SCHNEIDER, R., and SEEGER, B. 1990. The R*-tree: An efficient access method for points and rectangles. In Proceedings of InternationalConference on Geographic Information Systems, Ottawa, Canada.BERCHTOLD, S., BHM, C., and KRIEGEL, H-P. 1998. The Pyramid-technique: towardsbreaking the curse of dimensionality. In Proceedings of the ACM SIGMOD Conference,142-153, Seattle, WA.BERKHIN, P. and BECHER, J. 2002. Learning Simple Relations: Theory and Applications.InProceedings of the 2nd SIAM ICDM, 420-436, Arlington, VA. BEN-DOR, A. and YAKHINI, Z. 1999. Clustering gene expression patterns. InProceedings of the 3rd Annual International Conference on Computational Molecular Biology (RECOMB 99), 11-14, Lyon, France.BERRY, M.W. and BROWNE, M. 1999. Understanding Search Engines: Mathematical Modeling and Text Retrieval. SIAM.BERRY, M., DUMAIS, S., LANDAUER, T., and OBRIEN, G. 1995. Using linear algebra for intelligent information retrieval.SIAM Review, 37, 4, 573-595. BOTTOU, L. and BENGIO, Y. 1995. Convergence properties of the K-means algorithms.In Tesauro, G. and Touretzky, D. (Eds.) Advances in Neural Information Processing Systems 7, 585-592, The MIT Press, Cambridge, MA.BEYER, K., GOLDSTEIN, J., RAMAKRISHNAN, R., and SHAFT, U. 1999. When is nearest neighbor meaningful? In Proceedings of the 7th ICDT, Jerusalem, Israel. BEZDEK, D. 1981. Pattern Recognition with Fuzzy Objective Function Algorithms.Plenum Press, New York, NY. BOCK, H.H. 1996. Probability models in partitional cluster analysis. In Ferligoj, A. and Kramberger, A. (Eds.) Developments in Data Analysis, 3-25, Slovenia. 45BOLEY, D.L. 1998. Principal direction divisive partitioning.Data Mining and KnowledgeDiscovery, 2, 4, 325-344. BOZDOGAN, H. 1983. Determining the number of component clusters in the standardmultivariate normal mixture model using model-selection criteria.TR UIC/DQM/A83-1,Quantitative Methods Department, University of Illinois, Chicago, IL.BOZDOGAN, H. 1994. Mixture-model cluster analysis using model selection criteria and anew information measure of complexity. In Proceedings of the 1st US/Japan Conference on the Frontiers of Statistical Modeling: An Informational Approach, 69-113, Dordrecht,Netherlands.BRADLEY, P. S., BENNETT, K. P., and DEMIRIZ, A. 2000. Constrained k-meansclustering.Technical Report MSR-TR-2000-65. Microsoft Research, Redmond, WA.BRADLEY, P. and FAYYAD, U. 1998. Refining initial points for k-means clustering. In Proceedings of the 15th ICML, 91-99, Madison, WI.BRADLEY, P., FAYYAD, U., and REINA, C. 1998. Scaling clustering algorithms to largedatabases. In Proceedings of the 4th ACM SIGKDD, 9-15, New York, NY. BREUNIG, M., KRIEGEL, H-P., KROGER, P., and SANDER, J. 2001. Data Bubbles: quality preserving performance boosting for hierarchical clustering. In Proceedings of the ACM SIGMOD Conference, Santa Barbara, CA. BREUNIG, M.M., KRIEGEL, H.-P., NG, R.T., and SANDER, J. 2000. LOF: identifying density-based local outliers. InProceedings of the ACM SIGMOD Conference, 93-104, Dallas, TX.BROWN, D. and HUNTLEY, C. 1991. A practical application of simulated annealing to clustering.Technical report IPC-TR-91-003, University of Virginia. BUSYGIN, S., JACOBSEN, G., and KRMER, E. 2002.Double conjugated clustering applied to leukemia microarray data, 2nd SIAM ICDM, Workshop on clustering high dimensional data, Arlington, VA. CADEZ, I., GAFFNEY, S., and SMYTH, P. 2000. A general probabilistic framework for clustering individuals. Technical Report UCI-ICS 00-09.CADEZ, I., SMYTH, P., and MANNILA, H. 2001. Probabilistic modeling of transactional data with applications to profiling, Visualization, and Prediction, In Proceedings of the 7th ACM SIGKDD, 37-46, San Francisco, CA. Carpenter, G.A., Grossberg, S., and Rosen, D.B. 1991. Fuzzy art: Fast stable learning and categorization of analog patterns by an adaptive resonance system. Neural Networks, 4, 759-771.
CHEESEMAN, P. and STUTZ, J. 1996. Bayesian Classification (AutoClass): Theory and Results. In Fayyad, U.M., Piatetsky-Shapiro, G., Smyth, P., and Uthurusamy , R. (Eds.) Advances in Knowledge Discovery and Data Mining, AAAI Press/MIT Press. CHENG, C., FU, A., and ZHANG, Y. 1999. Entropy-based subspace clustering for miningnumerical data. In Proceedings of the 5th ACM SIGKDD, 84-93, San Diego, CA.46CHIU, T., FANG, D., CHEN, J., and Wang, Y. 2001. A Robust and scalable clustering algorithm for mixed type attributes in large database environments. In Proceedings of the 7th ACM SIGKDD, 263-268, San Francisco, CA. COOLEY, R., MOBASHER, B., and SRIVASTAVA, J. 1999. Data preparation for miningworld wide web browsing. Journal of Knowledge Information Systems, 1, 1, 5-32.CORTER, J. and GLUCK, M. 1992. Explaining basic categories: feature predictability and information.Psychological Bulletin, 111, 291-303.COVER, T.M. and THOMAS, J.A. 1990. Elements of Information Theory. John Wiley &Sons, New York, NY. CRISTOFOR, D. and SIMOVICI, D.A. 2002. An information-theoretical approach to clustering categorical databases using genetic algorithms.2nd SIAM ICDM, Workshop on clustering high dimensional data, Arlington, VA. CUTTING, D., KARGER, D., PEDERSEN, J., and TUKEY, J. 1992. Scatter/gather: a cluster-based approach to browsing large document collection. In Proceedings ofthe 15th ACM SIGIR Conference, 318-329, Copenhagen, Denmark.DANIEL, C. and WOOD, F.C. 1980.Fitting Equations To Data: Computer Analysis of Multifactor Data. John Wiley & Sons, New York, NY. DAY, W. and EDELSBRUNNER, H. 1984. Efficient algorithms for agglomerativehierarchical clustering methods.Journal of Classification, 1, 7, 7-24. DEFAYS, D. 1977. An efficient algorithm for a complete link method.The Computer Journal, 20, 364-366. DEMPSTER, A., LAIRD, N., and RUBIN, D. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, Series B, 39, 1, 1-38. DHILLON, I. 2001. Co-clustering documents and words using bipartite spectral graph partitioning. In Proceedings of the 7th ACM SIGKDD, 269-274, San Francisco, CA. DHILLON, I., FAN, J., and GUAN, Y. 2001. Efficient clustering of very large documentcollections. In Grossman, R.L., Kamath, C., Kegelmeyer, P., Kumar, V., and Namburu,R.R. (Eds.) Data Mining for Scientific and Engineering Applications, Kluwer AcademicPublishers.
DHILLON, I., GUAN, Y., and KOGAN, J. 2002. Refining clusters in high dimensional data. 2nd SIAM ICDM, Workshop on clustering high dimensional data, Arlington, VA. DHILLON, I., MALLELA, S., and KUMAR, R. 2002. Enhanced Word Clustering for Hierarchical Text Classification, InProceedings of the 8th ACM SIGKDD, 191-200,Edmonton, Canada. DHILLON, I. and MODHA, D. 1999. A data clustering algorithm on distributed memorymultiprocessors.5th ACM SIGKDD,Large-scale Parallel KDD Systems Workshop, 245-260, San Diego, CA. DUBES, R.C. 1993. Cluster Analysis and Related Issues. In Chen, C.H., Pau, L.F., and Wang, P.S. (Eds.) Handbook of Pattern Recognition and Computer Vision, 3-32, World Scientific Publishing Co., River Edge, NJ. 47DUDA, R. and HART, P. 1973. Pattern Classification and Scene Analysis. John Wiley & Sons, New York, NY. DUMOUCHEL, W., VOLINSKY, C., JOHNSON, T., CORTES, C., and PREGIBON, D. 1999. Squashing flat files flatter. In Proceedings of the 5th ACM SIGKDD, 6-15, San Diego, CA.ENGLEMAN, L. and HARTIGAN, J. 1969. Percentage points of a test for clusters. Journalof the American Statistical Association, 64, 1647-1648. ERTOZ, L., STEINBACH, M., and KUMAR, V. 2002. Finding clusters of different sizes, shapes, and densities in noisy, high dimensional data, Technical Report.ESTER M., FROMMELT A., KRIEGEL H.-P., and SANDER J. 2000. Spatial data mining:database primitives, algorithms and efficient DBMS support. Data Mining and Knowledge Discovery, Kluwer Academic Publishers, 4, 2-3, 193-216. ESTER, M., KRIEGEL, H-P., SANDER, J. and XU, X. 1996. A density-based algorithm fordiscovering clusters in large spatial databases with noise. InProceedings of the 2nd ACM SIGKDD, 226-231, Portland, Oregon. ESTER, M., KRIEGEL, H-P., and XU, X. 1995. A database interface for clustering in large spatial databases. In Proceedings of the 1st ACM SIGKDD, 94-99, Montreal, Canada. ESTIVILL-CASTRO, V. and LEE, I. 2000. AMOEBA: Hierarchical Clustering Based on Spatial Proximity Using Delaunay Diagram. InProceedings of the 9th International Symposium on Spatial Data Handling. Beijing, China.EVERITT, B. 1993. Cluster Analysis (3rd ed.). Edward Arnold, London, UK. FALOUTSOS, C. and LIN, K. 1995. Fastmap: A fast algorithm for indexing, data miningand visualization of traditional and multimedia. In Proceedings of the ACM SIGMOD Conference, 163-174, San Jose, CA. FALOUTSOS, C., RANGANATHAN, M., and MANOLOPOULOS, Y. 1994. Fast subsequence matching in time-series databases. In Proceedings of the ACM SIGMOD Conference,419-429, Minneapolis, MN. FASULO, D. 1999. An analysis of recent work on clustering algorithms.Technical ReportUW-CSE01 -03-02, University of Washington.FISHER, D. 1987. Knowledge acquisition via incremental conceptual clustering.MachineLearning, 2, 139-172. FISHER, D. 1996. Iterative optimization and simplification of hierarchical clustering. Journal of Artificial Intelligence Research, 4, 147-179. FORGY, E. 1965. Cluster analysis of multivariatedata: Efficiency versus interpretability of classification. Biometrics, 21, 768-780. FORREST, S., HOFMEYR, S.A., and SOMAYAJI, A. 1997. Computer immunology.Communications of the ACM, 40, 88-96. FOSS, A., WANG, W., and ZAANE, O. 2001. A non-parametric approach to Web log analysis.1st SIAM ICDM,Workshop on Web Mining, 41-50, Chicago, IL.48FRALEY, C. and RAFTERY, A. 1999. MCLUST: Software for model-based cluster and discriminant analysis, Tech Report 342, Dept. Statistics, Univ. of Washington.FRALEY, C. and RAFTERY, A. How many clusters? 1998. Which clustering method?Answers via model-based cluster analysis. The Computer Journal, 41, 8, 578-588. FRIEDMAN, J.H., BENTLEY, J.L., and FINKEL, R.A. 1977. An algorithm for finding best matches in logarithmic expected time. ACM Trans. Math. Software, 3, 3, 209-226. FUKUNAGA, K. 1990.Introduction to Statistical Pattern Recognition. Academic Press, San Diego, CA. GANTI, V., GEHRKE, J. and RAMAKRISHNAN, R. 1999a. CACTUS-Clustering Categorical Data Using Summaries. In Proceedings of the 5th ACM SIGKDD, 73-83, San Diego, CA.GANTI, V., RAMAKRISHNAN, R., GEHRKE, J., POWELL, A., and FRENCH, J. 1999b. Clustering large datasets in arbitrary metric spaces. In Proceedings of the 15th ICDE, 502-511, Sydney, Australia. GENNARI, J., LANGLEY, P., and FISHER, D. 1989. Models of incremental concept formation.Artificial Intelligence, 40, 11-61. GERSHO, A. and GRAY, R. M. 1992. Vector Quantization and Signal Compression. Communications and Information Theory. Kluwer Academic Publishers, Norwell, MA. GHOSH, J., 2002. Scalable Clustering Methods for Data Mining. In Nong Ye (Ed.)Handbook of Data Mining, Lawrence Erlbaum, to appear.GHOSH, A.K., SCHWARTZBARD, A., and SCHATZ. M. 1999. Learning program behavior profiles for intrusion detection. In Proceedings of the SANS Conference and Workshop on Intrusion Detection and Response, San Francisco, CA. GIBSON, D., KLEINBERG, J., and RAGHAVAN, P. 1998. Clustering categorical data: An approach based on dynamic systems. In Proceedings of the 24th International Conference on Very Large Databases, 311-323, New York, NY. GOIL, S., NAGESH, H., and CHOUDHARY, A. 1999. MAFIA: Efficient and scalable subspace clustering for very large data sets. Technical Report CPDC-TR-9906-010,Northwestern University.GOLDBERG, D. 1989. Genetic Algorithms in Search, Optimization, and Machine Learning. Addison-Wesley.GONZALEZ, T.F. 1985. Clustering to minimizethe maximum intercluster distance. Theoretical Computer  Science, 38, 293-306. GOVAERT, G. 1995. Simultaneous clustering of rows and columns.Control and Cybernetics, 24, 437-458. GOWDA, K.C. and KRISHNA, G. 1978. Agglomerative clustering using the concept of mutual nearest neighborhood. Pattern Recognition, 10, 105-112. GUHA, S., RASTOGI, R., and SHIM, K. 1998. CURE: An efficient clustering algorithm forlarge databases. In Proceedings of the ACM SIGMOD Conference, 73-84, Seattle, WA.49GUHA, S., RASTOGI, R., and SHIM, K. 1999. ROCK: A robust clustering algorithm for categorical attributes. In Proceedings of the 15th ICDE, 512-521, Sydney, Australia.GURALNIK, V. and KARYPIS, G. 2001. A Scalable algorithm for clustering sequential data.IEEE ICDM 2001, Silicon Valley, CA.GUTTMAN, A. 1984. R-trees: a dynamic index structure for spatial searching. In Proceedings of the ACM SIGMOD Conference, 47-57, Boston, MA.HALL, L.O., OZYURT, B., and BEZDEK, J.C. 1999. Clustering with a genetically optimized approach. IEEE Trans. on Evolutionary Computation, 3, 2, 103-112. HAN, E-H., KARYPIS, G., KUMAR, V., and MOBASHER, B. 1997. Clustering based on association rule hypergraphs. ACM SIGMOD Conference, Data Mining Workshop (DMKD'97), Tucson, Arizona. HAN, J. and KAMBER, M. 2001. Data Mining. Morgan Kaufmann Publishers. HAN, J., KAMBER, M., and TUNG, A. K. H. 2001. Spatial clustering methods in data mining: A survey. In Miller, H. and Han, J. (Eds.) Geographic Data Mining and Knowledge Discovery, Taylor and Francis. HAREL, D. and KOREN, Y. 2001. Clustering spatial data using random walks, In Proceedings of the 7th ACM SIGKDD, 281-286. San Francisco, CA.HARTIGAN, J. 1975. Clustering Algorithms. John Wiley & Sons, New York, NY. HARTIGAN, J. and WONG, M. 1979. Algorithm AS136: A k-means clustering algorithm.Applied Statistics, 28, 100-108. HEER, J. and CHI, E. 2001. Identification of Web user traffic composition using multi-modal clustering and information scent.1st SIAM ICDM,Workshop on Web Mining, 51-58, Chicago, IL. HINNEBURG, A. and KEIM, D. 1998. An efficient approachto clustering large multimediadatabases with noise. In Proceedings of the 4th ACM SIGKDD, 58-65, New York, NY. HINNEBURG, A. and KEIM, D. 1999. Optimal grid-clustering: Towards breaking the curse of dimensionality in high-dimensional clustering. In Proceedings of the 25th Conference on VLDB, 506-517, Edinburgh, Scotland. HUANG, Z. 1998. Extensions to the k-means algorithm for clustering large data sets with categorical values. Data Mining and Knowledge Discovery, 2, 3, 283-304. HULTEN, G., SPENCER, L., and DOMINGOS, P. 2001. Mining time-changing data streams.InProceedings of the 7th ACM SIGKDD, 97-106, San Francisco, CA. JAIN, A. and DUBES, R. 1988. Algorithms for Clustering Data. Prentice-Hall, Englewood Cliffs, NJ.JAIN, A.K. and FLYNN, P.J. 1966. Image segmentation using clustering. In Advances in Image Understanding:  A Festschrift for Azriel Rosenfeld, IEEE Press, 65-83. JAIN, A.K. and MAO, J. 1996. Artificial neural networks: A tutorial. IEEE Computer, 29, 3, 31-44. 50JAIN, A.K, MURTY, M.N., and FLYNN P.J. 1999. Data clustering: a review. ACMComputing Surveys, 31, 3, 264-323. JARVIS, R.A. and PATRICK, E.A. 1973. Clustering using a similarity measure based on shared nearest neighbors. IEEE Transactions on Computers, C-22, 11. JEBARA, T. and JAAKKOLA, T. 2000. Feature selection and dualities in maximum entropy discrimination. In Proceedings of the 16th UIA Conference, Stanford, CA. JOLIFFE, I. 1986. Principal Component Analysis. Springer-Verlag, New York, NY. KALTON, A., LANGLEY, P., WAGSTAFF, K., and YOO, J. 2001. Generalized clustering, supervised learning, and data assignment. In Proceedings of the 7th ACM SIGKDD, 299-304, San Francisco, CA. KANDOGAN, E. 2001. Visualizing multi-dimensionalclusters, trends, and outliers using star coordinates. In Proceedings of the 7th ACM SIGKDD, 107-116, San Francisco, CA.KARYPIS, G., AGGARWAL, R., KUMAR, V., and SHEKHAR, S. 1997. Multilevel hypergraph partitioning: application in VLSI domain, In Proceedings ACM/IEEE DesignAutomation Conference.KARYPIS, G., HAN, E.-H., and KUMAR, V. 1999a. CHAMELEON: A hierarchical clustering algorithm using dynamic modeling, COMPUTER, 32, 68-75. KARYPIS, G., HAN, E.-H., and KUMAR, V. 1999b. Multilevel refinement for hierarchical clustering.Technical Report # 99-020.KARYPIS, G. and HAN, E.-H. 2000. Concept indexing: A fast dimensionality reduction algorithm with applications to document retrieval & categorization.Technical Report TR-00-016, Department of Computer Science, University of Minnesota, Minneapolis. KARYPIS, G. and KUMAR, V., 1999. A fast and highly quality multilevel scheme for partitioning irregular graphs.SIAM Journal on Scientific Computing, 20, 1. KASS, R. and RAFTERY, A. 1995. Bayes factors. Journal of Amer. Statistical Association,90, 773-795. KAUFMAN, L. and  ROUSSEEUW, P. 1990. Finding Groups in Data: An Introduction to Cluster Analysis. John Wiley and Sons, New York, NY. KEOGH, E., CHAKRABARTI, K., MEHROTRA, S., and PAZZANI, M. 2001. Locally adaptive dimensionality reduction for indexing large time series databases. In Proceedings of the ACM SIGMOD Conference, Santa Barbara, CA. KEOGH, E., CHAKRABARTI, K., PAZZANI M., and MEHROTRA, S. 2001a. Dimensionalityreduction for fast similarity search in large time series databases.Journal of Knowledgeand Information Systems, 3, 3. KEOGH, E, CHU, S., and PAZZANI, M. 2001b. Ensemble-index: A new approach to indexing large databases. In Proceedings of the 7th ACM SIGKDD, 117-125, San Francisco, CA.KERNIGHAN, B.W. and LIN, S. 1970. An efficient heuristic procedure for partitioninggraphs.The Bell System Technical Journal, 49, 2, 291-307. 51KOHONEN, T. 1990. The self-organizing map.Proceedings of the IEEE, 9, 1464-1479. KOHONEN, T. 2001.Self-Organizing Maps. Springer Series in Information Sciences, 30,Springer.KOLATCH, E. 2001. Clustering Algorithms for Spatial Databases: A Survey. PDF isavailable on the Web.KOLLER, D. and SAHAMI, M. 1996. Toward optimal feature selection. In Proceedings of the 13th ICML, 284-292, Bari, Italy. KNORR E. and NG R. 1998. Algorithms for mining distance-based outliers in large datasets. In Proceedings of the 24h Conference on VLDB, 392-403, New York, NY. KNORR, E.M., NG, R.T., and ZAMAR, R.H. 2001. Robust Space Transformations fordistance-based operations. In Proceedings of the 7th ACM SIGKDD, 126-135, San Francisco, CA.KRIEGEL H.-P., SEEGER B., SCHNEIDER R., and BECKMANN N. 1990. The R*-tree: an efficient access method for geographic information systems. In Proceedings InternationalConference on Geographic Information Systems, Ottawa, Canada. KULLBACK, S. and LEIBLER, R.A. 1951. On information and sufficiency. Annals ofMathematical Statistics, 22, 76-86. LANCE, G. and WILLIAMS W. 1967. A general theory of classification sorting strategies. Computer Journal, 9, 373-386. LARSEN, B. and AONE, C. 1999. Fast and effective text mining using linear-timedocument clustering. In Proceedings of the 5th ACM SIGKDD, 16-22, San Diego, CA. LEE, C-Y. and ANTONSSON, E.K. 2000. Dynamic partitional clustering using evolution strategies. In Proceedings of the 3rd Asia-Pacific Conferenceon Simulated Evolution and Learning, Nagoya, Japan. LEE, W. and STOLFO, S. 1998. Data mining approaches for intrusion detection. In Proceedings of the 7th USENIX Security Symposium, San Antonio, TX. LIEBOVITCH, L. and TOTH, T. 1989. A fast algorithm to determine fractal dimensions by box counting. Physics Letters, 141A, 8. LIN, D. 1998. An information-theoretic definition of similarity. In Proceedings of the 15thICML, 296-304, Madison, WI.LIU, B., XIA, Y., and YU, P.S. 2000. Clustering through decision tree construction. InSIGMOD 2000.LIU, H. and SETIONO,R. 1996. A probabilistic approach to feature selection - a filter solution. In Proceedings of the 13th ICML, 319-327, Bari, Italy. MANILLA, H. and RUSAKOV, D. 2001. Decomposition of event sequences intoindependent components. In Proceedings of the 1st  SIAM ICDM, Chicago, IL. MAO, J. and JAIN, A.K. 1996. A Self-organizing network for hyperellipsoidal clustering (HEC).IEEE Transactions on Neural Networks, 7, 1, 16-29. MARDIA, K., KENT, J. and BIBBY, J. 1980. Multivariate Analysis. Academic Press. 52MARROQUIN, J.L. and GIROSI, F. 1993. Some extensions of the k-means algorithm for image segmentation and pattern classification.A.I. Memo 1390. MIT, Cambridge, MA. MASSART, D. and KAUFMAN, L. 1983. The Interpretation of Analytical Chemical Data by the Use of Cluster Analysis. John Wiley & Sons, New York, NY. MCCALLUM, A., NIGAM, K., and UNGAR, L.H. 2000. Efficient clustering of high-dimensional data sets with application to reference matching. In Proceedings of the 6thACM SIGKDD, 169-178, Boston, MA.MCLACHLAN, G. and BASFORD, K. 1988. Mixture Models: Inference and Applications to Clustering. Marcel Dekker, New York, NY. MCLACHLAN, G. and KRISHNAN, T. 1997. The EM Algorithm and Extensions. John Wiley & Sons, New York, NY. MICHALSKI, R.S. and STEPP, R. 1983. Learning from observations: conceptual clustering. In Machine Learning: An Artificial Intelligence Approach. San Mateo, CA, Morgan Kaufmann.MILLIGAN, G. and COOPER, M. 1985. An examination of procedures for determining the number of clusters in a data set.Psychometrika, 50, 159-179. MIRKIN, B. 1996. Mathematic Classification and Clustering. Kluwer AcademicPublishers.
MITCHELL, T. 1997. Machine Learning. McGraw-Hill, New York, NY.MODHA, D. and SPANGLER, W. 2002. Feature weighting in k-means clustering. MachineLearning, 47.MOORE, A. 1999. Very fast EM-based mixture model clustering using multiresolutionkd-trees.Advances in Neural Information Processing Systems, 11. MOTWANI, R. and RAGHAVAN, P. 1995. Randomized Algorithms. Cambridge UniversityPress.MURTAGH, F. 1983. A survey of recent advances in hierarchical clustering algorithms.Computer Journal, 26, 4, 354-359. MURTAGH, F. 1985. Multidimensional Clustering Algorithms. Physica-Verlag, Vienna.NAGESH, H., GOIL, S., and CHOUDHARY, A. 2001. Adaptive grids for clustering massivedata sets, In Proceedings of the 1st  SIAM ICDM, Chicago, IL. NG, R. and HAN, J. 1994. Efficient and effective clustering methods for spatial data mining. In Proceedings of the 20th Conference on VLDB, 144-155, Santiago, Chile. NISHISATO, S. 1980.Analysis of Categorical Data: Dual Scaling and Its Applications.University of Toronto.OLIVER, J., BAXTER, R. and WALLACE, C. 1996. Unsupervised learning using MML. In Proceedings of the 13th ICML, Bari, Italy.OLSON, C. 1995. Parallel algorithms for hierarchical clustering. Parallel Computing, 21, 1313-1325.53OYANAGI, S., KUBOTA, K., and NAKASE, A. 2001. Application of matrix clustering to Web log analysis and access prediction. 7th ACM SIGKDD, WEBKDD Workshop, San Francisco, CA.PADMANABHAN, B. and TUZHILIN, A. 1999. Unexpectedness as a measure of interestingness in knowledge discovery. Decision Support Systems Journal, 27, 3, 303-318.PADMANABHAN, B. and TUZHILIN, A. 2000. Small is beautiful: discovering the minimalset of unexpected patterns. In Proceedings of the 6th ACM SIGKDD, 54-63, Boston, MA.PELLEG, D. and MOORE, A. 1999. Accelerating exact k-means algorithms with geometricreasoning. In Proceedings of the 5th ACM SIGKDD, 277-281, San Diego, CA. PELLEG, D. and MOORE, A. 2000. X-means: Extending K-means with EfficientEstimation of the Number of Clusters. In Proceedings 17th ICML, Stanford University.PIATETSKY-SHAPIRO, G. and MATHEUS, C.J. 1994. The interestingness of deviations. In Proceedings of the AAAI-94 Workshop on Knowledge Discovery in Databases.RAMASWAMY, S., RASTOGI, R., and SHIM, K. 2000. Efficient algorithms for mining outliers from large data sets, Sigmoid Record, 29, 2, 427-438. RAND, W.M. 1971. Objective criteria for the evaluation of clustering methods.Journal of the American Statistical Assoc, 66, 846-850.RESNIK, P. 1995. Using information content to evaluate semanticsimilarity in a taxonomy. In Proceedings of IJCAI-95, 448-453, Montreal, Canada.RISSANEN, J. 1978. Modeling by shortest data description. Automatica, 14, 465-471. RISSANEN J. 1989. Stochastic Complexity in Statistical Inquiry. World ScientificPublishing Co., Singapore.SANDER, J., ESTER, M., KRIEGEL, H.-P., and XU, X. 1998. Density-based clustering in spatial databases: the algorithmGDBSCAN and its applications. In Data Mining and Knowledge Discovery, 2, 2, 169-194.SARAFIS, I., ZALZALA, A.M.S., and TRINDER, P.W. 2002. A genetic rule-based data clustering toolkit. To be published in Congress on Evolutionary Computation (CEC),Honolulu, USA. SAVARESI, S. and BOLEY, D. 2001. On performance of bisecting k-means and PDDP. In Proceedings of the 1st  SIAM ICDM, Chicago, IL. SAVARESI, S.M., BOLEY, D.L., BITTANTI, S., and GAZZANIGA, G. 2002. Cluster Selection in divisive clustering algorithms. In Proceedings of the 2nd SIAM ICDM, 299-314, Arlington, VA. SCHALKOFF, R. 1991. Pattern Recognition.Statistical, Structural and Neural Approaches. John Wiley & Sons, New York, NY. SCHIKUTA, E. 1996. Grid-clustering: a fast hierarchical clustering method for very large data sets. InProceedings 13th International Conference on Pattern Recognition, 2, 101-105.54SCHIKUTA, E., ERHART, M. 1997. The BANG-clustering system: grid-based data analysis. In Proceeding of Advances in Intelligent Data Analysis, Reasoning about Data, 2nd International Symposium, 513-524, London, UK. SCHWARZ, G. 1978. Estimating the dimension of a model.The Annals of Statistics, 6, 461-464.
SCOTT, D.W. 1992. Multivariate Density Estimation. Wiley, New York, NY. SHEIKHOLESLAMI, G., CHATTERJEE, S., and ZHANG, A. 1998. WaveCluster: A multi-resolution clustering approach for very large spatial databases. In Proceedings of the 24thConference on VLDB, 428-439, New York, NY. SIBSON, R. 1973. SLINK: An optimally efficient algorithm for the single link cluster method.Computer Journal, 16, 30-34. SILBERSCHATZ, A. and TUZHILIN, A. 1996. What makes patterns interesting in knowledge discovery systems.IEEE Trans. on Knowledge and Data Eng., 8, 6. SLONIM, N. and TISHBY, N. 2000. Document clustering using word clusters via the Information Bottleneck Method. In Proceedings SIGIR, 208-215. SLONIM, N. and TISHBY, N. 2001. The power of word clusters for text classification. In 23rd European Colloquium on Information Retrieval Research.SMYTH, P. 1998. Model selection for probabilistic clustering using cross-validatedlikelihood.ICS Tech Report 98-09, Statistics and Computing.SMYTH, P. 1999. Probabilistic model-based clustering of multivariate and sequential data. In Proceedings of the 7th International Workshop on AI and Statistics, 299-304. SPATH H. 1980. Cluster Analysis Algorithms. Ellis Horwood, Chichester, England. STEINBACH, M., KARYPIS, G., and KUMAR, V. 2000. A comparison of documentclustering techniques. 6th ACM SIGKDD, World Text Mining Conference, Boston, MA. STREHL, A. and GHOSH, J. 2000. A scalable approach to balanced, high-dimensionalclustering of market baskets, In Proceedings of 17th International Conference on High Performance Computing, Springer LNCS, 525-536, Bangalore, India.THOMOPOULOS, S., BOUGOULIAS, D., and WANN, C-D. 1995. Dignet: An unsupervised-learning clustering algorithm for clustering and data fusion. IEEE Trans. on Aerospace and Electr. Systems, 31, 1, 2,1-38. TISHBY, N., PEREIRA, F.C., and BIALEK, W. 1999. The information bottleneck method.InProceedings of the 37-th Annual Allerton Conference on Communication, Control and Computing, 368-377. TUKEY, J.W. 1977. Exploratory Data Analysis. Addison-Wesley.TUNG, A.K.H., HOU, J., and HAN, J. 2001. Spatial clustering in the presence of obstacles. InProceedings of the 17th ICDE, 359-367, Heidelberg, Germany.TUNG, A.K.H., NG, R.T., LAKSHMANAN, L.V.S., and HAN, J. 2001. Constraint-Based Clustering in Large Databases, In Proceedings of the 8th ICDT, London, UK. 55VOORHEES, E.M. 1986. Implementing agglomerativehierarchical clustering algorithmsfor use in document retrieval. Information Processing and Management, 22, 6, 465-476. WALLACE, C. and DOWE, D. 1994. Intrinsic classification by MML  the Snob program.In theProceedings of the 7th Australian Joint Conference on Artificial Intelligence, 37-44, UNE, World Scientific Publishing Co., Armidale, Australia.WALLACE, C. and FREEMAN, P. 1987. Estimation and inference by compact coding.Journal of the Royal Statistical Society, Series B, 49, 3, 240-265. XU, X., ESTER, M., KRIEGEL, H.-P., and SANDER, J. 1998. A distribution-based clustering algorithm for mining large spatial datasets. In Proceedings of the 14th ICDE,324-331, Orlando, FL.WANN C.-D. and THOMOPOULOS, S.A. 1997. A Comparative study of self-organizing clustering algorithms Dignet and ART2. Neural Networks, 10, 4, 737-743. WARD, J.H. 1963. Hierarchical grouping to optimize an objective function. JournalAmer. Stat, Assoc., 58, 301, 235-244.WANG, W., YANG, J., and MUNTZ, R. 1997. STING: a statistical information grid approach to spatialdata mining. In Proceedings of the 23rd Conference on VLDB, 186-195, Athens, Greece. 
WANG, W., YANG, J., and MUNTZ, R.R. 1998. PK-tree: a spatial index structure for high dimensional point data. In Proceedings of the 5th International Conference of Foundations of Data Organization.WANG, W., YANG, J., and MUNTZ, R.R. 1999. STING+: An approach to active spatial data mining. In Proceedings15th ICDE, 116-125, Sydney, Australia. XU, X., ESTER, M., KRIEGEL, H.-P., and SANDER, J. 1998. A distribution-based clustering algorithm for mining in large spatial databases. In Proceedings of the 14thICDE, 324-331, Orlando, FL. YAO, A. 1982. On constructing minimum spanning trees in k-dimensional space and related problems.SIAM Journal on Computing, 11, 4, 721-736. ZHANG, B. 2001. Generalized k-harmonic means  dynamic weighting of data in unsupervised learning. In Proceedings of the 1st  SIAM ICDM, Chicago, IL. ZHANG, T., RAMAKRISHNAN, R. and LIVNY, M. 1996. BIRCH: an efficient data clustering method for very large databases. In Proceedings of the ACM SIGMOD Conference, 103-114, Montreal, Canada. ZHANG, T., Ramakrishnan, R., and LIVNY, M. 1997. BIRCH: A new data clustering algorithm and its applications. Journal of Data Mining and Knowledge Discovery, 1, 2, 141-182.
ZHANG, Y., FU, A.W., CAI, C.H., and Heng. P.-A. 2000. Clustering categorical data. In Proceedings of the 16th ICDE, 305, San Diego, CA. 56 Procedia Computer Science   72  ( 2015 )  306  313 
1877-0509  2015 Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license 
(http://creativecommons.org/licenses/by-nc-nd/4.0/
).Peer-review under responsibility of organizing committee of Information Systems International Conference (ISICO2015)
doi: 10.1016/j.procs.2015.12.145 ScienceDirect
Available online at 
www.sciencedirect.com
The Third Information Systems International Conference 
Data Mining in Healthcare 
 A Review 
Neesha Jothi
a
b, Wahidah Husain
c, a* abcSchool of Computer Sciences, Universiti Sains Malaysia, 11800 Minden, Penang Malaysia 
 Abstract 
The knowledge discovery in database (KDD) is alarmed with development of methods and techniques for 
making use of data. One of the most important step
 of the KDD is the data mining. Data mining is the 
process of pattern discovery and extr
action where huge amount of data is
 involved. Both the data mining 
and healthcare industry have emerged some of 
reliable early detection systems and other various 
healthcare related systems from the clinical and diag
nosis data. In regard to this emerge, we have 
reviewed the various paper involved in this field in
 terms of method, algorithms and results. This review 
paper has consolidated the papers reviewed inline 
to the disciplines, model,
 tasks and methods. Results 
and evaluation methods are discussed 
for selected papers and a summary of the finding is presented to 
conclude the paper.  
 
 2015 Published by Elsevier Ltd. Selection and/or peer
-review under responsibility of the scientific 
committee of The Third Information System
s International Conference (ISICO 2015) 
 Keywords:
 Data Mining, Data Mining in Healthcare, Health Informactics;  
1. Introduction  
Across all the fields, data 
are being collected and accumulated at a vi
vid pace. There is an urgent need 
for a new generation of computational theories and tool
s to assist humans in extracting useful information 
(knowledge) from the rapidly growing volumes of digital data. At the core of the process is the 
application of specific data mining 
methods for pattern discovery and 
extraction [1]. Among the data 
mining techniques developed in recent years, the 
data mining methods are including generalization, 
characterization, classification, cluster
ing, association, evolution, patter
n matching, data visualization and 
meta-rule guided mining. [2]. As an element of data mining technique research, this paper surveys the 
 * Corresponding author. Tel.:+604-653-3645; fax: +604-657-4759. 
E-mail address
: nj14_com042@student.usm.my 
 2015 Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license 
(http://creativecommons.org/licenses/by-nc-nd/4.0/
).Peer-review under responsibility of organizing committee of Information Systems International Conference (ISICO2015)
307 Neesha Jothi et al.  /  Procedia Computer Science   72  ( 2015 )  306  313 
 development of data mining technique, through a liter
ature review and the classification of articles from 
2005 until 2015
 are reviewed. The period is importan
t because, during the time period
 there is a newly 
widespread of data mining techniques being used in the healthcare industry where technology
 has played 
a significant role especially in the development of methodologies for the collection
 of data from online 
databases. The review interest for this literature revi
ew, started in the March 2015 with searches made of 
the keyword indices on the ScienceDirect, Springer
link and IEEE Xplore online databases, for full article 


2015, 3840 articles were found. Topic filtering reduced this number to 205 ar
ticles, which were related to 
the keyword. From the 205 articles, 50 articles is
 used for this review, the papers are collected based on 

. The 
remaining part of the paper is organized as follows
. Section 2.0 discusses the overview of data mining. 
While section 3.0 discuss the various data mining algorithms used in healthcare. 
 2. Data Mining An Overview 
 Data size are generally growing from day to day. 
The need to understand large, complex, information 
enriched data sets has now 
increased in all the varied fields of 
technology, business and science. With 
these large amount of data, the ability 
to extract useful knowledge hidden in these large amount of data 


process of applying computer based informatio
n system (CBIS), including new techniques, for 
discovering knowledge from data is
 called data mining [3]. The followi
ng subsections will be oriented to 
define the mentioned 
attributes of data mining, provide their related instances and insight some figures on 
their occurrence among the 50 articles mentioned in the section 1.0.
   2.1. Disciplines I
nvolved in Data Mining 
 The data mining baseline is grounded by disciplines such as machine learning [4], artificial 
intelligence [5], probability [6]
 and statistics [7]. The disciplines iden
tified among the papers reviewed 
are summarized in Table 1. Table 1 assets the di
sciplines mentioned for the papers reviewed. 
 Table 1. The different disciplines in the papers reviewed 
 Discipline
 Count
 Machine learning 
 40 Artificial intelligence
 5 Statistical 
 3 Probability
 2  2.2. Data Mining Models
 Generally, there are two kinds of data mining mod
els: predictive model and d
escriptive model  [8]. 
The predictive model often apply supervised learning 
functions to predict unknown or future values of 
other variables of interest [8]. Th
e descriptive model on the other hand, often apply the unsupervised 
learning functions in finding patterns describing 
the data that can be interpreted humans [8]. 



. 

 308   Neesha Jothi et al.  /  Procedia Computer Science   72  ( 2015 )  306  313 
Table 2. The two different models in the papers reviews  
 Model
 Count
 Predictive
 47 Descriptive 
 3  2.3. Data Mining Tasks
 Usually, the implementation of a model is made by
 a task. For instance, clustering [9], association 
rules [10], correlation analysis [11], are often us
ed for descriptive models. While classification [12], 
regression [13]
 and categorization [14]
 are used for predictive models. Table 3 shows the task derived 
from the papers reviewed.
  Table 
3. The task derived from the papers reviewed 
 Task 
 Count
 Classification 
 42 Association rules
 5 Clustering 
 2 Anomaly detection 
 1 2.4. Data Mining Methods 
 Having the data mining model and task defined, next would be the data mining methods to build the 
approach based on discipline invo
lved. The methods used for anomaly detection are, standard support 
vector data description, density induced support ve
ctor data description, Gaussian mixture. While the 
vector quantization method is widely
 used for clustering. The methods
 widely used for classification are 
statistical, discriminant analysis, decision 
tree, Markov based, swarm intelligence, k
-nearest neighbor, 
genetic classifiers, artificial neural networ
k, support vector and association rule. 
 3. Data Mining Algorithms in Healthcare
 Healthcare covers a detailed processes of the diagnosis, 
treatment and prevention of disease, injury and 
other physical and mental impairments in humans [15].  The healthcare industry in most countries are 
evolving at a rapid pace. The healthcare industry can 
be regarded as place with 
rich data as they generate 
massive amounts of data including electronic medical
 records, administrative reports and other 
benchmarking finding [16]. 
 These healthcare data are however being under
-utilized. As discussed in 2.0 
data mining is able to search for new and valuable information from these large volumes of data. Data 
mining in healthcare are being used mainly for predicting various diseases as well as in assisting for 

diagnosis for the doctors in making their clinical decisi
on. The discussion on the various methods used in 
the healthcare industry are discussed as follows.
   309 Neesha Jothi et al.  /  Procedia Computer Science   72  ( 2015 )  306  313 
3.1. Anomaly Detection 
    Anomaly detection is used in discovering the most si
gnificant changes in the data set [17]. Bo Lie et al 
[18] had used three different anomaly detection method,
 standard support vector data description, density
-induced support vector data description and Gaussian 
mixture to evaluate the accuracy of the anomaly 
detection on uncertain dataset of 
liver disorder dataset which is ob
tained from UCI. The method is 
evaluated using the AUC accuracy. The results obtain
ed for a balanced dataset by average was 93.59%. 
While the average standard deviation obtained from 
the same dataset is 2.63. The uncertain dataset are 
prone to be available in all datasets, th
e anomaly detection would be a g
ood way to resolved this matter, 
however since there is only one paper discussing
 this method, we cannot comment much on the 
effectiveness of the method. 
 3.2. Clustering
        The clustering is a common 
descriptive task in which on
e seeks to identify a finite set of categories or 
clusters to describe the da
ta [17]. Rui Velosoa [19]
 had used the vector quantization method in clustering 
approach in predicting the readmissions in intens
ive medicine. The algorithms used in the vector 
quantization method are k
-means, k
-mediods and x
-means. The datasets used in this study were collected 


conducted using the Davies
-Bouldin Index. The k
-means obtained the best results while x
-means obtained 
a fair results while the k
-mediods obtained the worst results. From the results the work by these 
researchers provide a useful result in 
helping to characterize the different 
types of patients having a higher 
probability to be readmitted. A more significant compar
ison on the method cannot 
be made since this is 
the only one paper in my review discussing on the vector quantization.
  3.3. Classification 
     Classification is the discovery of a predictive learning 
function that classifies a data item into one of 
several predefined classes [17]. The related work in classification will be discussed in the following 

subsections. 
 3.3.1. Statistical  The MTS algorithm is being extensi
vely applied in multivariable statistical analysis. The Mahalanobis 
distance (MD) is used to build statistical judgements
 to distinguish one group from another and the 
Mahalanobis space (MS) is used to
 represent the degree of abnormality of observations from the known 
reference group.
 In the statistical classifiers, the authors Su et al.
 [20], have used the Mahalanobis 
Taguchi System (MTS) to design the prediction mod
el for pressure ulcers. The class imbalance problems 
are very much prevalent in the healthcare datasets.
 Usage of the data mining algorithms are often affected 
with skewed distribution when using skewed or 
imbalanced data sets. This problem often leads to the 
tendency of producting highly predictive classification accuracy over the majority class and poor 
accuracy over the minority class.
 Having such a nature to distinguish the degree of abnormality of 
observations, this method would be 
a good method to test on the real
 data set pressure ulcers. This 
method is also used since the MD is sui
tably scaled. The test conducted using this algorithms were done 
in four phases with scaled datasets ranging from 14 to 8, 5, and 2 accordingly. The results obtained in the 
paper [20]
 shows that the measurement scale for this algorithm has good a performance based on the huge 
difference between the normal and abnormal examp
les. Being an algorithm which is suitable for scaling
 the
 MTS proves to have better sensitivity and g
-means values in the testing st
age. The MTS has enhanced 
performance in terms of sensitivity. 
 310   Neesha Jothi et al.  /  Procedia Computer Science   72  ( 2015 )  306  313 
3.3.2. Discrimin
ant Analysis 
 Linear discriminant analysis
 (LDA)
 is widely used in discriminant analysis to predict the class based 
on a given set of measurements on new unlabeled observations [17]. Authors Armaanzas
 et al. [21] and 
Jen et al
 [22] have used the linear discriminant analysis in their respective work.
 Jen et al
 [21] had the 


atient using scores of non
-motor 
symptoms.
 Their study is intended to quantitatively anal
yze the inner relationships between both motor 
and non
-motor symptoms. The linear discriminant analysis is the conditional probability density function 
of the predicto


capture statistical dependencies among the predictor 
variables indicates that this algorithm would be 
suitable
 to explore the linear constraint of this study to discovery the synergy
 between motor and non
-motor symptoms. The proposed model obtained an ac
curacy estimation of 69% compared to other 


form. Based on the
 same nature of the algorithm the authors
 Armaanzas et al.
 [21], used the algorithm to 
evaluate the classification accuracy to seek 
the most substantial risk factor and 
establish the initial set of 
substantial risk factors for chronic illness early warning. From the results of the two work
s we can safely 
say that the algorithm has good results and it is suitable to
 be utilized to identify significant accuracy if 
the relationships of the healthcare data are in linear form. 
 3.3.3. Decision Tree Several study have explored the decision
 tree method to analyze clinical data.
 The authors Sharma & 
Om [23]
, Wang et al.
 [24] and Zolbanin 
et al.[25] have used the decision tree algor
ithm in their respective 
work. Having the nature to examine data and make the 
tree and its rules are used to make a prediction. All 
the three work
s have used the decision tree to the data s
et to improve the prognostic performance, in 
terms of accuracy. The nature of the data set used in
 this research are rather 
balanced set of data set. 
From the comparative of the work
s, we conclude that decision tree as cannot be used in proposing prognostic 
decision to solve imbalanced problems because the de
cision tree recursively separate observations into 
branches to construct a tree. 
 3.3.4. Swarm Intelligence
 The authors 
Yeh et al. [26], Fei 2010 [27], and Abdi & Giveki [28]
 have used the swarm intelligence 
method to designed their diagnosis model. The 
algorithm particle swarm optimization (PSO) is able to 
efficiently find the optimal or near optimal solutions
 in large search spaces. All the three authors tried
 to resolve optimzation problem which often involv
es features in the classification problems. The 
classification process will be faster and more accu
rate if less number of features are used. 
From the work 
studied, the PSO based approach proves to improv
e the overall classification resu
lts since PSO is being 
used to select suitable parameters in the involved classifiers. 
 3.3.5. K-Nearest Neighbor
 Authors 
Garca-
Laencina et al. [29], Armaanzas et al.[21], Jen et al.[22
], Bagui et al.[30], and a

et al. [31]
 have used the k
-nearest neighbour in their resp
ective predictive  models. The k
-nearest 
neighbour is an instance based classifier method. The pa
rameter units consists of samples that are used in 
the method and this algorithm then assumes th
at all instances relate to the points in the 
n-dimensional 
space RN. The algorithm is very expedient as the informatio
n in the training data is never lost. However, 
this algorithm would be suitable if 
the training data set is large as this
 algorithm is very time consuming 
when
 each of the sample in training set is processed wh
ile classifying a new data and this process requires 
a longer classification time. From the work by the mentioned authors, the classification accuracy is what 
311 Neesha Jothi et al.  /  Procedia Computer Science   72  ( 2015 )  306  313 
 they would like to attain
 instead of classification time as the classification accuracy
 is more important in 
the medical diagnosis. 
 3.3.6. Logistic Regression 
 Logistic regression (LR) is a method that would use the given set of features either continuos, discrete, 
or a mixture of both types and the binary target, th
e LR then computes a linear combination fo the inputs 
and passes through the logistic function
 [29]. This method is commonly used because 
it is easy to
 implementation and 
it provides competitive results. Authors 
Garca-
Laencina et al. [29], Mamiya et al. 
[32], Su et al. [20], Wang et al.[24], Zolbanin et al.
 [25], Thompson et al. [33], and Samanta et al. [34]
 have adopted the LR in their resp
ective research work. The results obtain
ed from all the authors are not 
very significant, due to the signifiant decrement in the size of the input data sets. 
The results would have 
been more significant if the datasets were large in quan
tity as the boundaries of accuracy would be larger. 
The LR works well for larger datasets. 
 3.3.7. Bayesian Classifier  Authors Armaanzas et al. [
21], and Bandyopadhyay et al.[35]
 have used the Bayesian classifier 
method in their respective predic
tive model. The Bayesian classifiers is well known for its computational 
efficient and ability to handle missing data naturally and eff
iciently. Having this
 advantage both the 
authors have recorded a good prediction accuracy from 
the models  designed respectively. By having the 
models implemented the Bayesian classifier also prov
es that the model is su
itable since the averaging 
approach has led to improved prediction accuracy an
d allows authors to extract more features from the 
data without being overfitting. This method would be
 a good approach if there data sets are suffering 
from missing data. 
 3.3.8. Support Vector 
 The support vector method (SVM) is proven to be 
advantageous in handling classification tasks with 
execellent generalization performance. The method seeks to minimize the upper bound of the 
generalization error based on
 the structural risk minimization principle. The SVM training is equivalent to 
solve a linear constrained quadratic programming problem [36]. The method is very commonly used in 

medic
al diagnosis. Authors 
Garca-
Laencina et al. [29], Zheng et al. [36], Kang et al. [37], and Su et 
al.[20] have used the method in their model in medical diagnose
s. Some of the authors have used the 
SVM method for comparative study purpose. The SVM meth
od generalization ability is controlled by two 
different factors, that is the training error and the ca
pacity of the learning machine measured. The training 
error rate can be controlled by changing the features 
in the classifiers. From the results obtained from the 
studies, it clearly shows that the SVM showed greater 
performance since it maps the features to higher 
dimensional space
  
 From the papers reviewed and discussed, the data mi
ning methods accuracy varies depending on the 
features of
 the data sets and the size of data set betwee
n the training and testing sets. The common 
characteristics among the healthcare
 data sets are highly imbalanced d
ata sets, where by the majority and 
the minority classifier are not balanced
 resulting prediction erroneous
 when run by the classifiers. 
Another characteristics of healthcare data 
sets are the missing values. The sample size
 of the data is often 
seen as another characteristics 
as the data available are usually in small scale. The
re is no one suitable 
data mining method to resolve all this issues.
  312   Neesha Jothi et al.  /  Procedia Computer Science   72  ( 2015 )  306  313 
 Conclusion 
 The data mining has played in an important role in 
healthcare industry, especially in predicting various 
types of diseases. The diagnosis is widely being used
 in predicting diseases, they are extensively used in
 medical diagnosing. In conclusion, there is no one 
data mining method to resolve the issues in the 
healthcare data sets. In order to obtain the highes
t accuracy among classifiers which is important in 
medical diagnosing
 with the characteristics of d
ata being taken care, we need
 to design a hybrid model 
which could resolve the mentioned issues. Our future di
rections is to enhance the predictions using hybrid 
models. 
 Acknowledgements
        
We would like to express our gr
atitude to Universiti Sains Malaysia (USM) for supporting this 
research. References [1]
 
Current
, pp. 3
8, 1995.
 [2]
 
ining techniques and applications 
- 
Expert Syst. Appl.
, vol. 39, no. 12, pp. 11303
11311, 2012.
 [3]
 
-based information systems by German managers to 
suppo

Inf. Manag.
, vol. 41, no. 6, pp. 763
779, 2004.
 [4]
 I. H. Witten, E. Frank, and M. a Hall, 
Data Mining: Practical Machine Learning Tools and Techniques (Google eBook)
. 2011.
 [5]
 D. K. Bhattacharyya and S. M. Hazarika, 
Networks, Data Mining And Artificial Intelligence: Trends And Future 
Directions
, 1st ed. Narosa Pub House, 2006.
 [6]
 

-Mining by Probability
-
360, 2008.
 [7]
 H. Thomas and L. Paul, 
Statistics: Methods and Applications
, 1st ed. StatSoft, Inc, 2005.
 [8]
 M. Kantardzic, 
Data Mining: Concepts, Models, Methods, and Algorithms
, 2nd ed. Wiley
-IEEE Press, 2011.
 [9]
 
Group. Multidimens. Data
, no. c, pp. 25
71, 2006.
 [10]
 

Fuzzy Sets Syst.
, vol. 138, no. 2, pp. 255
269, 2003.
 [11]
 
ation Analysis: An Overview with Application to Learning 

J. Neural Comput.
, vol. 16, no. 12, pp. 2639 
 2664, 2004.
 [12]
 

Lect. Notes 
Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics)
, vol. 3918 LNAI, pp. 199
204, 2006.
 [13]
 
-
988, 2007.
 [14]
 
-Scale
 
Technometrics
, vol. 49, no. 3, pp. 291
304, 2007.
 [15]
 J.-


Comput. Ind.
, vol. 69, pp. 3
11, 2015.
 [16]
 

18, 2005.
 [17]
 U. Fayyad, G. Piatetsky
-

AI Mag.
, pp. 
3754, 1996.
 [18]
 
-
Knowl. Inf. Syst.
, vol. 
34, no. 3, pp. 597
618, 2013.
 313 Neesha Jothi et al.  /  Procedia Computer Science   72  ( 2015 )  306  313 
 [19]
 R. Veloso, F. Portela, M. F. Santos, . Silva, F. Rua, A. Abelha, 


Procedia Technol.
, vol. 16, pp. 1307
1316, 2014.
 [20]
 

re ulcer 

J. Med. Syst.
, vol. 36, no. 4, pp. 2387
2399, 2012.
 [21]
 R. Armaanzas, C. Bielza, K. R. Chaudhuri, P. Martinez
-
-motor 


Artif. Intell. Med.
, vol. 58, no. 3, pp. 195
202, 2013.
 [22]
 C.-H. Jen, C.
-C. Wang, B. C. Jiang, Y.
-H. Chu, and M.
-
an early
-warning system for chronic illnesses
Expert Syst. Appl.
, vol. 39, no. 10, pp. 8852
8858, 2012.
 [23]
 

Netw. Model. Anal. Heal. 
Informatics Bioinforma.
, vol. 2, no. 4, pp. 285
295, 2013.
 [24]
 K.-J. 
Wang, B. Makond, and K.
-



BMC Med. Inform. Decis. Mak.
, vol. 13, p. 
124, 2013.
 [25]
 H. M. Zolbanin, D. De



Decis. Support Syst.
, vol. 74, pp. 150
161, 2015.
 [26]
 W.-C. Yeh, W.
-

pattern using discrete 

Expert Syst. Appl.
, vol. 36, no. 4, pp. 8204
8211, 2009.
 [27]
 

-
Expert Syst. Appl.
, vol. 37, no. 10, pp. 6748
6752, 2010.
 [28]
 
-squamous diseases using PSO
-SVM based on association 

Eng. Appl. Artif. Intell.
, vol. 26, no. 1, pp. 603
608, 2013.
 [29]
 P. J. Garca
-

-year survival 


Comput. Biol. Med.
, vol. 59, pp. 125
133, 2015.
 [30]
 S. C. Bagui, S. Bagui,
 

36, pp. 25
34, 2003.
 [31]
 

-artificial immune system and k
-nn 

Comput. Biol. Med.
, vol. 37, no. 3, pp. 415
423, 2007.
 [32]
 H. Mamiya, K. Schwartzman, A. Verma, C. Ja




J. Biomed. Inform.
, vol. 
53, pp. 237
242, 2015.
 [33]
 V. L. S. Thompson, S. Lander,
 



10, 2014.
 [34]
 B. Samanta, G. L. Bird, M. Kuijpers, R. a. Zimmerman, G. P. Jarvik, G. Wernovsky, R. R. Clancy, D. J. Licht, J. W. 



Artif. Intell. Med.
, vol. 46, no. 3, pp. 201
215, 2009.
 [35]
 S. Bandyopadhyay, J. Wolfson, D. M. Vock, G. Vazquez
-Benitez, G. Adomavicius, M. Elidrisi, P. E. Johnson, and P. J. 

Data mining for censored time
-to-event data: A Bayesian network model for predicting cardiovascular risk 
from electronic health record data
. 2014.
 [36]
 

-means and 

Expert Syst. Appl.
, vol. 41, no. 4 PART 1, pp. 1476
1482, 2014.
 [37]
 S. Kang, P. Kang, T. Ko, S. Cho, S. Rhee, and K.
-

for anti
-
Expert Syst. Appl.
, vol. 42, no. 9, pp. 4265
4273, 2015. 
  Available online at www.sciencedirect.com
2212-8271  2017 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license 
(http://creativecommons.org/licenses/by-nc-nd/4.0/
).Peer-review under responsibility of the scientific committee of the 9th CIRP IPSS Conference: Circular Perspectives on Product/Service-Systems.
doi: 10.1016/j.procir.2017.03.131 
 Procedia CIRP   64  ( 2017 )  306  311 
ScienceDirect
The 9th CIRP IPSS Conference: Circular Perspectives on Product/Service-Systems Data Mining in Product Service Systems Design: Literature Review and Research Questions  Alessandro Bertoni*, Tobias Larsson  Blekinge Institute of Technology, Campus Grsvik, 37179 Karlskrona, Sweden 
 * Corresponding author. Tel.: +46 455 38 55 02. E-mail address: alessandro.bertoni@bth.se Abstract The paper presents a literature review about data mining appli
cations in Product/Service-Systems (PSS) design. A systematic lit
erature review, combined with snowballing techniques, has been run to identify re
levant contributions in the area. The analysis has focused on the categorization of the contributions according to their impact on the PSS design process and according to their theoretical or e
mpirical nature. A 
picture of the different research achievements for each stage of the PSS design process have been drawn, identifying the research gaps in respect to the challenges of PSS design. Based on the analysis the paper proposes a set of research questions for each PSS design stage with the 
intent of facilitating the application of data mining techniques 
in PSS design, and ultimately push forward the state of the ar
t.   2017 The Authors. Published by Elsevier B.V. Peer-review under responsibility of the scientific committee of the 9th CIRP IPSS Conference: Circular Perspectives on Product/
Service-
Systems.  Keywords: Product service systems design; data mining; research questions; literature review.  1. Introduction Information and communication technologies have revolutionised lifestyles, global interaction and industrial 

working practices. Companies are nowadays potentially 

capable of collecting data about any product lifecycle activity 
and performance. Devices and sensors may become smart 
and are used in a variety of contexts: from monitoring the performances of machines, to predict failures and run 
preventive maintenance, to provide driving assistance and to 
manage a whole fleet of vehicles based on GPS (see for 

instance [1-3]). The use of data mining in combination with 
the development of IT infrastructures and with increased data storage capabilities, has propelled a profound shift toward 

more transparent, informed and autonomous decision-making [4]. However, while companies are often in the situation of 

being capable of collecting a huge amount of data, their use is 
often limited to maintenance and management purposes; more 
rarely those data become useful knowledge and insights in the 
design phase of a new products or services. A challenge is given by the multi-dimensional and multidisciplinary nature of 

the design process, generating a large amount of 
heterogeneous data for which suitable mining methods are not 

readily available [5], accentuated by a generic lack of context 
around the situation where data is collected. In the 
development of products and services combinations, i.e. 
product/service-systems (PSS), a formalized approach on how to use data to develop new and more value adding solutions is 

missing. This is due to the relative novelty of both the PSS 
design field and of the data mining field, which have developed with different focuses requiring different expertise.   The research presented in this paper has the purpose to explore the potential that resides in the integration of data 
mining techniques into methods and tools for PSS design. The 
aim of the paper is therefore to investigate, and review, the 
theoretical and empirical applications of data mining in PSS design literature, by:  Mapping and analyzing the curre
nt contributions into the PSS design process.   2017 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license 
(http://creativecommons.org/licenses/by-nc-nd/4.0/
).Peer-review under responsibility of the scienti c committee of the 9th CIRP IPSS Conference: Circular Perspectives on Product/Service-Systems.
307 Alessandro Bertoni and Tobias Larsson  /  Procedia CIRP   64  ( 2017 )  306  311 
 Highlighting a list of research questions to be addressed to 
enable a wider application of data mining into PSS design. The paper first describes the methodology applied in the literature review by briefly presenting the PSS design process 
used as a reference for the analysis. In section 3 the literature review and the analysis are presented together with a number of identified research questions. Section 4 closes the paper by 
drawing the final conclusions. 2. Methodology 
The research was initially a
pproached through a literature search run on a major research database (i.e. Scopus1). The scope of the search was deliberately limited to those papers presenting the use of data mining techniques referring to the 
design of PSS or to the conjunct design of products and 
services. The concept of machine learning, although not 

synonym of data mining (as explain in section 3) was also included in the research, to assure the completeness of the 

results. In a first step a systematic research for papers was performed. The search was directed toward titles, abstracts or 

keywords containing the term product service systems in combination with either data 
mining machine learning or data science. This first round of analysis led to the 
identification of only 13 unique papers. Due to such limited 
number of publications the literature base was expanded by 

applying a snowballing technique [6] on the initial set of 

papers. The reference list of each of the paper was screened 
for relevant contributions and 21 papers were selected for 

detailed analysis at the end of the process.  Those papers were later analyzed by classifying their content and contribution in relation to the PSS design process. To define the categories for analysis the Generic IPSS/PSS 

development process model by Mller and Stark [7] was 

adopted as reference. The model is V-shaped and 
encompasses 4 levels of detail: market/customer/environment 

level, value level, system level, module and component level. For each level, different activities take place involving 
different stakeholders. The following five main activities have 

been identified in accordance to the model and have been 
used to provide a first classification of the literature:  Planning  Idea generation   Embodiment design for subsystem 
 Detailed design  Delivery and use phase  The papers were further categorized by adding a distinction between those describing a framework or a conceptual 
method and those providing examples of real case study applications. The classification was run in a way that each 
paper could be assigned only to one category. The 
discriminant for the selection was the main contribution of   1 www.scopus.com the paper, evaluated based on the content and on the 
terminology stressed by the authors in the conclusion 
section of the papers.  The analysis of the literature and the research questions derived are the result of an analytical process applied to a 
critical literature review (similarly to what described by 

Jessons and Lacey [8]). The theoretical contributions were 
analyzed in respect with the available examples in applied 
research. This has allowed the production of a general 

framework of analysis in consideration of the main issues and 

challenges recognized in literature for the design of PSS, 
encompassing weak and sweets spots of the main PSS design 

methods and tools. Upon this basis, the direct impact on 
applied practice was the driving criterion for the definition of 

the questions, with a focus on the empirical benefits of 
developing more advanced method and tools for PSS design. 3. Data mining in PSS design: contributions and research 
questions Data mining is defined as the discovery of non-trivial, 
implicit, previously unknown, and potentially useful and understandable patterns from large datasets [9]. When it 
comes to application of data mining in industrial 
environments, the term is often associated with the concept of 

machine learning, i.e. the study 
of computer algorithms that improve automatically through experience [10]. Data mining 
and machine learning are used in engineering both with the predictive goal of forecasting the value of a variable and with 
the descriptive goal of understanding and discovering patterns 
in the available data [9]. The following sub-sections provide 

an account of the application of data mining and machine 
learning available in literature highlighting applications 
explicitly referring to the context of PSS design. 
3.1. Summary of state-of-the-art and paper categorization The papers identified in the literature review have been 
first categorized based on if their main contribution in respect to the PSS design process, and then based on the type of 
academic contribution, that is, if the paper presented a 
conceptual framework or method, or if it concerned the 
presentation of a real case implementation of an approach. A further distinction was made based on the terminology used 

when referring to the design of product and service 
combinations, i.e., the papers e
xplicitly referring to PSS in the 
text, and the papers not directly using the PSS terminology 
even if dealing with relevant t
opics for the investigation. The 
reason for doing this last classification was to verify the 

popularity of the data mining topic in the major publication arena for PSS research. The results of the classification are visualized in Figure 1. The numbers in the white circles indicate the number of 

publications for each category directly or indirectly related to 
the development of PSS, the numbers in the grey circles indicate instead the number of those publications explicitly 

referring to a PSS terminology. 
 308   Alessandro Bertoni and Tobias Larsson  /  Procedia CIRP   64  ( 2017 )  306  311 
309 Alessandro Bertoni and Tobias Larsson  /  Procedia CIRP   64  ( 2017 )  306  311 
310   Alessandro Bertoni and Tobias Larsson  /  Procedia CIRP   64  ( 2017 )  306  311 
311 Alessandro Bertoni and Tobias Larsson  /  Procedia CIRP   64  ( 2017 )  306  311 
 Procedia CIRP   62  ( 2017 )  123  128 
Available online at www.sciencedirect.com
2212-8271  2017 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license 
(http://creativecommons.org/licenses/by-nc-nd/4.0/
).Peer-review under responsibility of the scientific committee of the 10th CIRP Conference on Intelligent Computation in Manufacturing Engineering
doi: 10.1016/j.procir.2016.06.120 
ScienceDirect
10th CIRP Conference on Intelligent Computation in Manufacturing Engineering 
- CIRP ICME '16
 Data mining techniques applied to a manufacturing SME
   Michael S Packianather
a,*
, Alan Davies
a, Sam Harraden
a, Sajith Soman
b, John White
b  aSchool of Engineering,
 Cardiff University, Cardiff CF24 3AA, UK
 bBrick Fabrication Ltd, Pontypool NP4 6YW, South Wales, UK
 * Corresponding author. Tel.: +44
-029-20875911 ; fax: +44
-029
-20874716. E
-mail address: PackianatherMS@cf.ac.uk
 Abstract
 This paper examines how data mining, an aspect of analytical scie
nce, can be applied to assist a Small to Medium Enterprise (SM
E) industry 
using
 unsupervised learning techniques, association rules and
 time
-series analysis. Whilst recent developments have meant it is now possible 
for SME to
 compile large amounts of commercial data, this information is rarely
 utilised effectively. The study builds on a number of stan
dard 
data mining techniques to produce a tailored set of analyses that provide maximum benefit to the company. Self
-Organising Maps were utilised 
to visualise the core characteristics of the firms customers. 
The study outlines a new technique to determine associations between customer 
variables using the arules package available within RStudios.
 Finally, time
-series forecasting was conducted
 highlighting the seasonal 
variations and
 trends for potential growth in the coming year.
  201
6 The Authors. Published by Elsevier B.V.
 Peer
-review under responsibility of
 the Internat
ional Scientific Committee of 10
th CIRP ICME Conference"
.  Keywords:
 Data
 mining; time
-series analyses; Association rules; Unsupervised learning;
 K-means clustering; Hierarchical clustering and self
-organising maps
1. Introduction
 The advances in data proces
sing and storage power has 
meant that data analytics is no longer the privilege
 of large 
multinational technology corporations but it
 can be used as a 
key component in the formation of strategy for companies of a 
range of sizes. In fact, data an
alytics is being used more and 
more by Small to Medium Enterpr
ises (SMEs) to discover a 
wealth of information, including; customer purchasing 
patterns, sales forecasting, and efficient customer relationship 

management. Analysis by Bokman
 et al. [1] states a 126% 
profit improvement over competitors by companies that make 
extensive use of customer analytics. As such, failure to 
incorporate data analysis into 
strategy formulation can result 

in competitive disadvantage within the industry. 
 Brick Fabrication (BF) 
Ltd. is an SME involved in 
manufacturing various products made of cut bricks including 
brick clad chimneys, arches, etc., for the housing market.  The 

BF headquarters is
 in Pontypool, Wales, with an additional 
site in Bromley, Essex, providing
 Brick Clad Chimney 
products to a range of clients, generating approximately three 
million pounds worth of revenue per annum. The companys 
mission is to become the fastest, most reliable and the most 
innovative supplier of building solution services in the UK 
and as such, need an innovative and real
-time strategy to 
match. Whilst a wealth of commercial data is currently 
collected by the firm in order to process invoice and payment 

orders, this information has not yet been used as a means of 

improving the growth of the firm. This study uses BF as a case 

study into how data mining 
 an aspect of analytics 
 can be 
used to assist in the discovery of knowledge and formulation 
of strategy through analysis of commercial sales data.
 The core aim of this study
 is to demonstrate how data 
mining can be applied to real world data, using BFs 
commercial database as a case study. In addition to this, the 

study
 sets out to produce a series of results from which the 
company can make informed decisions in a strategic manner.
  The specific objectives of this
 study
 are:    Provide an understanding of key aspects of data mining: 
Unsupervised learning, association
 analysis, and time
-series analysis.
  Utilise
 unsupervised learning techniques to segment BFs 
customer list.  
  2017 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license 
(http://creativecommons.org/licenses/by-nc-nd/4.0/
).Peer-review under responsibility of the scienti c committee of the 10th CIRP Conference on Intelligent Computation in Manufacturing Engineering
124   Michael S Packianather et al.  /  Procedia CIRP   62  ( 2017 )  123  128 
 Expand on association analysis to better understand what 
constitutes a typical order for a BF customer. 
 Develop time-series forecasting techniques to discover 

patterns and trends of customers. 
 Establish a business case for using long-term data mining 

analysis within BF Ltd.   
 
The paper is organized as follows. Data mining techniques 
are described in section 2. The results of data mining 

techniques are presented in section 3. The paper is concluded 

in section 4.   
2. Datamining Techniques 
Data Mining is a general term, describing the method in 
which one can discover hi
dden trends, patterns and 
associations that would otherwise remain undiscovered [2]. In 
addition to this, it can be used to predict future occurrences, 

such as the growth of the market or the likelihood of customer 

loyalty. It relies on aspects of statistics, mathematics and 

computing, and should be used as part of an overall holistic 

approach to knowledge discovery. 
The methodology used for the data mining analysis is a 
framework referenced from 
Giudicis Applications of Data 
Mining [3] and is as follows: 
A.  Definition of the objectives for analysis. 

B.  Selection, organisation & pre-treatment of the data.  

C. Exploratory analysis of the data and subsequent 
transformation. 
D.  Specification of the statistical methods to be used in 
the analysis phase. 
E.  Analysis of the data based on the chosen methods. 

F.  Evaluation and comparison of the methods used and 
the choice of the final model for analysis. 
G. Interpretation of the chosen model and its subsequent 
use in decision processes. 
 
In an Introduction to Data Mining, Tan et al. [4] outlines 
five key aspects of data mining: data exploration, 

classification, clustering, association, and time-series analysis. 

To provide a comprehensive understanding of data mining, 

the theory of some of these major techniques will be 

discussed below. 
2.1. Data Exploration 
Data Exploration is an initial investigation of the data to 
understand its main characteris
tics and decide on the best 
approach to extract meaningful information.  Its primary 

purpose is to help decide on the most appropriate pre-

processing and data analysis techniques. Specific to this 

project, the aims of the data
 exploration were to: discover 
where the majority of revenue was generated for the firm, 

analyse general trends in growth, and determine the 

geographical location of customers. 
2.2. Classification 
Classification is a machine learning technique which 
classifies data into pre-defined groups. As such, distinct class 

labels are required. No such labels were available for the data 

used in this work and as a result classification will not be 

explored in this study. 
2.3. Clustering 
Clustering is the process of grouping together data points 
that are more similar in attributes than other data points. It is 
an unsupervised learning technique, meaning it can be used to 

analyse meaningful patterns without human supervision or 

intervention.  To quantify the similarity of two data points, the 

distance between points is required. This is typically 
calculated using the Euclidean distance given in equation (1): 
 
 (1) 
Where n is the number of dimensions and x
k and y
k are, 
respectively, the k
th attributes of features x and y.  
 There are two main Clustering techniques which group 
data based on similarity: H
ierarchical and Partioning. 
Hierarchical Clusterin
g builds a hierarchy of clusters through 
one of two approaches known as Agglomerative and Divisive 

clustering. Agglomerative clustering begins with all 

observations of a data set beginn
ing as an individual cluster, 
before pairs are merged togeth
er into a hierarchy structure. 
Meanwhile divisive clustering begins with all observations 
under one cluster, before moving individual observations 
down the hierarchy.  
Partioning on the other hand is a method that constructs 
various partitions, evaluating them by similarity and distance 

to one another. The k-means algorithm is one of the most 

popular methods to achieve this. The algorithm begins by 

assigning k different random 
data points as initial centroids. 
The distance between each data point and the cluster centre is 

subsequently calculated, and data
 points are assigned to the 
centre point with the minimum distance. New cluster centres 

are subsequently calculated, with the aim of minimising the 

Squared Error Function given in equation (2): 

Where c
i represents the number of data points in 
ith cluster. 
The distance between each data 
point and cluster centre is 
recalculated, assigning data points to their new nearest cluster. 

This process is continued until 
iteration results in no further 
changes.  
The advantage of the k-means algorithm is that it is simple 
to use and will automatically assign clusters. The primary 

disadvantage is that the analyst 
must pick an arbitrary number 
of clusters, without knowledge of the optimal number of 

groups to assign. In addition to this, the algorithm struggles to 
assign clusters when the data is not well separated or groups 
are not dissimilar from one another. 
125 Michael S Packianather et al.  /  Procedia CIRP   62  ( 2017 )  123  128 
Self-Organising Maps (SOMs) are a form of Artificial 
Neural Networks (a system of statistical learning algorithms 
that replicate human biological brain patterns) that can be 
used to cluster multidimensional data, lowering the 

dimensional space and allowing it to be presented in a two 

dimensional form. SOMs create networks of nodes across
 a grid, processing the data in a way that allows relationship 

between individual items to be represented through a 

topological structure (i.e. one where properties remain 

preserved) [5].  
The training process a SOM under
takes to train data begins 
by classifying weights for each node. The weightings assigned 

are small, random, standardized values that differ from one 

another. A vector (a quantified value of an item
s variables) is 
then presented randomly to the grid of nodes (known as a 

lattice) and assigned to the node which it is most similar to, 
known as the Best Matching Unit. This is typically done using 
the Euclidean distance discussed earlier. 
Once a Best Matching Unit has been determined, the node 
will assign itself a local neighbourhood. This neighbourhood 

will begin large, shrinking in radius over time by use of an 

exponential decay function.  
The algorithm can be succinctly described step by step as 
outlined by Bacao and Lobo, [6] as follows:  
1) Calculate the distance between the pattern and all 

neurons.  

2) Select the nearest neuron as winner. 

3) Update each neuron 
according to the rule: 
     4) Repeat the process until a 
certain stopping criterion is 
met. Usually, the stopping criterion is a fixed number of 

iterations  

 
Self-Organising Maps were tested on Brick Fabrication 
data where the process could 
automatically discover patterns 
between points based on a large number of variables. This 

could help discover patterns that the company are not 

currently aware of. 
2.4. Association Rules 
Association analysis deals with discovering hidden 
relationships within data sets. Fundamentally, it allows one to 
discover the probability of one specific event occurring as a 
direct result of another.  Association can be performed in a 
number of ways, one of which is market basket analysis. 

Market basket analysis inspects transaction history, looking at 

cases of multiple items being bought in one single transaction. 

This information could be used for multiple purposes at Brick 

Fabrication, such as altering the website catalogue so 

customers view complimentary products together or assisting 

with inventory.  
In addition to providing an understanding of what items 
customers are purchasing together, market basket analysis can 
also be used to highlight multiple items that may not be 
purchased together where one would expect them to, 
potentially discovering a gap in marketing which needs to be 

covered.  
To be able to interpret association rules, it is important that 
one has a basic understanding of the computations used.  

Pang-Ning Tan (Tan, 2003) describes an association rule as 

an implica
tion expression of the form X 
 Y, where X and 
Y are disjoint item sets, i.e. X 
e  Y =
X). This definition is 
better explained by examining the two key criteria used in 

assessing the value of an association rule: The Support and 

Confidence. Support signifies the frequency of a specific rule 

within a dataset. The higher the support, the more that rule is 

involved in the database. According to Agrawal et al. [7] 

support is defined as: 
    
 Meanwhile, confidence is defined as the percentage of 
occurrences that contain X which also contains Y. 

    The support helps one appreciate how prevalent a rule is 
within the data, whilst the confidence assesses the conditional 

probability of that rule occurring.  
The challenge for association analysis is that 100,000s of 
different rules can be generated from a small number of 

different products, even when a minimum support threshold 

and confidence threshold are set. As the majority of these 

rules are of little interest to th
e analyst, algorithms such as 
Apriori are used to eliminate sub-set of rules and rule 

duplication. 
2.5. Time-series Analysis 
A time-series is defined by Esling and Agon 
as a 
collection of values obtained from sequential measurements 
over time
 [8]. Data Mining can be of use for a huge range of 
purposes relating to time-series due to the high dimensionality 

of the data. Application of data mining can include areas such 

as trend alignment, similarit
y modelling, anomaly detection 
and forecasting.  
Time-Series forecasting involves quantifying a series of 
observations into a pattern and 
using this to predict future 
events. One of the most common means of forecasting within 

data mining is Exponential Smoothing. According to Kalekar, 

Exponential smoothing can be defined as a procedure f
or continually revising a forecast in the light of more recent 

experience 
[9]. Essentially, the more recent an observation 
is, the higher the importance given to that observation over 

older observations.  
Exponential Smoothing can be performed at three levels: 
Single order, double order and triple order. Single exponential 

smoothing is used when th
e data oscillates around a fixed 
average, that is, no trend is
 observed. Double exponential 
smoothing observes a general tr
end where no recurrent pattern 
across a fixed cycle is witnessed. Third exponential 
smoothing is used when both
 a trend and seasonality are 
apparent in the data.  
126   Michael S Packianather et al.  /  Procedia CIRP   62  ( 2017 )  123  128 
Accounts$Number.of.Orders
Frequency0100200300400500600
020406080100
The basis of triple exponential smoothing relies on a series 
of equations known as Holt-Winters. These equations will 
differ depending on whether the dataset is Multiplicative or 
Additive. The basis of multiplicative equations are outlined 

due to the likelihood of Brick Fabrications time-series data 

displaying this trend.  Firstly, the time series (y
t) is represented as in equation 6, by decomposing the time-series 

into a permanent component (b
1), a trend component (b
2), a 
seasonal component (S
t) and a random error component (
t).       
 Smoothing constants 
 (the overall smoothing constant), 
 (the trend constant), and 
 (the seasonality constant) are then 
introduced to the model to obtain smoothing parameters for R
t (the overall smoothing parameter), G
t (the trend parameter) 
and St (the seasonal parameter) respectively. Equation 7 can 
subsequently be applied to calculate a forecast for the next 

time period.  
    




3. Datamining Results 
Two years
 worth of commercial sales data was available 
for this study. The format of the data was one in which rows 
signified individual product orders, whilst columns 

represented the attributes associated to each individual 

product order. The attributes were as follows: Unique 

transaction code, stock description, quantity of stock ordered, 
price, account name, and date.  
Additional attributes on each 
account such as region and annual turnover were also 
obtained through pre-processing and external search.  
3.1. Data Exploration Results 
Data exploration was conducted with three objectives: 
Discover the distribution of order frequency by accounts, 
determine the key products sold by the firm and identify key 

regions where revenue was generated in geographical terms. 
Firstly, a histogram was plotted based on the orders made 
per account to understand the purchasing habits of clients. 

Fig. 1 shows that the majority of account holders made 

between 1-50 orders in the two year period. It can also be seen 

that one firm made over 600 orders which represented 9.25% 
of the total revenue.  
Secondly, the top 10 products (ranked in terms of total 
revenue) were inspected. These made up 65.05% of total 
revenue. The most frequently purchased products were Flat 

Gauge Arch Panelites and Segmental Arch Panelites, which 

were the two highest revenue generating products for over 

90% of individual accounts. As seen in Fig. 2, the top 10 

highest revenue generating products are a range of prices, 

representing good portfolio diversity. The standard deviation 

also shows the variation in the average cost of a product. 
     
   
  Fig. 1. Frequency of orders per account. 









Fig. 2. Top 10 most frequently ordered products.
Lastly, using the lookup table each account was assigned to 
a region, and this was used to
 generate Table 1 giving the 
breakdown of revenue created by account holders per region. 
This showed South Central generating the most revenue per 

account, whilst Wales generating the highest share of revenue 

overall. 
Table 1. Breakdown of account holders per region and revenue. 


 
 
 
 
  
 3.2. Customer Segmentation Results 
The aim of this analysis was to apply data clustering using 
unsupervised learning techniques to discover which of Brick 
Fabrication customers were the most profitable and valuable. 

The unsupervised learning techniques used were k-means 

clustering, and Kohonens Self Organising Map.
 The first analysis that was ca
rried out was clustering using 
the k-means algorithm. To achieve this, each variable was 

scaled on a value of 0-100, where the maximum value of each 
Region 
Number % 
Revenue % 
East England 
11.45% 
9.03% 
Midlands 
21.37% 
17.76% 
North West 
19.08% 
13.29% 
South West 
13.74% 
13.70% 
Wales 
19.85% 
24.88% 
South Central 
8.40% 
13.05% 
Other 
6.11% 
8.29% 
127 Michael S Packianather et al.  /  Procedia CIRP   62  ( 2017 )  123  128 
variable was assigned a value of 100, and all others as a 
proportion of that. This meant each variable had equal 
weighting. K
-means clustering was subsequently carried out, 
calculating the similarity between items using the Euclidean 
distance based on three key variables, Unique Orders, 
Revenue
, and Quantity of Products Ordered. The k
-means 
algorithm was decided upon as a starting point due to its 
simplicity and the globular dense structure of the smaller 

items of data.
 A plot of the clusters on a Revenue by Account vs. 
Quantity Ordered by Account scatter plot is shown in
 Fig.
 3, whilst Table 2 displays finan
cial characteristics for each 
cluster group.
  








Fig. 3. Scatterplot of Revenue Vs. Quantity per Account.
 Table 2. Breakdown of revenue per cluster
. Cluster
 Size
 Mean 
No. of 

Orders
 Revenue 
Generated 

() 
 Revenue 
/ Order 

() 
 Revenue / 
Quantity 

() 
 1 4 342
 1,517,416
 1,109.2
 3.6
 2 21 124
 2,248,895
 866.6
 3.2
 3 7 236
 1,172,516
 710.2
 0.9
 4 106
 16 1,310,317
 793.7
 2.1
    The cluster
 groupings provided some interesting results. 
Firstly, the algorithm automatically grouped the top four 
accounts in terms of revenue to Cluster 1, indicating the 

results to be accurate. Analysis of these four accounts show 

they generated 25% of total revenue across the 2 year period 

and as such are likely to be of high importance to Brick 

Fabrication. The second point of interest was the relatively 

lower Revenue per Quantity value (0.90) attributed to 
Cluster 3. This suggests Cl
uster 3 made low cost orders
 (below 750) with a preference for low cost products. This 
was looked into deeper and 
showed Cluster 3 has similar 
spending habits to the other groups, however was much more 

likely to purchase low value products (below 0.50p) such as 

gables. Meanwhile, Cluster 4 purchased less high end 

products such as BCCH products than other groups. The 

growth of each cluster was then calculated. Interestingly, all 

groups saw strong growth except Cluster 1. This should be 

taken into account by Brick Fabrication
 for risk management. 
If this cluster group continues to decline, 25% of the 
companys revenue could be at stake.
 3.3. Kohonen Self
-Organising Maps
 Results Whilst the clustering techniques proved useful at grouping 
customers based on the three key variables, introducing 
multiple variables made the data more difficult to visualise.  

For this reason, it was decided to use Kohonens Self 

Organising Map to explore a much larger degree of account 

characteristics, possibly di
scovering further hidden 
relationships. For comparison with the k
-means analysis, the 
algorithm was driven by the same three variables as before: 
Unique Orders, Revenue
, and Quantity of Products Ordered 
(again, these variables were scaled in the same way as 
before). Growth, Company Size and Number of Different 

Products ordered were also added to the data set to be 

visualised on the grid. A fan diagram shown in Fig.
 4 was 
created, providing an instant and clear view of the distribution 
of each variable for the nodes within the grid. It can be seen 

that a dense
 cluster of nodes in the top right hand corner of the 
grid exists, similar to Cluster 1.
   
 
 
 
 
 
 
 
   
 
  Fig. 4. Kohonen Self Organising Map with 5x5 nodes shown in Fan 
diagram displaying breakdown of variables.
 3.4. Association Rules Results
 This section presents the results of association analysis, 
conducted using the arules package in R. There were 
approximately 10 top products which made up over half of all 

revenue. The top 5 products ranked by revenue were 

discovered to be:  Flat Gauge Arch Panelite, Segmental Arch 
Panelite, Mid
-Ridge BCCH, Frome Mid Ridge BCCH
, and 
Bespoke Mid Ridge BCCH.
 The first analysis that was run was standard market basket 
analysis on the transaction data.
 The first requirement was to 
pre-process the data, converting the raw transactions from a 
data frame format to a transaction format. To begin the 
process, minimum thresholds of 0.01 Support and 0.1 
Confidence were set and the arules algorithm was 

implemented generating 327 rules. The Support Threshold 

specified that the rule had to be apparent in at least 1% of 
transactions (71 unique orders), whilst the confidence 
threshold of 0.1 meant that when item A (LHS) was 

purchased, item B was also purchased at least 10% of the 
 Cluster 1  Cluster 2  Cluster 3  Cluster 4 128   Michael S Packianather et al.  /  Procedia CIRP   62  ( 2017 )  123  128 
time.  Rule pruning was then carried out using the ap
riori algorithm. This algorithm filtered out repeating rules (i.e. IF 
A 
 B prune B 
 A) and subset rules (i.e. IF A, B 
 C 
PRUNE A, C 
 B), reducing the number of rules to 66. 
Inspection of the pruned rules showed when two or more 
products were purchased in the same order, they are typically 
dominated by two products and their variations: AN3 Internal 

Angles and AN2 External Angles. This is visualised by the 

top 50 rules matrix shown in Fig.
 5. Analysis showed this 
combination had a 66% confidence rating and that this 
combination (including variations) appeared in 47.94% of all 

orders. Beyond this, Segmental Arch Panelites and Gables 

also featured highly in a typical basket. This is similar to the 

findings of the data exploration that showed all these
 to be highly popular and regularly purchased orders, so it is not 
surprising they have high levels
 of confidence to one another. 
  
 
 
 
 
 
 
  Fig. 5. Top 50 matrix of rules
. 3.5. Time Series Results
 The aim of conducting Time Series
 analyses was to 
provide an insight into what growth Brick Fabrication could 
expect to see in 2015. As only two years worth of sales data 

was available on Brick Fabrication, a regression model was 

created using Construction data 
from the Office of National 
Statistics as an independent variable.
  As a starting point, total monthly revenue was plotted for 
2013-14, and decomposed to display: observed, trend, 
seasonality and randomness as shown in Fig.
 6. The graphs 
show a high level of seasonality 
and a rising trend pattern between July 2013 and July 2014.
  
 
 
 
 
   
 
 
  Fig. 6. Decomposition of Time
-Series.
  Table 3
 displays standardised seasonal trend values for the 
calendar year. This information is of use to the BF
 as it can 
help display when demand will be highest and plan 
accordingly.
 To achieve standardised values, July was 
assigned a base value of 100 (as it had the highest 
seasonality), and February a value of 0 (due to the lowest 
seasonality). It can be seen from Table
 3 that the months of 
May, June and July are extremely busy, and as such resources 
should be managed effectively around this time. The second 
half of the year typically experiences a steep slow down, and 

this time should be used for expansion planning, as it will 

minimise the effect on production.
 Table 3. Monthly Indexed Demand
. Month
 Index 
Demand
 Month
 Index 
Demand
 Month
 Index 
Demand
 January
 80 May 99 September
 64 February
 0 June
 93 October
 60 March
 75 July
 100
 November
 44 April
 60 August
 62 December
 8 4. Conclusion
 and future work
 This paper has presented some datamining techniques
 namely data exploration, customer segmentation, Kohonen
s Self organising map, association rules, and time series
 to real 
data to extract knowledge and 
patterns for strategic decision 
making
 and forecasting. Future work would look into data 
classification of manufacturing time and costs required to 
manufacture each product
 in order to assign a profitability 
label to each account based on purchases.
 Acknowledgements
 The authors would like to thank 
Innovate UK,
 ASTUTE
 2020 project and CAMSAC
 for supporting this work. 
 References
 [1]
 Bokman A, Fiedler
 L, Perrey
 J, Pickersgill
 A. Using customer analytics to 
boost corporate performance. McKinsey & Company; 2014.
 [2] Pham DT, Packianather MS, Dimov S, Soroka AJ, Girard T, Bigot S, 
Salem
 Z. An application of data mining and machine learning techniques 
in the metal industry. In Proceedings of the 4th CIRP International 
Seminar on Intelligent Computation in Manufacturing Engineering 
(ICME
-04), Sorrento (Naples), Italy;2004.
 [3] Giudici P. Applied Data Mining. Wiley; 2003.
 [4] Tan PN, Steinbach M, Kumar V. Association analysis: basic concepts and 
algorithms. Introduction to data mining, 2005; 327
-414.
 [5] Kohonen T. Self
-organizing maps,
 Series in Information Sciences, 
Heidelberg: Springer, 3
rd Ed. 2001.
 [6] Bao
 F, dan Lobo V.
 Introduction to Kohonens Self
-Organizing Maps, 
Instituto Superior de Estatistica E Gestao de Informacao
 (ISEGI), 
Universidade Nova de Lisboa, Portugal; 2010.
 [7] Agrawal R, Imielinski T, Swami A. Mining association rules between sets 
of items in large databases.
 ACM SIGMOD Record. 1993; 22(2), 207
-216.
 [8] Esling
 P, Agon C
. Time
-series data mining. ACM Computing Surveys 
(CSUR). 2012; 45(1), 12.
 [9] Kalekar
 PS. (2004). Time
-Series Forecasting using Holt
-Winters 
Exponential Smoothing. Kanwal Rekhi School of Information 
Technology. 2004; 4329008, 1
-13.  
       Procedia Computer Science   70  ( 2015 )  586  592 
1877-0509  2015 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license 
(http://creativecommons.org/licenses/by-nc-nd/4.0/
).Peer-review under responsibility of the Organizing Committee of ICECCS 2015
doi: 10.1016/j.procs.2015.10.040 ScienceDirect
Available online at 
www.sciencedirect.com
4th International Conference on Eco-friendly Computing and Communication Systems, ICECCS 
2015 Efficient Data Mining Method to Predict the Risk of Heart Diseases 
through Frequent Itemsets Ilayaraja M*, Meyyappan T Department of Computer Science and Engineerring, Alagappa University, Karaikudi-630 003, Tamil Nadu, India 
Abstract Data mining techniques are used in the field of medicine for various purposes.  Mining association rule is one of the interesti
ng 
topics in data mining which is used to generate frequent items
ets.  It was first proposed for ma
rket basket analysis.  Research
ers 
proposed variations in techniques to generate frequent itemsets. Generating large number of frequent itemsets is a time 
consuming process.  In this paper, the authors devised a method to predict the risk level of the patients having heart disease 
through frequent itemsets.  The dataset of various heart disease patients are used for this research work.  Frequent itemsets a
re generated based on the chosen symptoms and minimum support v
alue.  The extracted frequent itemsets help the medical 
practitioner to make diagnostic decisions and determine the risk level of patients at an early stage.  The proposed method can 
be applied to any medical dataset to predict the risk factors with risk level of the patients based on chosen factors.  An experim
ental 
result shows that the developed method identifies the risk level of patients efficiently from frequent itemsets. 
 2015 The Authors. Published by Elsevier B.V. 
Peer-review under responsibility of 
the Organizing Committee of ICECCS 2015. 
Keywords: Frequent Itemsets; Heart Disease Prediction; Association Rule Mining; Data Mining; Medical Data Mining  
1.Introduction  
Data mining is now widely used in many domains.  It pl
ays an important role in the clin
ical field.  Day by day, 
large numbers of patients are visiting hosp
itals for the purpose of various treatments.  Number of patients records 
are increasing in every department in the hospital.  In medical field, data mining algorithms are used to mine the 
hidden knowledge in the dataset of the medical domain [1]
.  The discovered patterns may aid decision making and 
saving of lives.  Various data mining approaches such as cl
assification, clustering, associ
ation rule mining, statistical
 * Corresponding author. Tel.: +919994719037 ; 
E-mail address:ilayarajaalu@gmail.com  
 2015 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license 
(http://creativecommons.org/licenses/by-nc-nd/4.0/
).Peer-review under responsibility of the Organizing Committee of ICECCS 2015
587 Ilayaraja M. and Meyyappan T.   /  Procedia Computer Science   70  ( 2015 )  586  592 
learning and link mining, all have their significance in data
 research and development [2].  Association rule mining 
is a most efficient algorithm for extracting frequent itemsets from huge data. 
 To find out the frequent itemsets, 
minimum support value has been used.  Support value of the itemset greater than or equal to minimum support value 
is called frequent itemset.  If an itemset is frequent, then all of its subsets also must be frequent [3]. 
Heart disease is the one of the leadin
g human killer diseases.  In United S
tates, the cause of death for both men 
and women is primarily by heart disease.  It is an 
equal opportunity killer which 
claims approximately 1 million 
lives annually.  The disease had killed nearly 787,000 pe
ople alone in 2011 and 380,
000 people annually by heart disease.  Every 30 seconds someone has a heart attack and someone dies from a heart related disease in every 60 

seconds [4].   
In this paper, the authors proposed a 
new mining method to predi
ct the risk level of heart disease based on chosen 
symptoms by analyzing the heart disease dataset.  The predictions of this method will help the medical practitioners 
in making diagnostic decisions to save lives of patients at risk. 
2.Literature Review  
Usha Rani et al. has introduced pincer search algorithm 
to discover the maximum frequent itemset [5].  It also 
reduces number of times the database is scanned.  Fre
quent itemset mining without the generation of conditional 
frequent pattern tress was expressed by Meera Narvekar et al. [6]. The desired association rules are also discovered 
from the frequent itemset.  Alagugowri et al. developed 
a predicting system to predict the heart disease [7].  K-
Means clustering technique is used to 
distinguish the risky and non-risky factors to categorize.  Tzung-Pei Hong et 
al. developed MFFP-Tree Fuzzy Mining Algorithm to find out the linguistic frequent Itemsets [8].  Marghny et al. 

has developed a new method to mine frequent itemset 
by avoiding the costly candidate generation-and-test 
processing.  It also compresses essential information 
about all itemset, minimal and maximal length of frequent 
itemsets and database scans repeatedly [9].  Jahangir Kabir 
et al. proposed a novel method to determine maximal 
frequent itemsets with genetic algorithm 
[10].  The weighted support measure is
 introduced by Subrata Bose et al. 
that adopted a balanced approach to mine frequent pattern
s [11].  To mine frequent closed sequential pattern in 
temporal transaction data, Antonio Gomari
z et al. proposed a ClaSP algorithm 
[12].  To mine frequent itemset based 
on nodesets, an efficient FIN algorithm was developed by Zhi-Hong Deng et al. [13].  Hai Duong et al. developed a 

new algorithm with double constraint
s to find out all frequent itemsets [14].  Mengchi Liu et al. proposed a HUI-
Miner (High Utility Itemset Miner) algorithm to mine high utility itemset [15].  Umair Shafique et al. implemented 

three various algorithms (Neural Network, Decision Tree 
and Nave Bayes) to discov
er interesting patterns from 
heart patients data.  The results reveal that the Nave Bayes algorithm has the highest accuracy among them [16].  

Darshan M. Tank has proposed an algorithm to reduce pruning 
operations.  It uses aprior
i-gen operation to generate 
the candidate itemsets-2 and also it calculates support value qui
ckly by adopting the tag-counting method [3].  Deepa 
S. Deshpande proposed a novel method 
for mining association rule using patter 
generation.  To find out frequent 
feature set, the Boolean operations fo
r pattern generation is adopted [17].  Zhou Zhiping et al. introduced matrix-
based sorting index association rules algorithm to find th
e frequency k-itemsets directly.
  It discovers k-itemsets 
directly when frequent item sets are higher [18].  Chanchal Yadav et al. developed a new algorithm to decrease the 

pruning operation of candidate itemset.  It also reduces storage space requirement [2].  Amr Jadi et al. proposed an 

algorithm to predict and mitig
ate the risks by using runtime 
monitoring with neural ne
tworks [19].  Sallam Osman 
Fageeri et al. introduced a binary-based technique to find
 out the frequent itemsets that outperforms classic Apriori 
algorithm in terms of running time [20].  To estimate th
e size of candidate itemsets in Apriori based algorithms, 
linear algebra method was used by Savo Tomovi
 et al. [21].  Sen Su et al. has de
signed differentially private FIM 
algorithm to offer high time efficiency rath
er than achieve high data utility and 
degree of privacy [22].
  This survey 
indicates that many algorithms were
 developed by researchers to genera
te frequent itemsets.  A new method 
proposed in this paper generates frequent itemsets effi
ciently based on chosen symptoms and support value. 
3.Data Source  
Simulated heart disease dataset containing 1000 patient 
records are used for this research work.  This dataset 
contains 19 symptoms as shown in Table 1.  They are the symptoms of various heart diseases, namely 
588   Ilayaraja M. and Meyyappan T.   /  Procedia Computer Science   70  ( 2015 )  586  592 
Ather
o(VH
DSI234567894.Pr
oIn tpatienhospi
tdifficualgoritmethoAlso, 
items
ethe engener
acomp
aosclerotic Dise
aD) and Heart In
feTable Symptom 
ID Sym1. Che2. Sho3. Painyouthos4. Painor 
b5. Flut6. Rac7. Slow8. Lig9. Fain10. Br
eoposed Metho
dtoday's world, 
ants affected by htals, the medic
ault to extract th
ethms are devel
ood to predict t
hit discovers th
eets.  It removes 
ntire column (i
tation and impr
oarisons.  Figure ase (AD), Hea
rfections (HI) [2
41. Symptoms of V
amptom (Attribute) 
Nest pain (angina) ortness of breath n, numbness, weak
nur legs or arms if th
ese parts of your bo
dn in the neck, jaw, 
tback ttering in your che
scing heartbeat (tach
yw heartbeat (brady
chtheadedness 
nting (syncope) or 
neathlessness with e
xdology
a lot of people 
aheart diseases i
al records of p
ae useful inform
aoped to extract 
the patients und
ee risk level of 
the factors (ite
mtemset) is rem
ooves the effici
e1 shows the fl
ort Arrhythmias 
4].   
arious Heart Disea
sName 
ness or coldness in e blood vessels in 
dy are narrowed 
throat, upper abdomst ycardia) 
cardia) 
near fainting xertion or at res
t are frequently a
fs increasing.  
Hatients with va
ration from the 
vthe useful kno
wer risk based o
nthose patients. 
msets) that do 
noved from fur
tency of itemset
sow of the propo
sFig. 1. Block D(HA), Dilatedses 
Symptom 
ID 
11. 
12. 
13. 
men 14. 
15. 
16. 

17. 
18. 
19. 
 ffected by vari
oHeart disease i
srious diseases avast volume of 
rwledge from m
an the chosen s
y The propose
dnot satisfy the s
uther analysis.  
s generation.  
Ised method. 
iagram of the Prop
od Cardiomyopa
tSymptom (Attri
bSwelling of the aSwelling in your 
Fatigue 
Irregular heartbe
fluttering  Fever Swelling in your 

Changes in your 
Dry or persistentSkin rashes or un ous heart relate
ds one of the lea
are maintained 
records manual
lassive data.  In tymptoms by a
nd algorithm av
oupport value.  
RIt simplifies t
hIt saves execut
iosed Method. thy (DC), Val
vbute) Name 
ankles and feet legs 
ats that feel rapid, 
pabdomen 
heart rhythm 
t cough 
nusual spots 
d diseases.  Eve
rding dangerou
sin electronic 
mly.  Nowadays, sthis paper, the 
analyzing the h
eoids the generatRows (record) hhe collection o
fion time by av
ovular Heart Di
spounding or 
ry day, the cou
ns diseases.  In 
mmedium.  It is 
vseveral data mi
nauthors develop
eart disease dat
ation of unnece
shaving zero val
uf frequent ite
moiding unnecessease 
nt of most 
very ning 
ed a 
aset.  ssary 
ue in 
msets 
ssary 
589 Ilayaraja M. and Meyyappan T.   /  Procedia Computer Science   70  ( 2015 )  586  592 
4.1.Algorithm 
Input 
D - Heart Disease Dataset 
ms - Minimum support threshold value (0.1 - 1) 
mps - Minimum percentage of symptoms (0.1 - 1) 
s  Symptoms 

Output
Fk - Frequent Itemsets 
Method 
Step 1: 
ts  Total number of symptoms (s) 
k = ts Combine all the chosen symptoms s using logical AN
D operation.  Then finds a zero value in the combined 
columns and deletes that particular row. 
Find the sum value of this column.

Calculate the support value for column using the formula: 
cordser of Total Numbof ColumnSum Value SSupport
Re)(
  if S < ms then terminate the process 
elseCombined column consid
er as frequent itemset F
k.  Then perform logical AND with F
k and all other columns.  
Repeat steps 2 to 5 until dataset is null. 

Step 2: 

Find the sum values in each column of the table.
Step 3: 
Calculate the support value for each column using the formula: 
cordser of Total Numbof ColumnSum Value SSupport
Re)(
  Step 4: 

If S < ms then delete the column from the table. 
k = k+1       
Add the column in table to Frequent Itemset F
k Step 5: 
Combines the column Max(F
k) with all other attributes without repetitiv
e using logical AND operation.  Delete the 
row having zero value in the entire column. 
Step 6: 
Calculate the percentage of symptoms for 
each frequent itemsets using the formula: 
1...n)k 1...m,
(i                                                                                                                  
 
ikiiseaseptoms in Dber of Symer of  NumTotal NumbFappe
ar in  Symptoms DiseaseNumber of ms (PS) of SymptoPercentage If PSmps then all these frequent itemsets are indicate risk factors of patients having heart disease.
Convert the dataset into binary format denoting the presence or absence of symptom that causes heart disease as 1 
or 0 respectively.  Dataset with minimum support value, 
minimum percentage of symptoms and the symptoms are 
given as input to the proposed method.  In the first step
, combine all the chosen symptoms (columns) using logical 
AND operation.  Then find a zero value in the combined column
s and delete that particular row.  Find the sum value 
of the combined column and calculate the support value of
 this column using the formula mentioned in the 
590   Ilayaraja M. and Meyyappan T.   /  Procedia Computer Science   70  ( 2015 )  586  592 
algoritOther
wfrequ
eand cavalue less thunsati

all othproce
sthe itementi
ogreatepatien5.Re
sIn tby thepractitTheinclud(attribPerpropo
sTechngener
arequir
of ite
mmethoPloMajo
rflutterPlocomb
iin the 
affectenumb
eof sy
mthm. The proc
wise the combi
nent itemset F
k walculate the su
pof the each col
han the minim
usfying column. 
her columns usi
nss from steps 2 
temsets F
k.  In oned in the ste
per than or equal 
nts who will be 
asult and Discu
sthis paper, 
the ae user.  It helps 
tioners to predi
ce developed 
mdes data of 10
0butes) of the dat
arformance of t
hsed method.  T
anique [20], and 
ate all the possi
bement.  The pr
omsets are very 
mod.  The limitati
oot in Fig. 2 is d
rrity of the pati
ering symptoms 
fot in Fig. 3 sho
wination of chos
echosen dataset 
ed by chosen s
er of patients a
rmptoms like ch
eess has been 
tned column is 
cwith all other c
opport value for 
umn with user 
um support val
u In the step 5, 
ng logical AN
Dto 5 repeatedly 
step 6, calculap 6 of the alg
oto user given 
maffected by the 
hssions  
authors have de
vto identify the 
ct the risk level 
method is succe00 patients aff
easet are shown 
ihe proposed 
mable 2 shows th
eAssociation 
Rble itemsets in 
eoposed method 
much reduced i
non of the existi
nrawn to have s
tents are affect
efor the chosen 
dws the results f
oen symptoms a
nmay be includ
eymptoms but 
bre affected by 
aest pain, shortn
eFig. 2. Num
bterminated if t
hconsidered as fr
eolumns.  In the 
each column u
sgiven minimu
mue.  All the co
lfind the colu
mnD operation.  D
euntil dataset b
eate the percentaorithm.  Finall
yminimum perce
nheart disease w
iveloped a meth
opatients at risk 
of patients wh
ossfully imple
mected by heart 
in Table 1. 
method is com
pe comparison 
wRule Mining Aleach iteration, 
tdoes not gener
an the proposed 
ng techniques i
statistical informed by shortness 
dataset. or minimum su
pnd also affected 
ed in the comb
ibelow 60% of 
satleast 60% of thess of breath, fa
tber of patients affe
che support val
uequent itemset 
Fnext two steps
sing the formu
lm support valu
elumn are consi
dn which has th
eelete the row ha
vcomes null.  Fi
nage of sympto
my, it extracts all ntage of sympto
ith risk level. 
od to generate t
hfrom the extra
o will be affect
emented with Javrelated diseasepared with exiswith Apriori Al
glgorithm Basedthus increasing 
tate the unnece
smethod.  This 
is overcome by 
tmation on num
bof breath and 
pport value=0.1by atleast 60% ination.  The z
esymptoms of a 
he symptoms o
ftigue or faintin
gcted by different sy
mue has been l
eFk.  Then perfo
r, find the sum 
vla mentioned a
be.  Delete the c
odered as frequ
ee maximum su
mving zero value 
nally, it genera
tms for each frethe itemsets 
wms.  The extra
che frequent itemcted itemsets.  
ed by heart dise
ava programmin
gs with 19 clinisting methods 
gorithm [23], I
Md on Pattern G
ethe computatio
nssary itemsets i
nis the major re
sthe developed 
mber of patients 
athe lowest nu
m1.  It predicts t
hof specific symero value indica
tdisease.   Fro
mf Valvular Hea
rg.     
mptoms causing he
aess than mini
mrm the logical 
Avalues in each 
bove.  In step 
4olumn which h
aent itemset F
k+m value, combi
nin the entire c
otes the maximu
mequent itemset
swhich has perc
ected itemsets aremsets based on tThe findings 
wase.  
g language.  
Tical attributes. 
to establish th
eMSIA Algorithmeneration [17]. 
n time in comp
an each frequent 
search contribu
tmethod. 
affected by 19 
dmber of patien
the number of p
amptoms of a dis
etes the number 
m the plot, it i
srt Disease (VHDart disease 
mum support v
aAND opera
tion 
column of the 
t4, check the su
pas the support 
v1 after deletin
gnes the column 
olumn.  Perfor
mm possible leng
ts using the for
mentage of symp
te used to predi
cthe symptoms 
gwill help the me
dThe training da
 List of symp
te efficiency o
fm [18], Semi-A
p Existing met
harisons and me
mitemsets.  Nu
mtion in the prop
different sympt
ots are affected 
atients affected 
ease.  Any sym
pof patients wh
os evident that 
mD) and combin
a alue.  
with table 
pport 
value g the 
with 
m this 
th of 
mula 
toms 
ct the given dical ataset toms 
f the 
priori hods 
mory 
mbers osed oms.    
with by a 
ptom 
o are 
more ation 
591 Ilayaraja M. and Meyyappan T.   /  Procedia Computer Science   70  ( 2015 )  586  592 
 Tafrequ
frequ

not r
ewhenover
cassumpropoitem
sprevi
opatienFreqItemItemItemItemItemItemItemItemItem6.CoMefind fauthoaffectFig. able 2 shows th
eent itemset.  
Tent itemset wh
ielevant to parti
n the disease be
come in the pr
omed as frequen
tosed method en
sset.  Other itemous frequent it
ents.  In this wa
yTablequent msets 
ExistinAprioriAlgorit[23] 
mset-1 19 
mset-2 171 
mset-3 741 
mset-4 1218 
mset-5 922 
mset-6 386 
mset-7 77 
mset-8 6 
onclusion  
edical data mi
nfrequent items
eors proposed an 
ted by heart di
3. Analysis of Hea
re result with th
eThe drawback 
oich requires mo
cular combinat
ars more num
boposed method
.t itemse
t-3.  T
hsures that the s
ymsets which do emset are inclu
dy, the proposed 
me 2. Comparison of 
ng Methods 
i thm IMSIA 
Algorithm[15] 
19 
171 
741 
1218 

922 
386 
77 
6 ning plays a vit
aet from patient 
 efficient meth
osease.  The de
vrt Disease Dataset 
ae given support 
of these metho
dre storage spac
eion of symp
tomber of symptom
.  In this anal
yherefore, the fi
rymptoms of a c
hnot include th
ded in the next method finds fr
eProposed Method 
wm Semi-Aprior
iTechnique [18] 
19 
171 
741 
1218 

922 
386 
77 
6 al role in the d
idata to predic
od that finds fr
eveloped metho
dagainst combinatio
nvalue 0.1.  Exi
sds is two-fold. 
e to retain all t
hms used for an
as.  This is the mysis, a particul
arst two itemset
shosen combina
tem are ignore
dfrequent items
eequent itemset 
qwith Existing Methoi Association Rule Mining 
Algorithm 
Based on PatterGeneration [1419 
171 
741 
1218 

922 
386 
77 
6 agnosis of dise
t the symptom
equent itemset
sd analyses and 
ns of symptoms wit
hsting methods g
 Firstly, it genhe sets.  Secon
dalysis.  Numbe
rmajor drawbackar combination 
s are not gener
ation are includ
ed.  It also ens
uet provided tho
quickly compa
rods. Proposed Metrn 4] Based on the symptoms: 

Chest pain, Shortness of Breath, Fatigu- - 1 16 

12 
7 3 - ases and in lif
es causing dan
gs and risk level 
predicts the n
uh ms=0.1 and mps
=enerate all pos
snerates very la
rdly, it includes 
sr of itemsets i
nk of the existin
gcontains three 
ated in the pro
ped in all itemset 
ures that the symose symptoms a
red to existing 
mthod ue Based on the symptoms: 
Chest pain, Shortness of 
Breath, Fainting - - 1 15 

12 
7 3 - e saving decisio
gerous diseases.to predict the pumber of patie
 =0.6 
sible itemsets i
nrge itemsets in 
symptoms whi
cncreases inordi
ng methods, wh
isymptoms wh
iposed method. 
of the each fre
qmptoms includ
ffect majority omethods.  Based on the symptoms: 
Chest pain, LightheadedneFatigue - - 1 14 

12 
9 2 1 ns.  It is essen
t.  In this pape
rpatients who 
wnts at risk leven each each 
ch are nately ich is 
ich is  The 
quent 
ed in 
of the 
ess, tial to 
r, the 
will be 
el.  It 
592   Ilayaraja M. and Meyyappan T.   /  Procedia Computer Science   70  ( 2015 )  586  592 
provides a rapid aid to the medical practitioner in making em
ergency decisions to save the lives of patients at risk 
level. In the proposed method, symp
toms representing columns and patient 
records representing rows are removed 
from further analysis, if they do not satisfy the chosen rules.  The proposed 
method is applied over a heart disease 
dataset of 1000 records of patients suffering from various heart related diseases.  The prediction results are 
encouraging and the efficiency of the 
method in frequent itemset generation 
is better than existing methods.  
References 
1.Saurav Mallik, Anirban Mukhopadhyay, Ujjwal Maulik. RANWAR: Rank-Based Weighted Association Rule Mining from Gene Expression 
and Methylation Data. IEEE Transact
ions on NanoBioscience 2013; 14:59-66. 
2.Chanchal Yadav, Shuliang Wang, Manoj Kumar. An Approach to
 Improve Apriori Algorithm Based on Association rule Mining.  
International Conference on Computing, Communications and Networking Technologies (ICCCNT) 2013;1-9. 
3.Darshan M. Tank. Improved Apriori Algorithm for Mining Asso
ciation Rules. International Journal of Information Technology and
 Computer Science (IJITCS) 2014; 6:15-23. 
4.The heart foundation http://www.theheartfoundation.org/heart-disease-facts/heart-disease-statistics/ 
5.Usha Rani G, Vijaya Prakash R, Govardhan A. Mining Multilevel Association Rule Using Pincer Search Algorithm. International J
ournal of Scientific Research 2013; 2. 
6.Meera Narvekar, Shafaque Fatma Syed. An Optimized Algor
ithm for Association Rule Mining using FP Tree. International Conferen
ce on 
Advanced Computing Technologies and Applications 2015; 45:101-110.  
7.Alagugowri S, Christopher T. Enhanced Heart Disease Analysi
s and Prediction System [EHDAPS] Using Data Mining. International 
Journal 
of Emerging Trends in Science and Technology 2014; 1:1555-1560. 
8.Tzung-Pei Hong, Chun-Wei Lin, Tsung-Ching Lin. The MFFP-Tree Fuzzy Mining Algorithm to Discover Complete Linguistic Frequent 
Itemsets. International Journal of Computational Intelligence 2014; 30:145166. 
9.Marghny H, Mohamed, Mohammed M, Darw
ieesh. Efficient Mining Frequent Itemsets 
Algorithms. International Journal of Machine 
Learning and Cybernetics 2013; 5:823-833. 
10.Mir Md. Jahangir Kabir, Shuxiang Xu, Byeong Ho Kang, Zongyuan Zh
ao. A Novel Approach to Mining Maximal Frequent Itemsets Bas
ed 
on Genetic Algorithm. 9th International Conference on Information T
echnology and Applications (ICITA), at Sydney, Australia, 20
14. 
11.Subrata Bose and Subrata Datta. Frequent Pattern Generation in
 Association Rule Mining using Weighted Support. Third Interna
tional Conference on Computer, Communication, Control and Information Technology (C3IT) 2015; 1-5. 
12.Antonio Gomariz, Manuel Campos, Roque Marin, Bart Goethals. Cl
aSP: An Efficient Algorithm for Mining Frequent Closed Sequenc
es.
 Advances in Knowledge Discovery and Data Mining 2013; 7818:50-61. 
13.Zhi-Hong Deng, Sheng-Long Lv. Fast Mining Frequent Itemsets using Nodesets. Expert Systems with Applications 2014; 41:4505-4
512. 
14.Hai Duong, Tin Truong, Bay Vo. An Efficien
t Method for Mining Frequent Itemsets with Double Constraints. Engineering Applica
tions of 
Artificial Intelligence 2014; 27:148-154. 
15.Mengchi Liu, Junfeng Qu. Mining High Utility Itemsets without Candidate Generation. Proceedings of the 21st ACM International 
Conference on Information and Knowledge Management 2012; 55-64. 
16.Umair Shafique, Fiaz Majeed, Haseeb Qaiser, Irfan Ul Mustafa.
 Data Mining in Healthcare for Heart Diseases. International Jo
urnal of 
Innovation and Applied Studies 2015; 10:1312-1322. 
17.Deepa S. Deshpande. A Novel Approach for Association Rule Mi
ning using Pattern Generation. International Journal of Informat
ion
 Technology and Computer Science(IJITCS) 2014; 6:59-65. 
18.Zhou Zhiping, Wang Jiefeng. An Improved Matrix Sorting 
Index Association Rule Data Mining Algorithm. 33rd Chinese Control Co
nference 
Proceedings, China 2014; 28-30. 
19.Amr Jadi, Hussein Zedan, Turki Alghamdi. Risk Management ba
sed Early Warning System for Healthcare Industry. International C
onference 
on Computer Medical Applications (ICCMA) 2013; 1-6. 
20.Sallam Osman Fageeri, Rohiza Ahmad, Baharu
m B. Baharudin. A Semi-Apriori Algorith
m for Discovering the Frequent Itemsets. 
International Conference on Computer and 
Information Sciences (ICCOINS) 2014; 1-5. 
21.Savo Tomovi
, Predrag Stanii
. Upper Bounds on the Number of Candidate Itemsets in Apriori Like Algorithms. 3rd Mediterranean 
Conference on Embedded Computing (MECO) 2014; 260-263. 
22.Sen Su, Shengzhi Xu, Xiang Cheng, Zhengyi Li, Fangchun Yang. 
Differentially Private Frequent Itemset Mining via Transaction 
Splitting. 
IEEE Transactions on Knowledge and 
Data Engineering 2015; 27:1875-1891. 
23.Ilayaraja M, Meyyappan T. Mining Medical Data to Identify Frequent Diseases using Apriori Algorithm. Proceedings of the Inte
rnational 
Conference on Pattern Recognition, Informatics and Mobile Engineering 2013; 194199. 
24.http://www.mayoclinic.org/diseases-conditions/heart-disease/basics/symptoms/con-20034056. 
English Language Teaching; Vol. 6, No. 2; 2013 
ISSN 1916-4742   E-ISSN 1916-4750 
Published by Canadian Center
 of Science and Education 
113 
 Why Use Music in English Language Learning? A Survey of the 
Literature Dwayne Engh
1 1 English Teacher, Tudor BEC College London, U.K. 
(Received Masters in English Language Teaching and 
Applied Linguistics, King
s College London, 2010) 
Correspondece: Dwayne Engh, St. Peters Vicarage, 
53 Belsize Square, London, U.K. NW3 4HY. E-mail: 
dwayneengh@shaw.ca 
 
Received: December 9, 2012   Accepted: December 
22, 2012   Online Pub
lished: January 9, 2013 
doi:10.5539/elt.v6n2p113   URL: http://dx.doi.org/10.5539/elt.v6n2p113 
 
Abstract 

The use of music and song in the 
English language-learning 
classroom is not new.
 While many teachers 
intuitively feel that music is beneficial in teaching English language, there is sometimes a lack of the theoretical 

underpinnings that support such a choice. There are exam
ples in the literature to argue the strong relationship 
between music and language that are su
bstantiated by research in the fields of cognitive science, anthropology, 
sociolinguistics, psycholinguistics, First Language Ac
quisition (FLA) and Second Language Acquisition (SLA). 
Keywords:
 music, song, English language learning 
1. Introduction 

The use of music and song in the English language-learning classroom is not new. As early as Bartle (1962), 
Richards (1969) or Jolly (1975), scho
lars have been arguing for use of music in a language acquisition context 
for both its linguistic benefits and for the motivati
onal interest it generates in language learners.  
There are examples in the literature to argue the st
rong relationship between music and language that are 
substantiated by research in the fiel
ds of cognitive science, 
anthropology, sociolinguis
tics, psycholinguistics, 
First Language Acquisition (FLA) and Second Language 
Acquisition (SLA). Music had been used on occasion 
with the Audiolingual 
Method in language teaching classrooms to
 reduce the boredom that could occur from 
repetitive drills from the 1950s through to the 1970s (Bartl
e, 1962; Kanel, 2000) and later, the use of classical 
instrumental music was used with the goal to produce 
a relaxed state of mind that makes the brain receptive to 
inputs and activates the subconscious in Suggestopedia methodology (Lozanov, 1978; see also Bancroft, 1978). 

However, it may not have been until Communicative La
nguage Teaching (CLT) and Task Based Learning (TBL) 
approaches became more pervasive that there was a sudd
en demand for pedagogical material for the use of songs 
in the language-learning classroom (Griffee, 2010).  
1.1 Why is it Important? 

There were two important outcomes from the author's recent
 research (2010) into effective use of music in the 
English language classroom (in press). One suggested that
 there was strong support for use of music in the 
language-learning classroom, but that there was actually very little occurring in most classrooms. Connected, but 

a separate issue, implied that while many teachers intu
itively felt music was beneficial in teaching English 
language, there was also the perception that there was a lack of understanding of the theoretical underpinnings 

that supported such a choice. Theref
ore, some educators felt unable to defend the decision to champion use of 
music in the classroom to administrators, business English students or those in a predominantly exam focused 
environment.  
Salcedo (2010), after a survey of foreign language teach
ing journals, suggests there are only a few articles on 
the subject compared to multitudinous articles on other me
thodological ideas. Other scholars have noted this as 
well: Coe (1972) stated there have been no controlled 
music use in the language classroom experiments and 
Griffee (1989), in an editor
ial introduction discussing why songs and music arent used more extensively in the 
language classroom, suggests there exists a lack of theo
retical perspective and empiri
cally based research in the 
field. I would propose that, while there has been some progress in this subfield, little has changed throughout the 
past decades.  
www.ccsenet.org/elt English Language Teaching Vol. 6, No. 2; 2013 
114 
 1.2 Subsections of Literature Review 
In this literature review, I will exam
ine academic perspectives from within the field of applied linguistics and 
will make connections to the field from other disciplines that argue there is a firm empirical, theoretical and 

pedagogical basis to consider for the use of music as an aid in language acquisition. The examination of the 
literature will consider the following five categories:  
1. Sociological Considerations 
2. Cognitive Science 
3. First Language Acquisition 
4. Second Language Acquisition 
5. Practical Pedagogical Resources 
2. Sociological Considerations 
There are four distinct sections in 
the exploration of Sociological Considerations that will be discussed. Those 
include the areas of Anthropological Arguments, Community, Breaking Boundaries and Culture. 
2.1 Anthropological Arguments 

The connection of song to both the development of human languages and the linguistic development of the 
individual are anthropologically worth noting (Murphey, 1990). Livingstone (1973), in a contentious article that 

made extrapolations from physical anthropological research of birdsong and mammal behaviour, hypothesised 

that humans evolved song before speech. While this can never conclusively be proven (see Count, Hewes, 

Livingstone & Mourant, 1974 for counter-argument), it raises interesting connections to infant development and 

first language acquisition research that will be discusse
d later in this review. From a social anthropological 
perspective, Merriam (1964) has argued that the survival 
of literature, epic poetry and ballads in oral traditions 
must be credited to the use of song. The odes of praise and stories of the tribe were passed on through song so 

that the texts would not be forgotten before the written word developed. Rubin (1995) suggests this may be 

because of the multilayered patterns of rhythm, sound, 
linguistic meaning and emo
tional content functioning 
simultaneously. Lastly, drawing on anthropological and ethnomusicological literature, Trehub and Trainor (1998) 

examine the historical and cultural functions of songs in 
child care, claiming the de
scriptive evidence seems to 
suggest that the practice of singing to infants and many
 details of song form and style are rooted in ancient 
traditions that have survived industrialization and urbanization (43). It does appear the anthropological 
development of human languages is, to some extent, connected to music and song. 
2.2 Community 
In addition to being utilized by caregivers and individu
al storytellers in oral traditions, singing has also 
historically evolved as a collective activity. It is communally practiced by people not requiring professional 

training, with a wide range of experience levels, and in a variety of settings including leisure, religious or 

educational communities. It is argued that use of song in
 the language classroom enhances social harmony (Huy 
Le, 1999), creates a safe space to experience learning 
collectively and contributes to the building of a community 
(Lems, 1996; Lake, 2003); all of which 
are essential factors for 
effectively attaining teach
ing and learning goals. 
Specifically examining trust and cooperation, Anshel and Kipper (1988) suggest that group participation in 

singing positively influences both trust and cooperation, 
which are primary contributors to the progression of 
group cohesion and a sense of community (see McMillan & 
Chavis, 1986 for further discussion of theories of 
community). Gao (2008), while discussing use of karaoke and singing contests as evidence of learner 

collaboration in English Corners in China, approaches the humanistic learning that can occur within this type of 

community as following traditional Chinese cultural values and Confucian pedagogical traditions (see also 

Nguyen, Terlouw & Pilot, 2006). The literature appears to
 suggest that the specific feeling of belonging to a 
cohesive community, that many teachers explicitly strive to
 establish for their learners
 in the classroom, can be 
promoted by use of musi
c and collective singing. 
2.3 Breaking Boundaries 
Music consistently surrounds our lives and may potentially 
assist educators in removi
ng boundaries between the 
various communities that students belong to (Nagy & Herman, 1987). Extending their work, Medina (1990) 

suggests it may be beneficial to attempt methods that more closely resemble life outside of the institutional 

classroom, which is generally filled with music, songs, 
stories and visual images. B
ecause students learn a great 
deal of language before school age and outside of the institution once school has begun, it is potentially 
justifiable to consider non-traditional teaching methods that are less structured and ritualised.  
www.ccsenet.org/elt English Language Teaching Vol. 6, No. 2; 2013 
115 
 Cheung (2001) prefers th
e image of music bridging gaps between for
mal and informal learning as opposed to 
breaking boundaries and Huy Le (1999) extends this bridging of formal and informal learning environments to 

include the bridging of the gap between teachers and st
udents as well. By breaki
ng down the boundaries or 
bridging the gaps between stereotypical institutional learning and the informal learning of our homes, work 

places and school grounds, we can contin
ue to engage students in natural a
nd authentic language that they will 
want to practice independently out of class (Jenkins & Dixon, 1983; Harwood, 1998). In a study of young 

learners engagement in music beyond the institutional cl
assroom, Campbell (1998) discusses how the function 
of music in learners lives, the use of music in play an
d the manner of musical encultu
ration relate to the place of 
music in institutional educational settings, suggesting more music should occur in educational institutions  

supporting Murpheys (1992a) argument that the school environment is the only place in society not using music 
and song to its full potential.  
2.4 Culture 
Candlin, in the preface as series editor that 
Songs in Action
 (Griffee, 1992) was a part of, argues for the 
importance songs may provide as an historical and social cultural context to the language being acquired. He 
states: To be sure, songs have a place in the classroom for helping create that friendly and co-operative atmosphere so 
important for language learning, but they can offer much more. They offer insights into the culture and 

especially the stories and myths of different societies, providing a window into the frames of reference and values 
of the peoples whose language we are learning. (Candlin, 1992: ix) 
There are two areas to consider under 
the heading of culture. First, that
 Pop Culture is a global phenomenon 
orientated toward a common youth culture with pop 
songs as its backbone (Gri
ffee, 1992) and hence is 
something that is both significant and familiar to the students (Dubin, 1975). Therefore, use of music from Pop 

Culture allows a window to the target language of the
ir culture (Cheung, 2001; Plagwitz, 2006) and not only 
works to bridge the gap between generations, between pre-defined teacher-student roles and between the 

stereotypical formal and informal lear
ning environments, but also validates and empowers their music, their 
language and their culture (Hamblin, 1987; Domoney & Harris, 1993).  
Secondly, songs are a potentially culturally rich resource 
for the language classroom (Murphey, 1992b; Ajibade 
& Ndububa, 2008) that can 
explicitly be used to teach the cultural 
norms and ideals of the target language 
(Gravenall, 1945; Jolly, 1975). Music, while universal, is 
culturally specific in that th
e musical content and style 
mirror a particular culture, acting as a 
cultural artefact that may both reflect 
and influence that culture (Griffee, 
1992; Failoni, 1993; Mishan, 2005). The introduction of vocal music as the foundation on which to either build 

a coordinated language-culture course or, more often, supplement an already established program, can be a 

powerful tool, especially when the music is a faithful 
reflection of the culture from which it derives (Jones, 
2008: 10). While a number of academics agree and appear 
to support the pedagogical use of music to increase 
cultural competency in the culture of the target language (i.e. Saricoban & Metin, 2000; Orlova, 2003), there are 

other linguists such as Huy Le (1999) who argues that music, particularly songs, is an encoding of cultural 

values and ideology which needs to be viewed critica
lly if foreign music is used in teaching a second 
language The use of music in the classroom raises significant questions concerning how culture, language 

and identity are related, and therefore demands critical refl
ective thought, if not a level of caution regarding the 
potential linguistic and cultural imperialism that could 
occur (see Pennycook 2003, 2007 for further discussion 
of transcultural flows and performativity with relationship to culture and identity).   
We now move from the discussion of Sociological Considerations into the exploration of the cognitive and 
neural relationship of language and music. 
3. Cognitive Science 
Cognitive research investigates the anatomic structure of the brain and its neural functions, suggesting that 
language and music have im
portant points of convergence and/or overl
ap. There are multiple recent studies in 
this field, likely somewhat due to the increase in soph
istication of neuroscience technology as well as increased 
interest from within the field in the neural and functional structure of both language and music domains (for 

developmental comparison, see McMullen & Saffran, 2004). Additionally, the popularisation of some studies 

such as the Mozart Effect (Rauscher, Shaw & Ky 1993, 1995; Hetland, 2000) by Campbell (1997) has brought 

increased attention from the general public to the field of
 cognitive science. The original Rauscher et al. (1993, 
1995) study extended the work of French therapist Tomatis (Thompson & Andrews, 2000) and indicated an 

enhancement of spatial-temporal abilities after listening 
to music composed by Mozart, an assertion that has 
been disputed by Chabris (1999), Nantais and Schellenberg (1999), Steele, Bass and Crook (1999) and Steele et 
www.ccsenet.org/elt English Language Teaching Vol. 6, No. 2; 2013 
116 
 al. (1999). 
Cognitive science research agrees that there are 
important connections between music and language: 
Like language, music is a human universal involving perceptu
ally discrete elements organized into hierarchically 
structured sequences. Music and language can thus serve as foils for each other in the study of brain 

mechanisms underlying complex sound processing, and comparative research can provide novel insights into the 
functional and neural architecture of both domains. (Patel, 2003: 674) 
There is, however, an interesting contradiction noted in
 the literature debating how that occurs. The evidence 
from neuropsychology argues that musical and linguistic elements can be dissociated, and therefore may work 

together as discrete domains. Borchgrevink (1982) postu
lated that linguistic and musi
cal elements are processed 
in different hemispheres of the brain and therefore language and music used concurrently provides effective 

pedagogical methodology to increase learning, while Jackendoff (1991, 2006) describes how music parallels the 

perception of language and suggests a parallel architecture approach. It is argued that language and music 

collaborate but operate in different domains, so that f
rom a conceptual standpoint, music and language must be 
dissociated, or else there would be no possibility for cooperation (Steinke, Cuddy & Holden, 1997: 316). A 

specific study of song lyrics and melody processing sugges
t that the monitoring of speech and music in songs is 
performed by independent neural processors (Peretz, 2002: 174) and a research study of how phonemes and 

pitch are controlled in different but collaborating areas 
corroborate those findings 
that the two aspects of 
language are handled separately, yet in harmony by a musical-linguistic collaboration (Zatorre, Meyer, Gjedde 
& Evans, 1996: 848). 
Yet these conclusions stand in direct contrast to recent neuroimaging data, which indicates a convergence in the 
processing of the syntactic relations in language and musi
c (Patel, 2003). This suggests that musical structure is 
processed in language areas of the brain (Patel, Edward, Ratner, Besson & Holcomb, 1998; Koelsch, Gunter & 

Friederici, 2000; Koelsch et al., 2002
), specifically localized in Broc
as area and its right-hemisphere 
homologue, in areas involved in syntactic analysis during auditory language comprehension indicating that 

these regions process syntactic information that is less
 language-specific than previously believed (Maess, 
Koelsch, Gunter & Friederici, 2001: 5
40; also see Levitin & Menon, 2003). Hence, because they are processed 
in the same region, our brains may r
ecognise aspects of linguistic and musi
cal sequences in a similar manner.  
Patel (2003) and Slevc, Rosenberg and Patel (2008) suggest a possible resolution to this contradiction in the 
literature with the use of linguistic and musical syntactic processing cognitive theories, postulating a specific 

point of convergence between these theories leads to th
e hypothesis that syntax in
 language and music share a 
common set of processes (instantiated in front brain areas) that operate on different structural representations (in 
posterior brain areas)
 (Patel, 2003: 674). 
As we move from the area of Cognitive Science to that of
 First Language Acquisition, Gomez and Gerken (2000) 
act as a transition in their literature review of cogn
itive research discussing the possible contributions and 
correlations of the language acquisition device (LAD
) and associative learning functions through the 
methodology of testing artificial language learning and 
language acquisition in infa
nts. The studies reviewed 
raise significant questions with respect 
to whether such neural functions are general or specific to language. The 
fine-grained characterizations of infant learning mechanisms that this approach permits should result in a better 

understanding of the relative contributions of, and the dynamic between, innate and learned factors in language 
acquisition (
ibid
: 178). 
4. First Language Acquisition  
The FLA field offers a number of insights that corroborate the arguments of Cognitive Neuroscience for the 
innate connection of language and music in human beings. Loewy (1995) postulates that music acts as the 

prelinguistic carrier for communicative intent, essentially 
that a foundation for peer social interaction is built 
through the infant's preverbal communication of crying. Other scholars, such as Chen-Hafteck (1997), suggest 

that the prelinguistic phase discussed in FLA literature is similar to the premusical phase discussed by 

musicologists, which echoes arguments that song-like 
vocalisation will commonly precede language in regular 
ontogenetic development (Murphey, 1989). 
Music and language are the two ways that humans communicate and express themselves through sound. Since 
birth, babies start to listen and produce sound without distinguishing between music and language, singing and 
speech. (Chen-Hafteck, 1997: 85) 
The intentional experimentation with linguistic production occurs through prosodic elements of speech and the 
vocal contour, including manipulating tone, timbre, stress and rhythm in the infants indistinct babbling 
www.ccsenet.org/elt English Language Teaching Vol. 6, No. 2; 2013 
117 
 vocalisation that precedes specific linguistic and musical
 production. "This music of speech is the earliest 
dimension of language that is used and understood by children (Loewy, 1995: 61). Papousek and Papousek 

(1981, 1991) support Loewys proposition and argue ther
e is very little difference between spoken intonational 
contour and sung melodic contour before the first lexical items are acquired by infants, which is corroborated by 

empirical research (Trehub, Trainor 
& Unyk, 1993) that tracked language development in infants by analysing 
pitch contours in linguistic and musical material.  
It is argued the input infants receive in affective baby-
talk or infant-directed speec
h prosody (Trainor, Austin & 
Desjardins, 2000), as well as the songs mothers and ot
her caregivers sing (Bergeson & Trehub, 2002), has an 
impact of linguistic and musical development. Nursery rh
ymes and lullabies are some of the first language input 
that occurs with enough repetition to encourage memo
rization and therefore acquis
ition (Howle, 1989). Murphey 
and Alber (1985) note that as children grow up, they re
ceive less and less of the affective musical motherese 
language that caregivers use with in
fants and postulate it is replaced by pop music as the motherese of 
adolescents, as well as suggest connections between the 
motherese of adolescents and the benefits of affective 
foreigner talk (see also Murphey, 1992a). This affective 
foreigner talk is observed in language teachers as a 
simplified code of comprehensible input at the appropriate language level in the language-learning classroom, 

noting that the use of modified intonation is often in
stinctive from both language teachers and caregivers, who 
are often unaware that they are singing with an exaggerated pronunciation and hyperbolic melodic contour at 

that moment (Fonseca Mora, 2000). It 
appears that melodic musicality of sp
eech is not only significant to FLA, 
but to the entire language acquisition process and connections can be drawn between first language and second 
language acquisition literature as we move into the next section. 
5. Second Language Acquisition  
For the purposes of this literature review, the extensive 
field of SLA research as it applies to use of music in 
language-learning will be discussed under the headings: Af
fective Filters, Motivation, Learning Strategies and 
Empirical SLA Studies. 
5.1 Affective Filters 
The affective filter hypothesis (Krashen
, 1982) argues that the most favourable learning occurs in a setting of 
low anxiety, self-confidence and high motivation. Built on
 SLA research, the hypothesis states acquirers with a 
low affective filter seek and receive mo
re input, interact with
 confidence, and are more
 receptive to the input 
they receive (Richards & Rodgers, 2008: 183). An explor
ation of the literature rega
rding how affective filters 
may relate to the use of music and song in the language
 classroom (Merriam, 1964; 
Coe, 1972; Claerr & Gargan, 
1984; Wilcox, 1995) suggests that music lowers affective 
barriers and assists in making students more relaxed, 
thereby more receptive to language learning. Based on
 corpus analysis, pop songs are conversation-like, 
repetitive and occur at roughly half the speed of spoken 
discourse (Murphey, 1992b), 
therefore also lowering 
affective barriers. This simplicity, their highly affective 
and dialogic features, and their vague references (ghost 
discourse), allow listeners to use th
em in personally associative ways (Murphey, 1992b: 771). Connecting this 
argument to earlier motherese of adolescents discussions
, it is postulated pop songs create a low affective filter 
and Murphey characterises them as a teddy-bear-in-the-e
ar because of their riskless nature. Additional research 
examining differences in affective filter, stress levels 
and language acquisition (Ganschow et al, 1994) implies 
the anomie language-learners feel, which may include a sense of homelessness, social uncertainty and 
dissatisfaction, can be addressed by use of music in the classroom (Schoepp, 2001; Lake, 2003).  
5.2 Motivation 

Motivation is multifaceted and concerns 
both the affective states and attitudes 
that impact the amount of effort a 
learner expends to acquire a new language. The degrees of the various types of instrumental, integrative, 

resultative and intrinsic motivation (Ellis, 1997) differ with
 the individual student and the literature supports the 
use of song and linguistic play in the language classroom (Cook, 1997) to increase individual language-learner 
motivation (see also Ndububa & Ajibade, 2006; King, 2010).  
The validity of using authentic and natural language input including real-world/target tasks, pedagogical tasks 
and enabling skills (Nunan, 2004: 19) so that there is some sort of relationship to comparable real-world 

activities (Skehan, 1998: 95) has been noted as a motiva
ting factor in the language classroom (see also Nunan, 
1989; Bell & Gower, 1998; McGrath, 2002). Mishan (2005) contends that because music is an authentic activity 

that occurs among first language users both in terms of discussion of popular music as well as in the group 

singing that occurs at many public events ranging from karaoke to football, it also proves motivating to use for 

language-learners in a classroom setting (see also Cook, 
1997). The arguments that the rhythmic patterns of Jazz 
Chant are fragments of authentic language use (Graham,
 1992); that popular rock music provides meaningful 
www.ccsenet.org/elt English Language Teaching Vol. 6, No. 2; 2013 
118 
 authentic activity with frequent integrated exposure (D
omoney & Harris, 1993); that rock songs are short, 
accessible authentic texts rich in conten
t that work as a catalyst to engage with and build meaning from (Pope, 
1995); that the poetry of rock music provides a valid auth
entic text (Abrate, 1983; Gr
iffee, 1992); and that this 
rock poetry may be considered an example of inclus
ive literature (Ferradas Moi, 1994, 2003), all provide 
further support to the motivating factor of music in the language classroom. 
5.3 Learning Strategies 
It has been suggested that motivation may have connections to other individual learner differences in second 
language acquisition such as learner strategies, learner autonomy and preferred learning styles (Sinclair, 1996; 

Cohen, 1998; Benson, 2006). Separating potential interconnectivity of the variables between motivations, 

individual learning preferences due to multiple intelligence
s and learner strategies (Carrell, Pharis & Liberto, 
1989; Brown, 2007) therefore becomes an issue. There is 
also a strong possibility that the influence of learning 
environments, which may include family, educational and cultural background factors might have an effect on 

the learner (Locastro, 1994; Harley, 2000). Gardner (1999) acknowledges Locastros point when considering the 

impact of personal decisions in the expression of multiple intelligences (Chen & Gardner, 2005) and defines 

intelligence as a biopsychological potential to process in
formation that can be activated in a cultural setting to 
solve problems or cr
eate products that are of value in a cultu
re (Gardner, 1999: 33
). Fonseca Mora (2000) 
applies Gardners multiple intelligences
 hypothesis specifically to langua
ge teaching (see also Berman, 1998) 
and asserts one of the main implications for teaching of 
this theory was that students should not only be taught 
to increase their verbal, spatial, and numerical intelligences, but also to nurture their musical, bodily-kinaesthetic, 
interpersonal, and intrapersonal intelligences (146).  
Although there are, admittedly, some ch
allenges with the learner strategy literature in general, there are a number 
of researchers who remain enthusiastic about the value 
of language learner strategies (Oxford, 1990; Naiman, 
Frohlich, Stern & Todesco, 1996; Chamot, 2005), suggesting that a more genuine interaction occurs (Lam & 

Wong, 2000). When music is situated as an aid to learning strategies in the language classroom, both cognitive 

and metacognitive strategies are enhanced (Jones, 2008), 
affective exploration is in
creased (Cullen, 1998) and 
the student is more receptiv
e to language inputs (Bancroft, 1978; Lo
zanov, 1978; Magaha
y-Johnson, 1984).  
This leads us now to specific empirical research regard
ing the use of music and its effect on second language 
acquisition. 
5.4 Empirical SLA Studies 
There is ample empirical evidence to
 suggest that use of music and song
 enhances students success in second 
language acquisition, which will be discussed in three 
sections: Recall and Memory, 
The Din/Involuntary Mental 
Rehearsal and Language Specific Skills. Medina claims the conclusions for the use of music in the second 

language classroom are clear. Since music can be as viab
le a vehicle for second language acquisition as stories, 
then songs should not be treated as 
extra-curricular en
tities (1990: 18).  
5.4.1 Recall and Memory 
Empirical research comparing conventional pedagogical methods with those that use music and song has 
produced positive results in the area of vocabulary recall.
 For example, studies with 
primary students by Medina 
(1990) investigated effectiveness of
 vocabulary acquisition with the use of music and story illustrations, and a 
study by Schunk (1999) examined the effect of signing when coupled with singing on receptive vocabulary 
skills. 
Based on research using background music to aid verbal
 phrase recall, Fonseca Mora (2000) asserts that songs 
have a positive outcome on the students language acquisition and that lexical patterns stored in long-term 

musical memory can be retrieved with ease at a later da
te for mental rehearsal, memorisation or during oral 
interaction. Wilcox (1995) studied the pronunciation of ta
rget vocabulary in adult learners through use of music 
cues to aid prosodic memory, which is confirmed by re
search into vocabulary reca
ll attached to visual or 
auditory cues (Brown & Perry, 1991), and research into music-dependent memory using background musical 

cues with specific target vocabulary (Balch, Bowman 
& Mohler, 1992). Brutten, Angelis and Perkins (1985) 
explored this research further by te
sting oral proficiency using musical ab
ility and memory in English language 
learners and suggested that innate musical abilities and verbal memory may have accounted for score variances.  
Pop songs are useful for not only discrete lexical voca
bulary item recall, but also for longer utterances and 
formulaic phrases. For example, Wray and Perkins (2000) 
have suggested that most speech is the repetition and 
variation of memorized formulas, and 
that these formulaic lexical phrases 
are flexible and therefore allow for 
many repetitions. Distinctive intonation, rhythmic and 
stress patterns can accompany each formulaic unit when 
www.ccsenet.org/elt English Language Teaching Vol. 6, No. 2; 2013 
119 
 incorporating the multiple formulaic lexical phrases found in songs, making it easier for the learner to remember 
and apply.  
The use of rhythm and rhyme to assist auditory recall has also been studied, and the multimodal combination of 
rhythm, melody and rhyme along with linguistic prosody appears to lead to greater retention (Graham, 1992; 

Palmer & Kelly, 1992). This has been applied to languag
e learning in a study of rhyming ability of preschool 
children as a phonological skill (Bryant, MacLean, Bradle
y & Crossland, 1990) and applied to English verse 
(Kelly & Rubin, 1988), where connections are sugges
ted between childrens verse and acquisition of prosodic 
rules (see also Krumhansl, 2000 for further psycholinguistic research in this area).  
5.4.2 The Din / Involuntary Mental Rehearsal 
Murphey (1989) provides 
potential evidence rega
rding why music effectively assist
s in lexical and phrasal recall 
in noting the resemblance of songs to conversational discou
rse and suggests they are linguistically processed in a 
similar manner. Thus, song may be called pseudo-dialogal or a form of Piagets (1923) egocentric language, a 

form of egocentric language that at first may be pronounced for us but is soon appropriated by us, 

psychologically and subvocally if not actually sung aloud (Murphey, 1989: 168). He subsequently goes on to 

hypothesize a relationship between Piagets (1923) egocentric speech, Krashens (1983) involuntary verbal 

rehearsal, Vygotskys (1934) inner speech and postul
ates the song-stuck-in-m
y-head (SSIMH) phenomena. 
The similarity of the Din to the SSIMH phenomenon is suggested It is hypothesized that song may act as a 

LAD activator, or be a strategy of the LAD in the onto
genetic development of language (Murphey, 1990: 53). 
The argument for subvocal involuntary mental rehearsal is
 corroborated as potential evidence why songs in the 
language classroom have such a positive effect on dela
yed vocabulary, formulaic lexical phrases and extended 
text recall (Fonseca Mora, 2000; Salcedo, 2010). 5.4.3 Language Specific Skills 
SLA research has also examined specific language skills th
at can be effectively developed with the use of music 
and song, such as explicit development of listening discrimination and comprehension. This has been examined 

for improving aural comprehension and spelling through use of modified cloze procedure (Froehlich, 1985); in a 

comparative study of conventional listening exercises an
d pop song gap-fill exercises (Kanel, 1997); through the 
use of music with passage correction tests (Odlin, 1986); 
and as practice for extracting meaning from noise with 
the mondegreens found in pop music (Smith, 2003). Music in the language classroom may also be utilized with 

an explicit vocabulary and grammar focus (Richards, 1969; Saricoban & Metin, 2000) and used to reinforce 

either grammar or pronunciation points (Allen & Vallette, 1977). Pronunciation and phonology are a natural use 
of songs in the aid of second language acquisition (Schn et al, 2008), and Leith (1979) states: 
there is probably not a better nor quicker way to teach 
phonetics than with
 songs. Phonetics instruction is one 
good use to which songs can be put even in beginning classes (540).  
The repetitive nature of songs makes them effective use 
for pronunciation drills (Bartle, 1962; Techmeier, 1969; 
Shaw, 1970) and lastly, it is argued that songs contextu
ally introduce supra-segmen
tal features (Lems, 2001; 
Wong and Perrachione, 2006), which aids in the learning of patterns for word identification.  
6. Practical Pedagogical Resources 
In the examination of the academic literature, we have 
seen a strong empirical and theoretical basis for the 
potential music has as an aid to language acquisition. Howe
ver, there appears to be a dearth of current literature 
that discusses the extent to which teachers are using musi
c and the manner in which it is being used in a practical 
context. Of the little research explor
ing teacher attitudes and practice that does exist, it seems to focus on 
primary school age children and rarely discusses the use of mu
sic and song with teen or adult learners. In the last 
category of the literature review, a brief survey, which 
does not claim to be exhaustive, of recent textbook 
publications and website journal articles may give some 
indication as to how music is being applied and what 
principles are being recommended in this representative sample.  
There are a number of web resources for use of music in th
e language classroom, including examples of practical 
lesson discussions with theoretical support in online journals such as the 
Internet TESL Journal
 or 
Humanising 
Language Teaching Magazine
. Examples include the use of music while writing to develop critical thinking 
(Aloha, 2005); use of music in language play (Cakir, 1999; Saricoban & Metin, 2000); use of music in 

conversation classes (Orlova, 2003); and use of song lyric word clouds as corpora for the language classroom 

(Kryszewska, 2010). There are also quite a few exampl
es of downloadable lessons and worksheets from various 
websites, the majority of which focus on grammatical or
 vocabulary activities, but few, if any, include any 
rationale or theoretical basis for decisions made in the development of the materials.  
www.ccsenet.org/elt English Language Teaching Vol. 6, No. 2; 2013 
120  Of the teacher resource books published since 2000, there 
is a focus on young learners, such as the Paterson and 
Willis (2008) 
English Through Music
 from the 
Oxford basics for children series
, which includes 36 varied 
practical activities with a theoretical discussion of ra
tionale for music use, what opportunities there are for 
language learning and improving the musical quality of classroom singing. There are also approaches that 

replace the original words of familiar folk, pop or ch
ildrens song melodies in order to learn phonics and 
grammatical concepts. For example, the melody of 
Frre Jacques
 is used with different words so that it becomes 
Talking and Walking
, with a focus on the spelling rules of 
long vowels and silent vowels in the 
Sing Your Way 
Through the Phonics
 program (Gifford, 2000); 
Rock Talk
 (Redding, 2006) explores basic grammar and phonics 
through rock melodies; and, in an effort to learn phonics and grammatical rules, the 
Learning English Through 
Songs
 and 
Phonics the Fun Way
 programs (Gabriel, 2007, 2010) use familiar childrens songs such as 
Twinkle, 
Twinkle Little Star
 and replaces the words to become the 
Grammar Rules Song 
or a song to learn 
Prepositions 
with Rhyming Words using structure There are
. There are also products that specialise in writing songs 
explicitly for the language learning market such as the 
Tutortunes
 grammatically focused pop Top-40-style 
song, Next to Me
 (Lock, 2009), which includes 54 different prepositions, or the 
Days of the Week
 (Mol, 2008) 
from 
Supasongs
, presumably both modelled to some degree on earlier language and music resources such as the 
grammar-orientated 
Hard to Learn that English as a Second Language Blues
 (Wellman & Byrd, 1975).  
There were a number of resource books about music use in the language classroom published in 1992 or 
immediately after, including Graham
 (1992), who created the concept of
 Jazz Chants, the rhythmic unison 
speaking of chants and poetry in the language classroom. Cranmer and Laroy (1992), Griffee (1992) and 

Murphey (1992) were also all published in the same year as part of different publishers established resource 

book series. Central to the Cranmer and Laroy (1992) text, 
which focuses on the use of 
Western orchestral music, 
is the technique of inner eye visualization, of the ability to form images in the mind in response to a musical 

stimulus (
ibid
: 2). The Griffee (1992) text also explores th
e affective response to music in a variety of 
approaches from dictations to listening exercises to music surveys and a number of extension activities along 

with firm theoretical underpinning to the choices made, noting that songs create their own world of feeling and 

emotion, and as we participate in the song, we participate in the world it creates (
ibid
: 4). Lastly, Murpheys 
(1992) text explores music and video, learner autonomy and a variety of authentic language activities that occur 

in first language discourse. Although first published in 1976, Grenough (1994) later republished and includes 

sheet music, lyrics, worksheet questions and visuals with
 additional information. If continuously published and 
therefore current in its song choices, it could prove to be a valuable resource, but by naming and working with 

specific pop songs, the text has a dated feel that the Grif
fee and Murphey texts do not. Unfortunately, out of all 
the texts mentioned in the last paragraph, 
Music and Song
 (Murphey, 1992) is the only one still in print at this 
time, which clearly limits the options for language 
teachers wanting to use music in the classroom.  
7. Conclusion 
One is left with a number of questions based on the lite
rature review of theoretical, empirical and practical 
classroom resources just completed. If connections betw
een music and language are as strong as the literature 
reviewed above suggests, why is there such disparity betw
een theoretical support and pr
actical application in the 
classroom? Why, outside of a few young
 learner texts, has there been such 
a gap in teacher pedagogical resource 
books supporting the use of music in language learning since the early 1990s? Why are the needs of adult and 
teen learners not reflected in the curren
t web or print pedagogical resources?  
7.1 Further Research 
Given the limitations of the authors original study, there appears to be a need for additional research of a similar 
nature examining teacher attitudes and practices
 with a larger sample size in order to increase statistical 
significance. Furthermore, several interesting questions arising from the study point to a need for further research. 

There is a need for an in-depth study with a class of teen and adult learners to measure effects of a music 

intensive teaching methodology versus
 conventional methodologies. There is
 also a need to establish, by 
conducting research on subjects of various age levels, 
whether the influence of music is greater at any one 
particular developmental period. Additionally, further ex
ploration of teacher attitudes and prejudices pertaining 
to Hip-Hop and rap, making connections to World Englishes and identity, is in order. Lastly, little has been done 

in the subvocal involuntary mental rehearsal of the Song-Stuck-In-My-Head-Phenomenon (Murphey, 1990) and 

the motherese of adolescence (Murphey & Alber, 1985; Murphey, 1992a) since first published, although both 
are still viable concepts and re
searchable (Murphey, 2010).  
7.2 Pedagogical Implications 
A number of practical suggestions made by respondents during the course of the authors study point to ways 
www.ccsenet.org/elt English Language Teaching Vol. 6, No. 2; 2013 
121  that a possible disconnect between t
eacher theoretical support and classr
oom use can be bridged. Numerous 
respondents requested materials development as a resour
ce for use of music by language teachers. Of the many 
useful music books aimed at English 
language teachers of adult and teen st
udents published in the 1990s, only 
the Murphey (1992) is still available. 
Therefore, the suggestion from many 
respondents to publish new materials 
may be valid.  Potential suggestions include (1) a journal aimed specifically at uses of music in language 

teaching, with opportu
nity for feedback and conferences (2); a 
practical resource book with ready-made 
photocopiable lesson plans and worksheets (3); and lastly, an edited collection of music in language-learning 

journal articles so that teachers and 
future researchers can easily access the 
growing body of published research 
in this subfield. This literature review is
 an attempt to address the last point. 
7.3 Closing 
Overall, the results are clear in suggesting use of musi
c and song in the language-learning classroom is both 
supported theoretically by practicing teachers and grounde
d in the empirical literature as a benefit to increase 
linguistic, sociocultural and communicative competencies. From an educational standpoint, music and language 
not only can, but should be studied together. 
References 

Abrate, J. (1983). Pedagogical Applications of the French Popular Song in the Foreign Language Classroom. 
The Modern Language Journal, 67
(1), 8-12. http://dx.doi.org/10.2307/326687 
Ahola, S. (2005). Digger Deep
er into Songs: A Writing Activity. 
The Internet TESL Journal
, XI(2). Retrieved 
20.12.2012 from http://iteslj.org/Lessons/Ahola-Songs.html 
Ajibade, Y., & Ndububa, K. (2008). Effects of Word Games, Culturally Relevant Songs, and Stories on Students 
Motivation in a Nigerian English Language Class. 
TESL Canada Journal/Revue TESL du Canada, 25
(2), 
27-48. 
Allen, J., & Vallette, E. (1977). 
Classroom teaching of foreign languages and English as a second language
. Chicago: University of Chicago Press. 
Anshel, A., & Kipper, D. (1988). The Influence of Group Singing on Trust and Cooperation. 
Journal of Music 
Therapy, 25
(3), 145-155. 
Balch, W., Bowman, K., & Mohler, L. (1992). Music-dependent memory in immediate and delayed word recall. 
Memory & Cognition, 20
(1), 21-28. http://dx.doi.org/10.3758/BF03208250 
Bancroft, W. (1978). The Lozanov Me
thod and Its American Adaptations. 
The Modern Language Journal, 62
(4), 167-175. http://dx.doi.org/10.2307/324351 
Bartle, G. (1962). Music in the language classroom. 
Canadian Modern Language Review, Fall
, 11-14. 
Bell, J., & Gower, R. (1998). Writing course materials for 
the world: a great compromise
. In Tomlinson, B. (ed.), 
Materials Development in Language Teaching
 (pp. 116-129). Cambridge: Cambridge University Press. 
Benson, P. (2006). Autonomy in language teaching and learning. 
Language Teacher, 40
, 21-40. 
http://dx.doi.org/10.1017/S0261444806003958 
Bergeson, T., & Trehub, S. (2002). Absolute Pitch and Tempo in Mothers Songs to Infants. 
Psychological 
Science,
 13(1), 72-75. http://dx.doi.org/10.
1111/1467-9280.00413 
Berman, M. (1998). 
A Multiple Intelligences Road to an ELT Classroom
. Wales: Crown House Publishing 
Limited. 
Borchgrevink, H. (1982). Prosody and musical rhythm are controlled by the speech hemisphere. In Clyne, M. 
(ed.), 
Music, Mind, and Brain: The Neuropsychology of Music 
(pp. 151-157). New York: Plenum. 
Brown, H. (2007). 
Principles of Language Learning and Teaching
 (5
th edition) (pp. 132-151). New York: 
Pearson. 
Brown, T., & Perry, F. Jr. (1991). A 
Comparison of Three Learning Strategi
es for ESL Vocabulary Acquisition. 
TESOL Quarterly, 25
(4), 655-670. http://dx.doi.org/10.2307/3587081 
Brutten, S., Angelis, P., & Perkins, K. (1985). Music and 
Memory: Predictors for Attained ESL Oral Proficiency. 
Language Learning, 35
(2), 299-313. http://dx.doi.org/10.1111/j.1467-1770.1985.tb01030.x 
Bryant, P., MacLean, M., Bradley, L., & Crossland, J. 
(1990). Rhyme and Alliteration, Phoneme Detection, and 
Learning to Read. 
Developmental Psychology, 26
(3), 429-438. 
http://dx.doi.org/10.1037//0012-1649.26.3.429 
www.ccsenet.org/elt English Language Teaching Vol. 6, No. 2; 2013 
122  Cakir, A. (1999). Musical Activitie
s for Young Learners of EFL. 
The Internet TESL Journal,
 V(11). Retrieved 
20.12.2012 from http://iteslj.org/Lessons/Cakir-MusicalActivities.html 
Campbell, D. (1997). 
The Mozart Effect: Tapping the Power of Musi
c to Heal the Body, Strengthen the Mind, 
and Unlock the Creative Spirit
. New York: Avon Books Inc. 
Campbell, P. (1998). The musical cultures of children. 
Research Studies in Music Education, 11
, 42-51. 
http://dx.doi.org/10.1177/1321103X9801100105 
Candlin, C. (1992). Pref
ace in Griffee, D. 
Songs in Action: Classroom Techniques and Resources 
(ix-x). New 
York: Prentice Hall. 
Carrell, P., Pharis, B., & Liberto, J. (1989). Me
tacognitive Strategy Training for ESL Reading. 
TESOL Quarterly, 
23(4), 647678. http://dx.doi.org/10.2307/3587536 
Chabris, C. (1999). Prelude or 
requiem for the Mozart effect? 
Nature, 400
, 826-827. 
Chamot, A. (2005). Language Learning Strategy Instruction: Current Issues and Research. 
Annual Review of 
Applied Linguistics
, 25, 112-130. http://dx.doi.org/10.1017/S0267190505000061 
Chen, J., & Gardner, H. (2005). Assessment Based on Multiple-Intelligences Theory. In Flanagan, D., & P. 
Harrison (eds.), 
Contemporary Intellectual Assessmen
t: theories, tests, and issues
 (pp. 77-102). New York: 
Guilford Press. 
Chen-Hafteck, L. (1997). Music and language development in early childhood: Integrating past research in the 
two domains. 
ECDC, 130, 85-97. http://dx.doi.org/10.1080/0300443971300109  
Cheung, C. K. (2001). The use of popular culture as a stimulus to motivate secondary students English learning 
in Hong Kong. 
ELT Journal
, 55(1), 55-61. http://dx.doi.org/10.1093/elt/55.1.55 
Claerr, T., & Gargan, R. (1984). The Role of Songs in the Foreign Language Classroom. 
OMLTA Journal, 
FL014904
, 28-32. 
Coe, N. (1972). What use 
are songs in FL teaching? 
International Review of Applied Linguistics in Language 
Teaching, 10
(4), 357-360. 
Cohen, A. (1998). 
Strategies in learning and using a second language
. London: Longman. 
Cook, G. (1997). Language Learning, Language Play. 
ELT Journal
, 51(3), 224-231. 
http://dx.doi.org/10.1093/elt/51.3.224 
Count, E., Hewes, G., Livingstone, F., & Mourant, A. (1974). On the Phylogenesis of the Speech Function. 
Current Anthropology, 15
(1), 81-90. http://dx.doi.org/10.1086/201440 
Cranmer, D., & Laroy, C. (1992). 
Musical Openings: Using music in the language classroom
. Essex: Addison 
Wesley Longman Limited. 
Cullen, B. (1998). Music and Song in Discussion. 
The Internet TESL Journal, IV(
10). Retrieved 20.12.2012 from 
http://iteslj.org/Techniques/Cullen-Music.html 
Domoney, L., & Harris, S. (1993). Justified and Ancient  Pop Music in EFL Classrooms. 
ELT Journal
, 47(3), 
234-241. http://dx.doi.org/10.1093/elt/47.3.234 
Dubin, F. (1975). An Overlooked Resource for English Language Teaching: Pop, Rock, and Folk Music. 
CATESOL (California Association of Teachers of English to Speakers of Other Languages) Occasional 
Papers, No. 2
, ED126673. 
Ellis, R. (1997). 
Second Language Acquisition: Oxford Introductions to Language Study
. Oxford: Oxford 
University Press. 
Failoni, J. (1993). Music as Means to Enhance Cultura
l Awareness and Literacy in the Foreign Language 
Classroom. 
Mid-Atlantic Journal of Foreign Language Pedagogy, 1
, 97-108. 
Ferradas Moi, C. (1994). Rock poetry: the literature our students listen to. 
The Journal of the Imagination in 
Language Learning, 2
, 56-9. Jersey City State College. 
Ferradas Moi, C. (2003). Materials for Language through Literature. Rocking the Classroom: Rock Poetry 
Materials in the EFL Class. In Tomlinson, B. (ed.), 
Developing Materials for Language Teaching 
(pp. 
406-421). London: Continuum. 
Fonseca Mora, C. (2000). Foreign langu
age acquisition and melody singing. 
ELT Journal, 54
(2), 146-152. 
http://dx.doi.org/10.1093/elt/54.2.146 
www.ccsenet.org/elt English Language Teaching Vol. 6, No. 2; 2013 
123  Froehlich, J. (1985). Improving Aural Comprehension and Spelling through a Modified Cloze Procedure Using 
Songs and Poems. 
Die Unterrichtspraxis/Teaching German, 18
(1), 49-54. 
http://dx.doi.org/10.2307/3529993 
Gabriel, B. (2007). 
Learning English Through Songs
. Singapore: Bettyland Publications. 
Gabriel, B. (2010). 
Phonics the Fun Way. 
Singapore: Bettyland Publications. 
Ganschow, L., Sparks, R., Anderson, R., Javorshy, J., Skin
ner, S., & Patton, J. (1994). Differences in Language 
Performance among High-, Average-, and Low-Anxious College Foreign Language Learners. 
The Modern 
Language Journal, 78
(1), 41-55. http://dx.doi.org/10.2307/329251 
Gao, X. (2008). English Corner as Out of Class Learning. 
ELT Journal, 63
(1), 60-67. 
http://dx.doi.org/10.1093/elt/ccn013 
Gardner, H. (1999). 
Intelligence reframed
. New York. Basic Books. 
Gifford, M. (2000). 
Sing Your Way Through Phonics: Song Lyrics 
(Volumes I, II, III).
 Beavercreek: Action Factor 
Publishing. 
Gomez, R., & Gerken, L. (2000). Infant artific
ial language learning and language acquisition. 
Trends in 
Cognitive Sciences, 4
(5), 178-186. http://dx.doi.org/10.1016/S1364-6613(00)01467-4 
Graham, C. (1992). 
Singing, Chanting, Telling Tales: Arts in the Language Classroom
. New Jersey: Prentice 
Halls. Gravenall, B. (1945). Mu
sic in Language-Teaching. 
ELT Journal, 3
(5), 123-127. 
http://dx.doi.org/10.1093/elt/III.5.123 
Grenough, M. (1994). 
Sing It! Learn English Through Song Text/Workbook 
(Vol. 2). Mexico: Mc
Graw-Hill, Inc.  
Grenough, M. (1994). 
Sing It! Learn English Through Song Text/Workbook 
(Vol. 5). Mexico: Mc
Graw-Hill, Inc.  
Griffee, D. (1989). Editorial Introduction. 
The Language Teacher, XIII
(5), 3. 
Griffee, D. (1992). 
Songs in Action: Classroom Techniques and Resources 
(ix-x). New York: Prentice Hall. 
Griffee, D. (2010). Personal 
communication with the author. 
Hamblin, V. (1987). Integrating Popular Songs into the Curriculum. 
The French Review, 60
(4), 479-484. 
Harley, B. (2000). Learning Strategies in 
ESL: Do Age and L1 Make a Difference. 
TESOL Quarterly, 34
(4), 
769-777. 
Harwood, E. (1998). Music Learning in Context: A Playground Tale. 
Research Studies in Music Education, 11
, 52-60. http://dx.doi.org/10.1177/1321103X9801100106 
Hetland, L. (2000). Listening to Musi
c Enhances Spatial-Temporal Reasoning:
 Evidence for the 
Mozart Effect. 
Journal of Aesthetic Education, 34
(3, 4), 105-148. http://dx.doi.org/10.2307/3333640 
Howle, M. (1989). Twinkle, twinkle little star: Its more than just a nursery song. 
Children Today
, 18(4), 18-22.  
Huy Le, M. (1999). The Role of Music in Second Language Learning: A Vietnamese Perspective. 
Paper 
presented at Combined 1999 Conference of the Australian Association for Research in Education and the 
New Zealand Association for Research in Education
: LE99034. 
Jackendoff, R. (1991
). Musical Parsing and Musical Affect. 
Music Perception, 9
(2), 199-229. 
http://dx.doi.org/10.2307/40285529 
Jackendoff, R. (2006). A Parallel Archit
ecture perspective on 
language processing. 
Brain Research, 1146
, 2-22. 
http://dx.doi.org/10.1016/j.brainres.2006.08.111 
Jenkins, J., & Dixon, R. (1983). Vocabulary Learning. 
Contemporary Educational Psychology, 8
, 237-260. 
http://dx.doi.org/10.1016/0361-476X(83)90016-4 
Jolly, Y. (1975). The Use of Songs in Teaching Foreign Languages. 
The Modern Language Journal, 59
(1,2), 
11-14. http://dx.doi.org/10.2307/325440 
Jones, R. (2008). Echoing Their Lives: Teaching Russian Language and Culture Through the Music of Vladimir 
S. Vysotsky. 
Dissertation, University of Texas at Austin. 
Kanel, K. (1997). Teaching with Music: A comparison of conventional listening exercises with pop song gap-fill 
exercises. 
JALT Journal, 19
(2), 217-234. 
www.ccsenet.org/elt English Language Teaching Vol. 6, No. 2; 2013 
124  Kanel, K. (2000). Songs in Language Teaching: Theory and Practice. 
Paper presented at The Proceedings of the 
JALT (Japan Association for Language Teaching) 25
th Annual International Conference on Language 
Teaching & Learning and Educational Materials Expo
, 69-75. 
Kelly, M., & Rubin, D. (1988). Natural Rhythmic Patterns in English Verse: Evidence from Child Counting-Out 
Rhymes. 
Journal of Memory and Language, 27
, 718-740. http://dx.doi.org/10.1016/0749-596X(88)90017-4 
King, R. (2010). Music and Storytelling in the EFL Classroom. 
Humanising Language Teaching,
 12(2). ISSN 
1755-9715. Retrieved 20.12.2012 from http://www.hltmag.co.uk/apr10/sart05.htm 
Koelsch, S., Gunter, T., & Friederici, A. (2000). Brain 
Indices of Music Processing: 
Nonmusicians are Musical. 
Journal of Cognitive Neuroscience, 12
(3), 520-541. http://dx.doi.org/10.1162/089892900562183 
Koelsch, S., Gunter, T., Cramon, D., Zysset, S., Lohmann,
 G., & Friederici, A. (2002)
. Bach Speaks: A Cortical 
Language-Network Serves th
e Processing of Music. 
NeuroImage,
 17,
 956-966. 
http://dx.doi.org/10.1006/nimg.2002.1154 
Krashen, S. (1982). 
Principles and Practices in Second Language Acquisition
. Oxford: Pergamon. 
Krumhansl, C. (2000). Rhythm and Pitch in Music Cognition. 
Psychological Bulletin, 126
(1), 159-179.  
Kryszewska, H. (2010). Ideas from the Corpora: Word Clouds. 
Humanising Language Teaching Magazine for 
teachers and teacher trainers, 12
(3)
. Retrieved 20.12.2012 from http://www.hltmag.co.uk/jun10/idea.htm 
Lake, R. (2003). Enhancing Acquisition through Music. 
The Journal of Imagination in Language Learning, 
Jersey City State College, 7. 
Lam, W., & Wong, J. (2000). The effects of strategy training on developing discussion skills in an ESL 
classroom. 
ELT Journal, 54
(3), 245255. http://dx.doi.org/10.1093/elt/54.3.245 
Leith, W. (1979). Advanced French Conversation through Popular Music. 
The French Review, 52
(4), 537-551. 
Lems, K. (1996). For a Song: Mu
sic across the ESL Curriculum. 
Paper presented at Annual Meeting of the 
Teachers of English to Speakers of Other Languages, 
1-18. ED 396524. 
Lems, K. (2001). Using Music in the Adult ESL Classroom (1-4). 
Paper presented at National-Louis University
. Levitin, D., & Menon, V. (2003). Musical structure is processed language areas of the brain: a possible role for 
Brodmann Area 47 in temporal coherence. 
NeuroImage, 20
, 2142-2152. 
http://dx.doi.org/10.1016/j.neuroimage.2003.08.016 
Livingstone, F. (1973). Did the Australopithecines Sing? 
Current Anthropology 14
(1, 2), 25-29. 
http://dx.doi.org/10.1086/201402 
Locastro, V. (1994). Learning Strategies & Learning Environments. 
TESOL Quarterly, 28
(2), 409-414. 
http://dx.doi.org/10.2307/3587445 
Lock, G. (2009). 
Next to Me
. Tutortunes: Pop Songs with a Learning Element for ESL. Retrieved 20.12.2012 
from http://tutortunes.net/tutortunes/category/samples 
Loewy, J. (1995). Th
e Musical Stages of Speech: A Developmen
tal Model of Pre-ve
rbal Sound Making. 
Music 
Therapy, 13
(1),
 47-73. 
Lozanov, G. (1978). 
Suggestology and outlines of suggestopedy.
 New York: Gordon and Breach Publishing.  
Maess, B., Koelsch, S., Gunter, T., & Friederici, A. (
2001). Musical syntax is processed Brocas area: an MEG 
study. 
Nature Neuroscience, 4
(5), 540-545. 
Magahay-Johnson, W. (1984). Music Hath Charms: Music and Student-Created Stories in the ESL Classroom. 
TESL Canada Journal/Revue TESL du Canada, 1
(1), 81-82. 
McGrath, I. (2002). 
Materials Evaluation and Design for Language Teaching
. Edinburgh: Edinburgh University 
Press. McMillan, D., & Chavis, D. (1986). Sense of Community: A Definition and Theory. 
Journal of Community 
Psychology, 14
(1), 6-23. 
http://dx.doi.org/10.1002/1520-6629(198601)14:1<6::AID-JCOP2290140103>3.0.CO;2-I 
McMullen, E., & Saffran, J. (2004). Music and Language: A Developmental Comparison. 
Music Perception, 
21(3), 289-311. http://dx.doi.org/10.1525/mp.2004.21.3.289 
Medina, S. (1990). The effects of music 
upon second language vocabulary acquisition.
 Paper presented at the 
www.ccsenet.org/elt English Language Teaching Vol. 6, No. 2; 2013 
125  TESOL conference. San Francisco, CA. 
ED352834.
 Merriam, A. (1964). 
The Anthropology of Music
. Evanston: Northwestern University Press. 
Mishan, F. (2005). 
Designing Authenticity into Language Learning Materials
. Bristol: Intellect Books. 
Mol, H. (2008). 
Days of the Week
. Supasongs: Learning English Through Music. Retrieved 20.12.2012 from 
https://secure.ntechmedia.com/sites/supasongs/index.php?template=Main&catid=1 
Murphey, T., & Alber, J. (1985). A Pop Song Register
: The Motherese of Adolesce
nts as Affective Foreigner 
Talk. TESOL Quarterly, 19
(4), 793-795. http://dx.doi.org/10.2307/3586679 
Murphey, T. (1989). The Top 40 for Teachers (in no special order). 
The Language Teacher XIII
(5), 3-6. 
Murphey, T. (1990). The Song Stuck in My Head Phenomenon: A Melodic Din in the LAD? 
System, 18
(1), 
53-64. http://dx.doi.org/10.1016/0346-251X(90)90028-4 
Murphey, T. (1992a). 
Music and Song
. Oxford: Oxford University Press. 
Murphey, T. (1992b). The Discourse of Pop Songs. 
TESOL Quarterly, 26
(4), 770-774. 
http://dx.doi.org/10.2307/3586887 
Murphey, T. (2010). Personal communication with the author. 

Nagy, W., & Herman, P. (1987). Breadth and Depth of Vocabulary Knowledge: Implications for Acquisition and 
Instruction. In McKeown, M., & M. Curtiss (eds.), 
The Nature of Vocabulary Acquisition 
(pp. 19-35). 
Hillsdale: Erlbaum Publishers. 
Naiman, N., Frohlich, M., Stern, H., & Todesco, A. (1996). 
The Good Language Learner. 
Clevedon, UK: 
Multilingual Matters. 
Nantais, K., & Schellenberg, E. (1999). Th
e Mozart Effect: An Ar
tifact of Preference. 
Psychological Science, 
10(4), 370-373. http://dx.doi.org/10.1111/1467-9280.00170 
Ndububa, K., & Ajibade, Y. (2006). Effects of Word Ga
mes, Culturally Relevant Songs and Stories on Students 
Motivation in a Nigerian English Language Class. 
European Journal of Social Sciences, 2
(2), 240-250. 
Nguyen, P., Terlouw, C., & Pilot, A. (2006). Culturally appropriate pedagogy: the case of group learning in a 
Confucian Heritage Culture context. 
Intercultural Education, 17
(1), 1-19. 
http://dx.doi.org/10.1080/14675980500502172 
Nunan, D. (1989). 
Designing tasks for the Communicative Classroom
. Cambridge: Cambridge University Press. 
Nunan, D. (2004). 
Task-Based Language Teaching
. Cambridge: Cambridge University Press. 
Odlin, T. (1986). Another Look at Passage Correction Tests. 
TESOL Quarterly, 20(1
), 123-130. 
http://dx.doi.org/10.2307/3586394 
Orlova, N. (2003). Helping Prospective EFL Teachers
 Learn How to Use Songs 
in Teaching Conversation 
Classes. The Internet TESL Journal, IX
(3). Retrieved 20.12.2012 from 
http://iteslj.org/Techniques/Orlova-Songs.html 
Oxford, R. (1990). 
Language Learning Strategies: What Every Teacher Should Know
. Boston, Heinle and 
Heinle. 
Palmer, P. (1998). 
The Courage to Teach: Exploring the Inner Landscape of a Teachers Life
 (2
nd edition). (2007). 
San Francisco: Jossey-Bass. 
Palmer, C., & Kelly, M. (1992). Linguistic Prosody and Musical Meter in Song. 
Journal of Memory and 
Language, 31
, 525-542. http://dx.doi.org/10.1016/0749-596X(92)90027-U 
Papousek, M., & Papousek, H. (1981). 
Musical elements in the infant's 
vocalisation: their significance for 
communication, cognition, and creativity. In Lipsitt, L. (ed.), 
Advances in infancy research: Volume 1
 (pp. 
163-224)
. Norwood, Ablex Publishing Corporation. 
Papousek, M., & Papousek, H. (1991). Early Verbalizations as Precursors of Language Development. In Lamb, 
M., & H. Keller (eds.), 
Infant Development: Perspectives
 from German-Speaking Countries
 (pp. 299-328). 
Hillsdale, Lawrence Erlbaum As
sociates, Inc. Publishers. 
Patel, A., Edward, G., Ratner, J., Besson, M., & Holcomb, P. (1998). Processing Syntactic Relations in Language 
and Music: An Event-Related Potential Study. 
Journal of Cognitive Neuroscience, 10
(6), 717-733. 
http://dx.doi.org/10.1162/089892998563121 
www.ccsenet.org/elt English Language Teaching Vol. 6, No. 2; 2013 
126  Patel, A. (2003). Language, mu
sic, syntax and the brain. 
Nature Neuroscience, 6
(7), 674-681. 
http://dx.doi.org/10.1038/nn1082 
Paterson, A., & Willis, J. (2008). 
English Through Music
. Oxford: Oxford University Press. 
Pennycook, A. (2003). Global Englishes, Rip Slyme, and performativity
. Journal of Sociolinguistics, 7
(4),
 513-533
. http://dx.doi.org/10.1111/j.1467-9841.2003.00240.x 
Pennycook, A. (2007). 
Global Englishes and Transcultural Flows
. New York: Routledge.  
Peretz, I. (2002). Brain 
Specialization for Music. 
The Neuroscientist
, 372-380. 
http://dx.doi.org/10.1111/j.1749-6632.2001.tb05731.x 
Plagwitz, T. (2006). Karaoke in the Digital Audio Lab. 
Proceedings of the Digital Stream Conference 2006 at 
California State University, Monterey Bay, ISSN 1946-1526
.  Pope, R. (1995). 
Textual Intervention: Critical and Creative Strategies for Literary Studies.
 London: Routledge. 
Rauscher, F., Shaw, G., & Ky, K. (1993)
. Music and Spatial 
Task Performance. 
Nature, 365
, 611. 
http://dx.doi.org/10.1038/365611a0 
Rauscher, F., Shaw, G., & Ky, K. (1995). Listening to 
Mozart enhances spatial-temp
oral reasoning: towards a 
neurophysiological basis. 
Neuroscience Letters, 185
, 44-47. 
http://dx.doi.org/10.1016/0304-3940(94)11221-4 
Redding, J. (2006). 
Rock Talk by Request
. Rock Talk: Americas Number 
One Musical Phonics Program for 
ESL. Retrieved 20.12.2012 from www.eslrocks.com 
Richards, J. (1969). Songs in Language Learning. 
TESOL Quarterly, 3
(2), 161-174. 
http://dx.doi.org/10.2307/3586103 
Richards, J., & Rodgers, T. (2008). 
Approaches and Methods in Language Teaching
. Cambridge: Cambridge 
University Press. 
Rubin, D. (1995). 
Memory in Oral Traditions: the Cognitive Psychology of Epic, Ballads, and Counting-Out 
Rhymes. 
New York: Oxford University Press. 
Salcedo, C. (2010). The Effects of Songs in the Foreign Language Classroom on Text Recall, Delayed Text 
Recall and Involuntary Mental Rehearsal. 
Paper presented at 2010 IABR (International Applied Business 
Research) & ITLC (International College Teac
hing and Learning) Conference Proceedings,
 1-12. 
Saricoban, A., & Metin, E. (2000). Songs, Verse and Games for Teaching Grammar. 
The Internet TESL Journal, 
VI(10). Retrieved 20.12.2012 from http://iteslj.org/Techniques/Saricoban-Songs.html 
Schoepp, K. (2001). Reasons for Using Songs in the ESL/EFL Classroom. 
The Internet TESL Journal, VII
(2). 
Retrieved 20.12.2012 from http://iteslj.org/Articles/Schoepp-Songs.html 
Schn, D., Boyer, M., Moreno, S., Besson, M., Peretz, I
., & Kolinsky, R. (2008). Songs as an aid for language 
acquisition. 
Cognition 106
(2), 975-983. http://dx.doi.org/10.1016/j.cognition.2007.03.005 
Schunk, H. (1999). The Effect of Si
nging Paired with Signing on Receptiv
e Vocabulary Skills of Elementary 
ESL Students. 
Journal of Music Therapy, XXXVI
(2), 110-124. 
Shaw, A. (1970). How to make Songs for Language Drill. 
English Language Teaching, 24
(1), 125-132. 
http://dx.doi.org/10.1093/elt/XXIV.2.125 
Sinclair, B. (1996). Learner autonomy and literature
 teaching. In Carter, C., & McRae, J. (eds.), 
Language, 
Literature and the Learner: Creative Classroom Practice 
(pp.
 138-150). New York, Longman. 
Skehan, P. (1998). 
A Cognitive Approach to Language Learning
. Oxford, Oxford University Press. 
Slevc, L., Rosenberg, J., & Patel, A. (2008). Language
, Music, and Modularity: Evidence for Shared Processing 
of Linguistic and Musical Syntax. 
Proceedings of the 10
th International Conference on Music Perception 
and Cognition ICMPC10), Japan
, 598-605. 
Smith, G. (2003). Music and mondegreens: extracting meaning from noise. 
ELT Journal, 57
(2). 113-121. 
http://dx.doi.org/10.1093/elt/57.2.113 
Steele, K., Bass, K., & Crook, M. 
(1999). The Mystery of the Mozart
 Effect: Failure to Replicate. 
Psychological 
Science, 10
(4), 366-369. http://dx.doi.org/10.1111/1467-9280.00169 
Steele, K., Bella, S., Peretz, I., Dunlop, T., Dawe, L., 
Humphrey, G., Shannon, R., Kirby, J. Jr., & Olmstead, C. 
www.ccsenet.org/elt English Language Teaching Vol. 6, No. 2; 2013 
127  (1999). Prelude or requiem for the Mozart effect? 
Nature, 400
, 827. 
Steinke, W., Cuddy, L., & Holden, R. (1997). Dissoci
ation of Musical Tonality and Pitch Memory from 
Nonmusical Cognitive Abilities. 
Canadian Journal of Experimental Psychology, 51
(4), 316-334. 
http://dx.doi.org/10.1037/1196-1961.51.4.316 
Techmeier, M. (1969). Music in the Teaching of French. 
The Modern Language Journal, 53
(2), 96. 
http://dx.doi.org/10.1111/j.1540-4781.1969.tb04568.x 
Thompson, B., & Andrews, S. (2000). An Historical Commentary on the Physiological Effects of Music: 
Tomatis, Mozart and Neuropsychology. 
Integrative Physiological and Behavioral Science,
 35(3), 174-188. 
http://dx.doi.org/10.1007/BF02688778 
Trainor, L., Austin, C., & Desjardins, R. (2000). Is Infant-Directed Speech Prosody a Result of the Vocal 
Expression of Emotion? 
Psychological Science, 11
(3), 188-195. http://dx.doi.org/10.1111/1467-9280.00240 
Trehub, S., Trainor, L., & Unyk. (1993). Music and Speech
 Processing in the First Year
 of Life. In Reese, H. 
(ed.), 
Advances in Child Development and Behavior 
(Volume 24) (pp. 2-29). Lon
don: Academic Press, Inc. 
Trehub, S., & Trainor, L. (1998). Sing
ing to Infants: Lullabies and Play Songs. In Rovee-Collier, C., L. Lipsitt, 
& H. Hayne (eds.), 
Advances in Infancy Research 
(Volume 12) (pp. 43-78). Stamford: Alex Publishing 
Corporation. 
Wellman, L., & Byrd, D. (1975). 
Hard to Learn that English as a Second Language Blues
. New York: Collier 
McMillan. 
Wilcox, W. (1995). Music cues from classroom singing 
for second language acquisition: Prosodic memory for 
pronunciation of target vocabulary by adult non-native English speakers. 
Doctoral dissertation, University 
of Kansas: UMI 9544866. 
Wong, P., & Perrachione, T. (2007). Learning pitch patte
rns in lexical identification by native English-speaking 
adults. 
Applied Psycholinguistics, 27
, 565-585. http://dx.doi.org/10.1017/S0142716407070312 
Wray, A., & Perkins, M. (2000). Functions of Formulaic Language. 
Language and Communication, 20
, 1-28. 
http://dx.doi.org/10.1016/S0271-5309(99)00015-4 
Zatorre, R., Meyer, E., Gjedde, A., & Evans, A. (1996). PET Studies of Phonetic Processing of Speech: Review, 
Replication, and Reanalysis. 
Cerebral Cortex, 6
, 21-30. http://dx.doi.org/10.1093/cercor/6.1.21 
  nData mining and knowledge discovery in
databases have been attracting a signicant
amount of research, industry, and media atten-

tion of late. What is all the excitement about?This article provides an overview of this emerging
eld, clarifying how data mining and knowledge
discovery in databases are related both to each

other and to related elds, such as machine
learning, statistics, and databases. The article

mentions particular real-world applications,

specic data-mining techniques, challenges in-

volved in real-world applications of knowledge
discovery, and current and future research direc-
tions in the eld.Across a wide variety of elds, data arebeing collected and accumulated at a
dramatic pace. There is an urgent needfor a new generation of computational theo-ries and tools to assist humans in extractinguseful information (knowledge) from the

rapidly growing volumes of digital data.
These theories and tools are the subject of the
emerging eld of knowledge discovery in

databases (KDD).At an abstract level, the KDD eld is con-
cerned with the development of methods andtechniques for making sense of data. The basicproblem addressed by the KDD process is one

of mapping low-level data (which are typically

too voluminous to understand and digest easi-

ly) into other forms that might be more com-

pact (for example, a short report), more ab-

stract (for example, a descriptive
approximation or model of the process that
generated the data), or more useful (for exam-
ple, a predictive model for estimating the val-

ue of future cases). At the core of the process is

the application of specic data-mining meth-

ods for pattern discovery and extraction.
1This article begins by discussing the histori-cal context of KDD and data mining and theirintersection with other related elds. A brief

summary of recent KDD real-world applica-
tions is provided. Denitions of KDD and da-ta mining are provided, and the general mul-tistep KDD process is outlined. This multistepprocess has the application of data-mining al-
gorithms as one particular step in the process.The data-mining step is discussed in more de-tail in the context of specic data-mining al-
gorithms and their application. Real-worldpractical application issues are also outlined.Finally, the article enumerates challenges for
future research and development and in par-
ticular discusses potential opportunities for AI
technology in KDD systems.Why Do We Need KDD?
The traditional method of turning data into
knowledge relies on manual analysis and in-terpretation. For example, in the health-care

industry, it is common for specialists to peri-
odically analyze current trends and changes
in health-care data, say, on a quarterly basis.
The specialists then provide a report detailingthe analysis to the sponsoring health-care or-
ganization; this report becomes the basis for
future decision making and planning forhealth-care management. In a totally differ-

ent type of application, planetary geologists
sift through remotely sensed images of plan-ets and asteroids, carefully locating and cata-loging such geologic objects of interest as im-pact craters. Be it science, marketing, nance,
health care, retail, or any other eld, the clas-sical approach to data analysis relies funda-mentally on one or more analysts becoming
Articles
FALL 1996    37
From Data Mining toKnowledge Discovery in
DatabasesUsama Fayyad, Gregory Piatetsky-Shapiro, and Padhraic Smyth 
Copyright  1996, American Association for Artificial Intelligence. All rights reserved. 0738-4602-1996 / $2.00
areas is astronomy. Here, a notable success
was achieved by SKICAT
,a system used by as-
tronomers to perform image analysis,
classication, and cataloging of sky objectsfrom sky-survey images (Fayyad, Djorgovski,
and Weir 1996). In its rst application, the
system was used to process the 3 terabytes
(1012bytes) of image data resulting from theSecond Palomar Observatory Sky Survey,
where it is estimated that on the order of 109sky objects are detectable. SKICAT
can outper-form humans and traditional computationaltechniques in classifying faint sky objects. SeeFayyad, Haussler, and Stolorz (1996) for a sur-
vey of scientic applications.In business, main KDD application areas
includes marketing, nance (especially in-
vestment), fraud detection, manufacturing,telecommunications, and Internet agents.Marketing:In marketing, the primary ap-
plication is database marketing systems,which analyze customer databases to identifydifferent customer groups and forecast their
behavior. 
Business Week 
(Berry 1994) estimat-
ed that over half of all retailers are using or
planning to use database marketing, andthose who do use it have good results; for ex-ample, American Express reports a 10- to 15-percent increase in credit-card use. Another
notable marketing application is market-bas-ket analysis (Agrawal et al. 1996) systems,which nd patterns such as, If customerbought X, he/she is also likely to buy Y andZ. Such patterns are valuable to retailers.Investment: Numerous companies use da-ta mining for investment, but most do notdescribe their systems. One exception is LBSCapital Management. Its system uses expert
systems, neural nets, and genetic algorithmsto manage portfolios totaling $600 million;since its start in 1993, the system has outper-formed the broad stock market (Hall, Mani,and Barr 1996).Fraud detection: HNC Falcon and NestorPRISMsystems are used for monitoring credit-card fraud, watching over millions of ac-
counts. The FAIS
system (Senator et al. 1995),from the U.S. Treasury Financial Crimes En-
forcement Network, is used to identify nan-
cial transactions that might indicate money-laundering activity.
Manufacturing: The CASSIOPEEtrou-bleshooting system, developed as part of ajoint venture between General Electric andSNECMA, was applied by three major Euro-
pean airlines to diagnose and predict prob-
lems for the Boeing 737. To derive families of
faults, clustering methods are used. CASSIOPEEreceived the European rst prize for innova-
intimately familiar with the data and serving
as an interface between the data and the usersand products.For these (and many other) applications,this form of manual probing of a data set isslow, expensive, and highly subjective. In

fact, as data volumes grow dramatically, this
type of manual data analysis is becomingcompletely impractical in many domains.Databases are increasing in size in two ways:(1) the number Nof records or objects in thedatabase and (2) the number dof elds or at-tributes to an object. Databases containing onthe order of N= 109objects are becoming in-creasingly common, for example, in the as-tronomical sciences. Similarly, the number of
elds dcan easily be on the order of 10
2oreven 103, for example, in medical diagnosticapplications. Who could be expected to di-gest millions of records, each having tens orhundreds of elds? We believe that this job is
certainly not one for humans; hence, analysiswork needs to be automated, at least partially.
The need to scale up human analysis capa-bilities to handling the large number of bytesthat we can collect is both economic and sci-entic. Businesses use data to gain competi-tive advantage, increase efciency, and pro-
vide more valuable services to customers.
Data we capture about our environment are
the basic evidence we use to build theoriesand models of the universe we live in. Be-cause computers have enabled humans togather more data than we can digest, it is on-ly natural to turn to computational tech-
niques to help us unearth meaningful pat-
terns and structures from the massive
volumes of data. Hence, KDD is an attempt toaddress a problem that the digital informa-tion era made a fact of life for all of us: dataoverload. Data Mining and KnowledgeDiscovery in the Real World
A large degree of the current interest in KDDis the result of the media interest surroundingsuccessful KDD applications, for example, thefocus articles within the last two years inBusiness Week
, Newsweek, Byte, PC Week
, andother large-circulation periodicals. Unfortu-
nately, it is not always easy to separate fact
from media hype. Nonetheless, several well-documented examples of successful systemscan rightly be referred to as KDD applicationsand have been deployed in operational useon large-scale real-world problems in scienceand in business.In science, one of the primary application
There is an
urgent needfor a new generation ofcomputation-al theoriesand tools toassist humans inextractinguseful information
(knowledge)from the
rapidly growing 
volumes of digital data. Articles
38AI MAGAZINE
tive applications (Manago and Auriol 1996).Telecommunications: 
The telecommuni-
cations alarm-sequence analyzer (TASA
) wasbuilt in cooperation with a manufacturer of

telecommunications equipment and three

telephone networks (Mannila, Toivonen, and

Verkamo 1995). The system uses a novel

framework for locating frequently occurring
alarm episodes from the alarm stream and
presenting them as rules. Large sets of discov-
ered rules can be explored with exible infor-
mation-retrieval tools supporting interactivity
and iteration. In this way, 
TASA
offers pruning,grouping, and ordering tools to rene the re-sults of a basic brute-force search for rules.
Data cleaning: The MERGE-PURGEsystemwas applied to the identication of duplicate
welfare claims (Hernandez and Stolfo 1995).
It was used successfully on data from the Wel-

fare Department of the State of Washington.
In other areas, a well-publicized system is
IBMs 
ADVANCEDSCOUT
,a specialized data-min-
ing system that helps National Basketball As-
sociation (NBA) coaches organize and inter-

pret data from NBA games (U.S. News 1995).ADVANCEDSCOUT
was used by several of theNBA teams in 1996, including the Seattle Su-personics, which reached the NBA nals.Finally, a novel and increasingly important
type of discovery is one based on the use of in-

telligent agents to navigate through an infor-
mation-rich environment. Although the idea

of active triggers has long been analyzed in the
database eld, really successful applications of
this idea appeared only with the advent of the
Internet. These systems ask the user to specify
a prole of interest and search for related in-

formation among a wide variety of public-do-
main and proprietary sources. For example,
FIREFLY
is a personal music-recommendation
agent: It asks a user his/her opinion of several
music pieces and then suggests other music
that the user might like (<http://
www.fy.com/>). 
CRAYON
(http://crayon.net/>)allows users to create their own free newspaper
(supported by ads); NEWSHOUND(<http://www.
sjmercury.com/hound/>) from the 
San JoseMercury News
and FARCAST
(<http://www.far-
cast.com/> automatically search information

from a wide variety of sources, including

newspapers and wire services, and e-mail rele-

vant documents directly to the user.
These are just a few of the numerous suchsystems that use KDD techniques to automat-
ically produce useful information from large
masses of raw data. See Piatetsky-Shapiro et

al. (1996) for an overview of issues in devel-

oping industrial KDD applications.Data Mining and KDDHistorically, the notion of nding useful pat-
terns in data has been given a variety ofnames, including data mining, knowledge ex-
traction, information discovery, information

harvesting, data archaeology, and data pattern

processing. The term data mininghas mostlybeen used by statisticians, data analysts, and
the management information systems (MIS)
communities. It has also gained popularity in
the database eld. The phrase knowledge dis-covery in databases 
was coined at the rst KDDworkshop in 1989 (Piatetsky-Shapiro 1991) toemphasize that knowledge is the end product
of a data-driven discovery. It has been popular-

ized in the AI and machine-learning elds.In our view, KDD refers to the overall pro-
cess of discovering useful knowledge from da-
ta, and data mining refers to a particular step
in this process. Data miningis the applicationof specic algorithms for extracting patterns

from data. The distinction between the KDDprocess and the data-mining step (within theprocess) is a central point of this article. The
additional steps in the KDD process, such as

data preparation, data selection, data cleaning,
incorporation of appropriate prior knowledge,
and proper interpretation of the results of
mining, are essential to ensure that useful

knowledge is derived from the data. Blind ap-
plication of data-mining methods (rightly crit-
icized as data dredging in the statistical litera-ture) can be a dangerous activity, easily
leading to the discovery of meaningless and

invalid patterns. The Interdisciplinary Nature of KDD
KDD has evolved, and continues to evolve,
from the intersection of research elds such as

machine learning, pattern recognition,

databases, statistics, AI, knowledge acquisition
for expert systems, data visualization, and
high-performance computing. The unifyinggoal is extracting high-level knowledge from
low-level data in the context of large data sets.The data-mining component of KDD cur-
rently relies heavily on known techniques
from machine learning, pattern recognition,
and statistics to nd patterns from data in the
data-mining step of the KDD process. A natu-
ral question is, How is KDD different from pat-
tern recognition or machine learning (and re-
lated elds)? The answer is that these eldsprovide some of the data-mining methodsthat are used in the data-mining step of the
KDD process. KDD focuses on the overall pro-
cess of knowledge discovery from data, includ-

ing how the data are stored and accessed, how

algorithms can be scaled to massive data setsThe basicproblem 

addressed by

the KDD 
process is 

one of 
mapping 
low-level 
data  into
other forms

that might be
more 

compact,
more 

abstract, 
or more 

useful. Articles
FALL 1996   39
A driving force behind KDD is the database
eld (the second D in KDD). Indeed, the
problem of effective data manipulation when
data cannot t in the main memory is of fun-

damental importance to KDD. Database tech-niques for gaining efcient data access,grouping and ordering operations when ac-
cessing data, and optimizing queries consti-

tute the basics for scaling algorithms to largerdata sets. Most data-mining algorithms fromstatistics, pattern recognition, and machine

learning assume data are in the main memo-
ry and pay no attention to how the algorithm

breaks down if only limited views of the dataare possible.A related eld evolving from databases is
data warehousing,
which refers to the popularbusiness trend of collecting and cleaningtransactional data to make them available foronline analysis and decision support. Data

warehousing helps set the stage for KDD in
two important ways: (1) data cleaning and (2)
data access.Data cleaning: As organizations are forced
to think about a unied logical view of the
wide variety of data and databases they pos-
sess, they have to address the issues of map-
ping data to a single naming convention,uniformly representing and handling missingdata, and handling noise and errors when

possible.Data access: Uniform and well-denedmethods must be created for accessing the da-ta and providing access paths to data that
were historically difcult to get to (for exam-
ple, stored ofine).Once organizations and individuals havesolved the problem of how to store and ac-cess their data, the natural next step is the
question, What else do we do with all the da-
ta? This is where opportunities for KDD natu-rally arise.A popular approach for analysis of datawarehouses is called online analytical processing
(OLAP), named for a set of principles pro-

posed by Codd (1993). OLAP tools focus onproviding multidimensional data analysis,which is superior to SQLin computing sum-maries and breakdowns along many dimen-
sions. OLAP tools are targeted toward simpli-

fying and supporting interactive data analysis,
but the goal of KDD tools is to automate asmuch of the process as possible. Thus, KDD is

a step beyond what is currently supported by

most standard database systems.
Basic DenitionsKDD is the nontrivial process of identifying
valid, novel, potentially useful, and ultimate-and still run efciently, how results can be in-
terpreted and visualized, and how the overallman-machine interaction can usefully bemodeled and supported. The KDD processcan be viewed as a multidisciplinary activity
that encompasses techniques beyond thescope of any one particular discipline such asmachine learning. In this context, there areclear opportunities for other elds of AI (be-sides machine learning) to contribute toKDD. KDD places a special emphasis on nd-ing understandable patterns that can be inter-preted as useful or interesting knowledge.
Thus, for example, neural networks, althougha powerful modeling tool, are relatively
difcult to understand compared to decisiontrees. KDD also emphasizes scaling and ro-

bustness properties of modeling algorithmsfor large noisy data sets. Related AI research elds include machine
discovery, which targets the discovery of em-
pirical laws from observation and experimen-
tation (Shrager and Langley 1990) (see Kloes-gen and Zytkow [1996] for a glossary of terms
common to KDD and machine discovery),
and causal modeling for the inference of
causal models from data (Spirtes, Glymour,
and Scheines 1993). Statistics in particularhas much in common with KDD (see Elderand Pregibon [1996] and Glymour et al.[1996] for a more detailed discussion of thissynergy). Knowledge discovery from data is
fundamentally a statistical endeavor. Statistics
provides a language and framework for quan-tifying the uncertainty that results when onetries to infer general patterns from a particu-lar sample of an overall population. As men-tioned earlier, the term 
data mininghas hadnegative connotations in statistics since the1960s when computer-based data analysis
techniques were rst introduced. The concernarose because if one searches long enough in
any data set (even randomly generated data),one can nd patterns that appear to be statis-tically signicant but, in fact, are not. Clearly,
this issue is of fundamental importance to
KDD. Substantial progress has been made in
recent years in understanding such issues instatistics. Much of this work is of direct rele-vance to KDD. Thus, data mining is a legiti-mate activity as long as one understands howto do it correctly; data mining carried outpoorly (without regard to the statistical as-pects of the problem) is to be avoided. KDDcan also be viewed as encompassing a broaderview of modeling than statistics. KDD aims toprovide tools to automate (to the degree pos-sible) the entire process of data analysis andthe statisticians art of hypothesis selection.
Data miningis a step inthe KDD process that
consists of ap-plying dataanalysis anddiscovery al-
gorithms thatproduce a par-
ticular enu-meration ofpatterns 
(or models)over the data. Articles
40AI MAGAZINE
ly understandable patterns in data (Fayyad,Piatetsky-Shapiro, and Smyth 1996).Here, dataare a set of facts (for example,cases in a database), and pattern
is an expres-sion in some language describing a subset ofthe data or a model applicable to the subset.
Hence, in our usage here, extracting a patternalso designates tting a model to data; nd-
ing structure from data; or, in general, mak-

ing any high-level description of a set of data.The term process 
implies that KDD comprisesmany steps, which involve data preparation,
search for patterns, knowledge evaluation,
and renement, all repeated in multiple itera-tions. By nontrivial, we mean that somesearch or inference is involved; that is, it is
not a straightforward computation of

predened quantities like computing the av-erage value of a set of numbers.The discovered patterns should be valid onnew data with some degree of certainty. We
also want patterns to be novel (at least to thesystem and preferably to the user) and poten-
tially useful, that is, lead to some benet tothe user or task. Finally, the patterns should
be understandable, if not immediately then
after some postprocessing. The previous discussion implies that we candene quantitative measures for evaluating

extracted patterns. In many cases, it is possi-
ble to dene measures of certainty (for exam-
ple, estimated prediction accuracy on newdata) or utility (for example, gain, perhaps in
dollars saved because of better predictions or
speedup in response time of a system). No-

tions such as novelty and understandabilityare much more subjective. In certain contexts,
understandability can be estimated by sim-

plicity (for example, the number of bits to de-
scribe a pattern). An important notion, called
interestingness
(for example, see Silberschatzand Tuzhilin [1995] and Piatetsky-Shapiro and
Matheus [1994]), is usually taken as an overallmeasure of pattern value, combining validity,

novelty, usefulness, and simplicity. Interest-
ingness functions can be dened explicitly orcan be manifested implicitly through an or-
dering placed by the KDD system on the dis-
covered patterns or models. Given these notions, we can consider apattern
to be knowledge if it exceeds some in-terestingness threshold, which is by no

means an attempt to dene knowledge in thephilosophical or even the popular view. As a
matter of fact, knowledge in this denition is
purely user oriented and domain specic andis determined by whatever functions andthresholds the user chooses.Data mining is a step in the KDD processthat consists of applying data analysis anddiscovery algorithms that, under acceptable

computational efciency limitations, pro-
duce a particular enumeration of patterns (ormodels) over the data. Note that the space ofArticles
FALL 1996   41
DataT
DataPatternsPreprocessingData MiningEvaluationTransformation
Selection---------
---------
---------
KnowledgePreprocessed DataTarget Date
Figure 1. An Overview of the Steps That Compose the KDD Process.
methods, the effective number of variablesunder consideration can be reduced, or in-
variant representations for the data can befound.Fifth is matching the goals of the KDD pro-cess (step 1) to a particular data-mining
method. For example, summarization, clas-

sication, regression, clustering, and so on,

are described later as well as in Fayyad, Piatet-
sky-Shapiro, and Smyth (1996).Sixth is exploratory analysis and model
and hypothesis selection: choosing the data-
mining algorithm(s) and selecting method(s)
to be used for searching for data patterns.
This process includes deciding which modelsand parameters might be appropriate (for ex-
ample, models of categorical data are differ-

ent than models of vectors over the reals) and
matching a particular data-mining method
with the overall criteria of the KDD process

(for example, the end user might be more in-
terested in understanding the model than its
predictive capabilities).Seventh is data mining: searching for pat-
terns of interest in a particular representa-
tional form or a set of such representations,
including classication rules or trees, regres-sion, and clustering. The user can signicant-ly aid the data-mining method by correctly

performing the preceding steps.Eighth is interpreting mined patterns, pos-sibly returning to any of steps 1 through 7 for
further iteration. This step can also involve
visualization of the extracted patterns and

models or visualization of the data given the
extracted models.Ninth is acting on the discovered knowl-edge: using the knowledge directly, incorpo-

rating the knowledge into another system forfurther action, or simply documenting it andreporting it to interested parties. This process
also includes checking for and resolving po-

tential conicts with previously believed (or

extracted) knowledge.The KDD process can involve signicantiteration and can contain loops between
any two steps. The basic ow of steps (al-

though not the potential multitude of itera-

tions and loops) is illustrated in gure 1.

Most previous work on KDD has focused on
step 7, the data mining. However, the other

steps are as important (and probably moreso) for the successful application of KDD inpractice. Having dened the basic notions
and introduced the KDD process, we now
focus on the data-mining component,
which has, by far, received the most atten-

tion in the literature.
patterns is often innite, and the enumera-
tion of patterns involves some form of
search in this space. Practical computational
constraints place severe limits on the sub-

space that can be explored by a data-mining
algorithm.The KDD process involves using the
database along with any required selection,
preprocessing, subsampling, and transforma-
tions of it; applying data-mining methods
(algorithms) to enumerate patterns from it;

and evaluating the products of data miningto identify the subset of the enumerated pat-terns deemed knowledge. The data-mining
component of the KDD process is concerned
with the algorithmic means by which pat-

terns are extracted and enumerated from da-
ta. The overall KDD process (gure 1) in-
cludes the evaluation and possible
interpretation of the mined patterns to de-

termine which patterns can be considered

new knowledge. The KDD process also in-cludes all the additional steps described inthe next section. The notion of an overall user-driven pro-
cess is not unique to KDD: analogous propos-
als have been put forward both in statistics
(Hand 1994) and in machine learning (Brod-
ley and Smyth 1996).The KDD ProcessThe KDD process is interactive and iterative,involving numerous steps with many deci-sions made by the user. Brachman and Anand

(1996) give a practical view of the KDD pro-
cess, emphasizing the interactive nature of

the process. Here, we broadly outline some of
its basic steps:First is developing an understanding of theapplication domain and the relevant prior

knowledge and identifying the goal of the
KDD process from the customers viewpoint.
Second is creating a target data set: select-ing a data set, or focusing on a subset of vari-
ables or data samples, on which discovery is

to be performed.Third is data cleaning and preprocessing.
Basic operations include removing noise if

appropriate, collecting the necessary informa-

tion to model or account for noise, deciding
on strategies for handling missing data elds,
and accounting for time-sequence informa-
tion and known changes.Fourth is data reduction and projection:
nding useful features to represent the data

depending on the goal of the task. With di-

mensionality reduction or transformationArticles
42AI MAGAZINE
The Data-Mining Step of the KDD ProcessThe data-mining component of the KDD pro-cess often involves repeated iterative applica-
tion of particular data-mining methods. This
section presents an overview of the primary

goals of data mining, a description of the
methods used to address these goals, and abrief description of the data-mining algo-
rithms that incorporate these methods.The knowledge discovery goals are dened
by the intended use of the system. We can

distinguish two types of goals: (1) verication
and (2) discovery. With 
verication,the sys-tem is limited to verifying the users hypothe-

sis. With 
discovery,
the system autonomouslynds new patterns. We further subdivide the
discovery goal into 
prediction,
where the sys-tem nds patterns for predicting the future

behavior of some entities, and description,where the system nds patterns for presenta-
tion to a user in a human-understandable
form. In this article, we are primarily con-
cerned with discovery-oriented data mining. 
Data mining involves tting models to, ordetermining patterns from, observed data.
The tted models play the role of inferredknowledge: Whether the models reect useful
or interesting knowledge is part of the over-

all, interactive KDD process where subjective
human judgment is typically required. Two

primary mathematical formalisms are used in

model tting: (1) statistical and (2) logical.
The statistical approach
allows for nondeter-ministic effects in the model, whereas a logi-cal modelis purely deterministic. We focus
primarily on the statistical approach to data

mining, which tends to be the most widely
used basis for practical data-mining applica-
tions given the typical presence of uncertain-
ty in real-world data-generating processes. Most data-mining methods are based ontried and tested techniques from machine

learning, pattern recognition, and statistics:classication, clustering, regression, and soon. The array of different algorithms under
each of these headings can often be bewilder-
ing to both the novice and the experienced
data analyst. It should be emphasized that of
the many data-mining methods advertised in
the literature, there are really only a few fun-
damental techniques. The actual underlyingmodel representation being used by a particu-lar method typically comes from a composi-

tion of a small number of well-known op-
tions: polynomials, splines, kernel and basis

functions, threshold-Boolean functions, and
so on. Thus, algorithms tend to differ primar-ily in the goodness-of-t criterion used toevaluate model t or in the search method
used to nd a good t. In our brief overview of data-mining meth-
ods, we try in particular to convey the notion

that most (if not all) methods can be viewed
as extensions or hybrids of a few basic tech-
niques and principles. We rst discuss the pri-

mary methods of data mining and then show

that the data- mining methods can be viewed
as consisting of three primary algorithmic
components: (1) model representation, (2)
model evaluation, and (3) search. In the dis-

cussion of KDD and data-mining methods,
we use a simple example to make some of the
notions more concrete. Figure 2 shows a sim-
ple two-dimensional articial data set consist-
ing of 23 cases. Each point on the graph rep-
resents a person who has been given a loan
by a particular bank at some time in the past.
The horizontal axis represents the income ofthe person; the vertical axis represents the to-tal personal debt of the person (mortgage, car
payments, and so on). The data have been
classied into two classes: (1) the xs repre-

sent persons who have defaulted on their
loans and (2) the os represent persons whose

loans are in good status with the bank. Thus,
this simple articial data set could represent a
historical data set that can contain useful
knowledge from the point of view of thebank making the loans. Note that in actualKDD applications, there are typically many

more dimensions (as many as several hun-

dreds) and many more data points (many
thousands or even millions).Articles
FALL 1996   43
xxxxooooIncomeDebtoxooooooxxxxxooFigure 2. A Simple Data Set with Two Classes Used for Illustrative Purposes.
The purpose here is to illustrate basic ideason a small problem in two-dimensional
space.Data-Mining MethodsThe two high-level primary goals of data min-
ing in practice tend to be prediction and de-scription. As stated earlier, prediction in-

volves using some variables or elds in the
database to predict unknown or future valuesof other variables of interest, and descriptionfocuses on nding human-interpretable pat-

terns describing the data. Although the
boundaries between prediction and descrip-

tion are not sharp (some of the predictivemodels can be descriptive, to the degree thatthey are understandable, and vice versa), the
distinction is useful for understanding the
overall discovery goal. The relative impor-
tance of prediction and description for partic-ular data-mining applications can vary con-

siderably. The goals of prediction and

description can be achieved using a variety of
particular data-mining methods. Classicationis learning a function thatmaps (classies) a data item into one of sever-
al predened classes (Weiss and Kulikowski

1991; Hand 1981). Examples of classication
methods used as part of knowledge discovery
applications include the classifying of trendsin nancial markets (Apte and Hong 1996)
and the automated identication of objects of
interest in large image databases (Fayyad,Djorgovski, and Weir 1996). Figure 3 shows a
simple partitioning of the loan data into two
class regions; note that it is not possible to
separate the classes perfectly using a linear
decision boundary. The bank might want to
use the classication regions to automaticallydecide whether future loan applicants will be
given a loan or not.Regression
is learning a function that mapsa data item to a real-valued prediction vari-able. Regression applications are many, for

example, predicting the amount of biomass

present in a forest given remotely sensed mi-
crowave measurements, estimating the proba-bility that a patient will survive given the re-
sults of a set of diagnostic tests, predicting
consumer demand for a new product as a

function of advertising expenditure, and pre-
dicting time series where the input variablescan be time-lagged versions of the predictionvariable. Figure 4 shows the result of simple
linear regression where total debt is tted as a
linear function of income: The t is poor be-cause only a weak correlation exists betweenthe two variables.Clusteringis a common descriptive taskArticles
44AI MAGAZINE
Figure 3. A Simple Linear Classication Boundary for the Loan Data Set.
The shaped region denotes class no loan.xxxxooooIncomeDebtoxooooooxxxxxoNo LoanLoanoFigure 4. A Simple Linear Regression for the Loan Data Set.
xxxxooooIncomeDebtoxooooooxxxxxooRegression Linewhere one seeks to identify a nite set of cat-
egories or clusters to describe the data (Jainand Dubes 1988; Titterington, Smith, and
Makov 1985). The categories can be mutuallyexclusive and exhaustive or consist of a richerrepresentation, such as hierarchical or over-
lapping categories. Examples of clustering ap-plications in a knowledge discovery context
include discovering homogeneous subpopula-tions for consumers in marketing databasesand identifying subcategories of spectra frominfrared sky measurements (Cheeseman andStutz 1996). Figure 5 shows a possible cluster-ing of the loan data set into three clusters;
note that the clusters overlap, allowing datapoints to belong to more than one cluster.
The original class labels (denoted by xs and

os in the previous gures) have been replaced
by a + to indicate that the class membershipis no longer assumed known. Closely relatedto clustering is the task of probability density
estimation,which consists of techniques forestimating from data the joint multivariateprobability density function of all the vari-ables or elds in the database (Silverman
1986).Summarizationinvolves methods for nd-
ing a compact description for a subset of da-ta. A simple example would be tabulating themean and standard deviations for all elds.
More sophisticated methods involve thederivation of summary rules (Agrawal et al.
1996), multivariate visualization techniques,and the discovery of functional relationships
between variables (Zembowicz and Zytkow1996). Summarization techniques are often
applied to interactive exploratory data analy-
sis and automated report generation.Dependency modelingconsists of nding amodel that describes signicant dependenciesbetween variables. Dependency models existat two levels: (1) the structural level
of themodel species (often in graphic form) whichvariables are locally dependent on each otherand (2) the quantitative level of the modelspecies the strengths of the dependenciesusing some numeric scale. For example, prob-abilistic dependency networks use condition-al independence to specify the structural as-
pect of the model and probabilities or
correlations to specify the strengths of the de-pendencies (Glymour et al. 1987; Heckerman1996). Probabilistic dependency networks areincreasingly nding applications in areas asdiverse as the development of probabilistic
medical expert systems from databases, infor-mation retrieval, and modeling of the humangenome.Change and deviation detectionfocuses ondiscovering the most signicant changes inthe data from previously measured or norma-tive values (Berndt and Clifford 1996; Guyon,Matic, and Vapnik 1996; Kloesgen 1996;
Matheus, Piatetsky-Shapiro, and McNeill
1996; Basseville and Nikiforov 1993).The Components of Data-Mining AlgorithmsThe next step is to construct specic algo-rithms to implement the general methods weoutlined. One can identify three primary
components in any data-mining algorithm:(1) model representation, (2) model evalua-

tion, and (3) search.
This reductionist view is not necessarilycomplete or fully encompassing; rather, it is a
convenient way to express the key concepts
of data-mining algorithms in a relatively

unied and compact manner. Cheeseman
(1990) outlines a similar structure.Model representation
is the language used todescribe discoverable patterns. If the repre-
sentation is too limited, then no amount of
training time or examples can produce an ac-curate model for the data. It is important thata data analyst fully comprehend the represen-
tational assumptions that might be inherent
in a particular method. It is equally impor-tant that an algorithm designer clearly statewhich representational assumptions are beingmade by a particular algorithm. Note that in-
creased representational power for models in-creases the danger of overtting the trainingdata, resulting in reduced prediction accuracyon unseen data.Model-evaluation criteriaare quantitativeArticles
FALL 1996   45
++++++++IncomeDebt+++++++++++++++Cluster 2Cluster 3Cluster 1Figure 5. A Simple Clustering of the Loan Data Set into Three Clusters.
Note that original labels are replaced by a +.Decision Trees and Rules
Decision trees and rules that use univariatesplits have a simple representational form,

making the inferred model relatively easy for
the user to comprehend. However, the restric-

tion to a particular tree or rule representation
can signicantly restrict the functional form
(and, thus, the approximation power) of themodel. For example, gure 6 illustrates the ef-
fect of a threshold split applied to the income
variable for a loan data set: It is clear that us-
ing such simple threshold splits (parallel to
the feature axes) severely limits the type ofclassication boundaries that can be induced.
If one enlarges the model space to allow more
general expressions (such as multivariate hy-
perplanes at arbitrary angles), then the model
is more powerful for prediction but can bemuch more difcult to comprehend. A large
number of decision tree and rule-induction

algorithms are described in the machine-
learning and applied statistics literature
(Quinlan 1992; Breiman et al. 1984).To a large extent, they depend on likeli-
hood-based model-evaluation methods, with
varying degrees of sophistication in terms of

penalizing model complexity. Greedy search

methods, which involve growing and prun-ing rule and tree structures, are typically usedto explore the superexponential space of pos-
sible models. Trees and rules are primarily

used for predictive modeling, both for clas-
sication (Apte and Hong 1996; Fayyad, Djor-
govski, and Weir 1996) and regression, al-
though they can also be applied to summary
descriptive modeling (Agrawal et al. 1996).Nonlinear Regression and Classication MethodsThese methods consist of a family of tech-
niques for prediction that t linear and non-linear combinations of basis functions (sig-
moids, splines, polynomials) to combinations
of the input variables. Examples include feed-
forward neural networks, adaptive spline
methods, and projection pursuit regression(see Elder and Pregibon [1996], Cheng and

Titterington [1994], and Friedman [1989] for

more detailed discussions). Consider neural

networks, for example. Figure 7 illustrates the
type of nonlinear decision boundary that aneural network might nd for the loan dataset. In terms of model evaluation, although
networks of the appropriate size can univer-

sally approximate any smooth function to
any desired degree of accuracy, relatively little
is known about the representation propertiesof xed-size networks estimated from nite
data sets. Also, the standard squared error andstatements (or t functions) of how well a par-ticular pattern (a model and its parameters)meets the goals of the KDD process. For ex-ample, predictive models are often judged by
the empirical prediction accuracy on some
test set. Descriptive models can be evaluated
along the dimensions of predictive accuracy,

novelty, utility, and understandability of the

tted model. Search method
consists of two components:(1) parameter search and (2) model search.
Once the model representation (or family of
representations) and the model-evaluation
criteria are xed, then the data-mining prob-
lem has been reduced to purely an optimiza-
tion task: Find the parameters and models
from the selected family that optimize the
evaluation criteria. In parameter search, the
algorithm must search for the parameters
that optimize the model-evaluation criteria
given observed data and a xed model repre-

sentation. Model search occurs as a loop over

the parameter-search method: The model rep-

resentation is changed so that a family of
models is considered. Some Data-Mining MethodsA wide variety of data-mining methods exist,but here, we only focus on a subset of popu-lar techniques. Each method is discussed in
the context of model representation, model

evaluation, and search.
Articles
46AI MAGAZINE
xxxxooooIncomeDebtoxooooooxxxxxoNo LoanLoanotFigure 6. Using a Single Threshold on the Income Variable to 
Try to Classify the Loan Data Set.
cross-entropy loss functions used to trainneural networks can be viewed as log-likeli-

hood functions for regression and

classication, respectively (Ripley 1994; Ge-
man, Bienenstock, and Doursat 1992). Backpropagation is a parameter-search method
that performs gradient descent in parameter
(weight) space to nd a local maximum of
the likelihood function starting from randominitial conditions. Nonlinear regression meth-ods, although powerful in representational

power, can be difcult to interpret.
For example, although the classicationboundaries of gure 7 might be more accu-rate than the simple threshold boundary of
gure 6, the threshold boundary has the ad-

vantage that the model can be expressed, to
some degree of certainty, as a simple rule of
the form if income is greater than threshold,then loan will have good status.Example-Based MethodsThe representation is simple: Use representa-
tive examples from the database to approxi-
mate a model; that is, predictions on new ex-amples are derived from the properties of
similar examples in the model whose predic-
tion is known. Techniques include nearest-

neighbor classication and regression algo-rithms (Dasarathy 1991) and case-basedreasoning systems (Kolodner 1993). Figure 8

illustrates the use of a nearest-neighbor clas-
sier for the loan data set: The class at anynew point in the two-dimensional space isthe same as the class of the closest point in
the original training data set.A potential disadvantage of example-basedmethods (compared with tree-based methods)
is that a well-dened distance metric for eval-
uating the distance between data points is re-

quired. For the loan data in gure 8, this

would not be a problem because income and
debt are measured in the same units. Howev-
er, if one wished to include variables such as

the duration of the loan, sex, and profession,

then it would require more effort to dene a

sensible metric between the variables. Modelevaluation is typically based on cross-valida-
tion estimates (Weiss and Kulikowski 1991) of

a prediction error: Parameters of the model to
be estimated can include the number of
neighbors to use for prediction and the dis-tance metric itself. Like nonlinear regression
methods, example-based methods are often

asymptotically powerful in terms of approxi-
mation properties but, conversely, can be
difcult to interpret because the model is im-
plicit in the data and not explicitly formulat-

ed. Related techniques include kernel-density
Articles
FALL 1996   47
xxxxooooIncomeDebtoxooooooxxxxxoNo LoanLoanoFigure 7. An Example of Classication Boundaries Learned by a Nonlinear
Classier (Such as a Neural Network) for the Loan Data Set.xxxxooooIncomeDebtoxooooooxxxxxoNo LoanLoanoFigure 8. Classication Boundaries for a Nearest-Neighbor 
Classier for the Loan Data Set.evitably limited in scope; many data-miningtechniques, particularly specialized methodsfor particular types of data and domains, werenot mentioned specically. We believe the
general discussion on data-mining tasks andcomponents has general relevance to a vari-
ety of methods. For example, consider time-series prediction, which traditionally hasbeen cast as a predictive regression task (au-toregressive models, and so on). Recently,
more general models have been developed fortime-series applications, such as nonlinear ba-sis functions, example-based models, and ker-nel methods. Furthermore, there has beensignicant interest in descriptive graphic andlocal data modeling of time series rather thanpurely predictive modeling (Weigend and

Gershenfeld 1993). Thus, although different
algorithms and applications might appear dif-ferent on the surface, it is not uncommon tond that they share many common compo-nents. Understanding data mining and modelinduction at this component level clariesthe behavior of any data-mining algorithmand makes it easier for the user to understandits overall contribution and applicability tothe KDD process.An important point is that each techniquetypically suits some problems better thanothers. For example, decision tree classiers
can be useful for nding structure in high-di-mensional spaces and in problems withmixed continuous and categorical data (be-cause tree methods do not require distance
metrics). However, classication trees might
not be suitable for problems where the truedecision boundaries between classes are de-
scribed by a second-order polynomial (for ex-
ample). Thus, there is no universal data-min-ing method, and choosing a particular
algorithm for a particular application is some-thing of an art. In practice, a large portion ofthe application effort can go into properly
formulating the problem (asking the rightquestion) rather than into optimizing the al-gorithmic details of a particular data-mining
method (Langley and Simon 1995; Hand1994).Because our discussion and overview of da-
ta-mining methods has been brief, we wantto make two important points clear:First, our overview of automated search fo-
cused mainly on automated methods for ex-
tracting patterns or models from data. Al-
though this approach is consistent with thedenition we gave earlier, it does not neces-
sarily represent what other communitiesmight refer to as data mining. For example,some use the term to designate any manualestimation (Silverman 1986) and mixture
modeling (Titterington, Smith, and Makov

1985).Probabilistic Graphic Dependency ModelsGraphic models specify probabilistic depen-dencies using a graph structure (Whittaker
1990; Pearl 1988). In its simplest form, the

model species which variables are directly de-
pendent on each other. Typically, these mod-

els are used with categorical or discrete-valuedvariables, but extensions to special cases, suchas Gaussian densities, for real-valued variables
are also possible. Within the AI and statistical

communities, these models were initially de-
veloped within the framework of probabilistic
expert systems; the structure of the model and
the parameters (the conditional probabilities
attached to the links of the graph) were elicit-
ed from experts. Recently, there has been sig-

nicant work in both the AI and statistical
communities on methods whereby both the

structure and the parameters of graphic mod-
els can be learned directly from databases
(Buntine 1996; Heckerman 1996). Model-eval-uation criteria are typically Bayesian in form,
and parameter estimation can be a mixture of
closed-form estimates and iterative methods

depending on whether a variable is directly

observed or hidden. Model search can consist

of greedy hill-climbing methods over various
graph structures. Prior knowledge, such as a
partial ordering of the variables based on
causal relations, can be useful in terms of re-
ducing the model search space. Although still

primarily in the research phase, graphic model
induction methods are of particular interest toKDD because the graphic form of the model

lends itself easily to human interpretation.Relational Learning ModelsAlthough decision trees and rules have a repre-
sentation restricted to propositional logic, rela-
tional learning
(also known as inductive logicprogramming
) uses the more exible pattern
language of rst-order logic. A relational learn-
er can easily nd formulas such as X= Y. Mostresearch to date on model-evaluation methods

for relational learning is logical in nature. Theextra representational power of relational
models comes at the price of signicant com-
putational demands in terms of search. See

Dzeroski (1996) for a more detailed discussion.DiscussionGiven the broad spectrum of data-mining
methods and algorithms, our overview is in-
Understand-ing data mining andmodel induction atthis componentlevel clariesthe behaviorof any data-miningalgorithmand makes iteasier for theuser to understand itsoverall contributionand applicabilityto the KDD process.
Articles
48AI MAGAZINE
search of the data or search assisted by queries
to a database management system or to refer
to humans visualizing patterns in data. In
other communities, it is used to refer to the

automated correlation of data from transac-
tions or the automated generation of transac-

tion reports. We choose to focus only on

methods that contain certain degrees of
search autonomy.
Second, beware the hype: The state of theart in automated methods in data mining is
still in a fairly early stage of development.There are no established criteria for decidingwhich methods to use in which circum-

stances, and many of the approaches are
based on crude heuristic approximations to

avoid the expensive search required to nd

optimal, or even good, solutions. Hence, the
reader should be careful when confronted

with overstated claims about the great ability
of a system to mine useful information from
large (or even small) databases.Application IssuesFor a survey of KDD applications as well as

detailed examples, see Piatetsky-Shapiro et al.
(1996) for industrial applications and Fayyad,
Haussler, and Stolorz (1996) for applications

in science data analysis. Here, we examine

criteria for selecting potential applications,
which can be divided into practical and tech-
nical categories. The practical criteria for KDD
projects are similar to those for other applica-tions of advanced technology and include thepotential impact of an application, the ab-

sence of simpler alternative solutions, and
strong organizational support for using tech-
nology. For applications dealing with person-

al data, one should also consider the privacy
and legal issues (Piatetsky-Shapiro 1995). The technical criteria include considera-
tions such as the availability of sufcient data
(cases). In general, the more elds there areand the more complex the patterns beingsought, the more data are needed. However,

strong prior knowledge (see discussion later)
can reduce the number of needed cases sig-

nicantly. Another consideration is the rele-

vance of attributes. It is important to have da-
ta attributes that are relevant to the discovery

task; no amount of data will allow prediction
based on attributes that do not capture the
required information. Furthermore, low noiselevels (few data errors) are another considera-tion. High amounts of noise make it hard to
identify patterns unless a large number of cas-
es can mitigate random noise and help clarify
the aggregate patterns. Changing and time-oriented data, although making the applica-tion development more difcult, make it po-
tentially much more useful because it is easier
to retrain a system than a human. Finally,

and perhaps one of the most important con-siderations, is prior knowledge. It is useful toknow something about the domain what
are the important elds, what are the likely
relationships, what is the user utility func-
tion, what patterns are already known, and soon. Research and Application Challenges
We outline some of the current primary re-

search and application challenges for KDD.
This list is by no means exhaustive and is in-tended to give the reader a feel for the types
of problem that KDD practitioners wrestle

with. Larger databases: Databases with hun-dreds of elds and tables and millions of
records and of a multigigabyte size are com-
monplace, and terabyte (1012bytes) databasesare beginning to appear. Methods for dealing
with large data volumes include more
efcient algorithms (Agrawal et al. 1996),
sampling, approximation, and massively par-
allel processing (Holsheimer et al. 1996).High dimensionality: Not only is there of-ten a large number of records in the database,but there can also be a large number of elds
(attributes, variables); so, the dimensionality
of the problem is high. A high-dimensionaldata set creates problems in terms of increas-ing the size of the search space for model in-

duction in a combinatorially explosive man-
ner. In addition, it increases the chances that

a data-mining algorithm will nd spuriouspatterns that are not valid in general. Ap-proaches to this problem include methods to
reduce the effective dimensionality of the

problem and the use of prior knowledge toidentify irrelevant variables.Overtting: When the algorithm searches
for the best parameters for one particular

model using a limited set of data, it can mod-
el not only the general patterns in the databut also any noise specic to the data set, re-sulting in poor performance of the model on
test data. Possible solutions include cross-vali-
dation, regularization, and other sophisticat-

ed statistical strategies.Assessing of statistical signicance: Aproblem (related to overtting) occurs when
the system is searching over many possible

models. For example, if a system tests modelsat the 0.001 signicance level, then on aver-age, with purely random data, N/1000 ofthese models will be accepted as signicant.Articles
FALL 1996   49
edge is important in all the steps of the KDDprocess. Bayesian approaches (for example,
Cheeseman [1990]) use prior probabilitiesover data and distributions as one form of en-coding prior knowledge. Others employ de-

ductive database capabilities to discoverknowledge that is then used to guide the da-ta-mining search (for example, Simoudis,

Livezey, and Kerber [1995]).
Integration with other systems:A stand-alone discovery system might not be very
useful. Typical integration issues include inte-
gration with a database management system
(for example, through a query interface), in-
tegration with spreadsheets and visualizationtools, and accommodating of real-time sensorreadings. Examples of integrated KDD sys-
tems are described by Simoudis, Livezey, and

Kerber (1995) and Stolorz, Nakamura, Mesro-biam, Muntz, Shek, Santos, Yi, Ng, Chien,
Mechoso, and Farrara (1995). Concluding Remarks: The Potential Role of AI in KDDIn addition to machine learning, other AI el-ds can potentially contribute signicantly tovarious aspects of the KDD process. We men-

tion a few examples of these areas here:Natural languagepresents signicant op-
portunities for mining in free-form text, espe-cially for automated annotation and indexingprior to classication of text corpora. Limited
parsing capabilities can help substantially inthe task of deciding what an article refers to.Hence, the spectrum from simple natural lan-
guage processing all the way to language un-derstanding can help substantially. Also, nat-
ural language processing can contributesignicantly as an effective interface for stat-ing hints to mining algorithms and visualiz-
ing and explaining knowledge derived by aKDD system. Planningconsiders a complicated dataanalysis process. It involves conducting com-plicated data-access and data-transformation

operations; applying preprocessing routines;and, in some cases, paying attention to re-
source and data-access constraints. Typically,

data processing steps are expressed in terms ofdesired postconditions and preconditions for
the application of certain routines, which
lends itself easily to representation as a plan-
ning problem. In addition, planning ability
can play an important role in automatedagents (see next item) to collect data samplesor conduct a search to obtain needed data sets.
Intelligent agentscan be red off to col-lect necessary information from a variety of
This point is frequently missed by many ini-tial attempts at KDD. One way to deal withthis problem is to use methods that adjustthe test statistic as a function of the search,
for example, Bonferroni adjustments for inde-pendent tests or randomization testing.Changing data and knowledge: Rapidlychanging (nonstationary) data can make pre-
viously discovered patterns invalid. In addi-tion, the variables measured in a given appli-cation database can be modied, deleted, oraugmented with new measurements over
time. Possible solutions include incremental
methods for updating the patterns and treat-ing change as an opportunity for discovery
by using it to cue the search for patterns of
change only (Matheus, Piatetsky-Shapiro, and
McNeill 1996). See also Agrawal and Psaila(1995) and Mannila, Toivonen, and Verkamo
(1995). Missing and noisy data: This problem isespecially acute in business databases. U.S.census data reportedly have error rates asgreat as 20 percent in some elds. Important
attributes can be missing if the database wasnot designed with discovery in mind. Possible
solutions include more sophisticated statisti-
cal strategies to identify hidden variables anddependencies (Heckerman 1996; Smyth et al.1996).Complex relationships between elds:Hierarchically structured attributes or values,
relations between attributes, and more so-
phisticated means for representing knowl-
edge about the contents of a database will re-quire algorithms that can effectively use suchinformation. Historically, data-mining algo-
rithms have been developed for simple at-

tribute-value records, although new tech-
niques for deriving relations between
variables are being developed (Dzeroski 1996;Djoko, Cook, and Holder 1995).Understandability of patterns: 
In manyapplications, it is important to make the dis-coveries more understandable by humans.Possible solutions include graphic representa-tions (Buntine 1996; Heckerman 1996), rule
structuring, natural language generation, andtechniques for visualization of data andknowledge. Rule-renement strategies (for ex-ample, Major and Mangano [1995]) can beused to address a related problem: The discov-ered knowledge might be implicitly or explic-itly redundant.User interaction and prior knowledge:Many current KDD methods and tools are nottruly interactive and cannot easily incorpo-
rate prior knowledge about a problem exceptin simple ways. The use of domain knowl-
Articles
50AI MAGAZINE
sources. In addition, information agents can
be activated remotely over the network orcan trigger on the occurrence of a certain
event and start an analysis operation. Finally,

agents can help navigate and model the
World-Wide Web (Etzioni 1996), another area

growing in importance.Uncertainty in AI
includes issues for man-
aging uncertainty, proper inference mecha-

nisms in the presence of uncertainty, and the
reasoning about causality, all fundamental to
KDD theory and practice. In fact, the KDD-96

conference had a joint session with the UAI-96
conference this year (Horvitz and Jensen 1996).Knowledge representationincludes on-tologies,new concepts for representing, stor-ing, and accessing knowledge. Also included
are schemes for representing knowledge and

allowing the use of prior human knowledgeabout the underlying process by the KDD
system.These potential contributions of AI are buta sampling; many others, including human-
computer interaction, knowledge-acquisition
techniques, and the study of mechanisms for
reasoning, have the opportunity to con-

tribute to KDD.In conclusion, we presented some deni-
tions of basic notions in the KDD eld. Ourprimary aim was to clarify the relation be-

tween knowledge discovery and data mining.

We provided an overview of the KDD process

and basic data-mining methods. Given the
broad spectrum of data-mining methods and
algorithms, our overview is inevitably limit-

ed in scope: There are many data-mining
techniques, particularly specialized methods
for particular types of data and domain. Al-
though various algorithms and applications
might appear quite different on the surface,
it is not uncommon to nd that they share

many common components. Understanding
data mining and model induction at this

component level claries the task of any da-

ta-mining algorithm and makes it easier for
the user to understand its overall contribu-tion and applicability to the KDD process.
This article represents a step toward acommon framework that we hope will ulti-

mately provide a unifying vision of the com-
mon overall goals and methods used in
KDD. We hope this will eventually lead to a

better understanding of the variety of ap-

proaches in this multidisciplinary eld and
how they t together.
AcknowledgmentsWe thank Sam Uthurusamy, Ron Brachman, and
KDD-96 referees for their valuable suggestions
and ideas. Note1. Throughout this article, we use the term pattern
to designate a pattern found in data. We also refer
to models. One can think of patterns as compo-
nents of models, for example, a particular rule in aclassication model or a linear component in a re-
gression model.  ReferencesAgrawal, R., and Psaila, G. 1995. Active Data Min-ing. In Proceedings of the First International Con-ference on Knowledge Discovery and Data Mining

(KDD-95), 38. Menlo Park, Calif.: American Asso-ciation for Articial Intelligence.Agrawal, R.; Mannila, H.; Srikant, R.; Toivonen, H.;
and Verkamo, I. 1996. Fast Discovery of Association
Rules. In Advances in Knowledge Discovery and Data
Mining,eds. U. Fayyad, G. Piatetsky-Shapiro, P.
Smyth, and R. Uthurusamy, 307328. Menlo Park,

Calif.: AAAI Press.Apte, C., and Hong, S. J. 1996. Predicting Equity
Returns from Securities Data with Minimal RuleGeneration. In Advances in Knowledge Discovery and
Data Mining, eds. U. Fayyad, G. Piatetsky-Shapiro, P.
Smyth, and R. Uthurusamy, 514560. Menlo Park,
Calif.: AAAI Press.Basseville, M., and Nikiforov, I. V. 1993. 
Detectionof Abrupt Changes: Theory and Application. 
Engle-wood Cliffs, N.J.: Prentice Hall.Berndt, D., and Clifford, J. 1996. Finding Patternsin Time Series: A Dynamic Programming Approach.

In Advances in Knowledge Discovery and Data Mining,
eds. U. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and
R. Uthurusamy, 229248. Menlo Park, Calif.: AAAI
Press.Berry, J. 1994. Database Marketing. 
Business Week,
September 5, 5662.Brachman, R., and Anand, T. 1996. The Process of
Knowledge Discovery in Databases: A Human-Cen-
tered Approach. In Advances in Knowledge Discovery
and Data Mining, 3758, eds. U. Fayyad, G. Piatet-sky-Shapiro, P. Smyth, and R. Uthurusamy. Menlo
Park, Calif.: AAAI Press.Breiman, L.; Friedman, J. H.; Olshen, R. A.; andStone, C. J. 1984. Classication and Regression Trees.
Belmont, Calif.: Wadsworth.
Brodley, C. E., and Smyth, P. 1996. Applying Clas-
sication Algorithms in Practice. Statistics and Com-puting. Forthcoming.Buntine, W. 1996. Graphical Models for Discover-
ing Knowledge. In Advances in Knowledge Discovery
and Data Mining, eds. U. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, 5982.
Menlo Park, Calif.: AAAI Press.Cheeseman, P. 1990. On Finding the Most Probable
Model. In Computational Models of Scientic Discov-ery and Theory Formation, 
eds. J. Shrager and P. Lan-
gley, 7395. San Francisco, Calif.: Morgan Kauf-
mann.Cheeseman, P., and Stutz, J. 1996. Bayesian Clas-
sication (AUTOCLASS): Theory and Results. In 
Ad-vances in Knowledge Discovery and Data Mining,
eds.Articles
FALL 1996   51
ering Informative Patterns and Data Cleaning. InAdvances in Knowledge Discovery and Data Mining,
eds. U. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and

R. Uthurusamy, 181204. Menlo Park, Calif.: AAAI

Press.Hall, J.; Mani, G.; and Barr, D. 1996. Applying
Computational Intelligence to the Investment Pro-
cess. In Proceedings of CIFER-96: Computational
Intelligence in Financial Engineering. Washington,
D.C.: IEEE Computer Society.
Hand, D. J. 1994. Deconstructing Statistical Ques-
tions. Journal of the Royal Statistical Society A. 
157(3):317356.Hand, D. J. 1981. Discrimination and Classication.Chichester, U.K.: Wiley.
Heckerman, D. 1996. Bayesian Networks for Knowl-edge Discovery. In 
Advances in Knowledge Discovery
and Data Mining, eds. U. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, 273306.

Menlo Park, Calif.: AAAI Press.Hernandez, M., and Stolfo, S. 1995. The MERGE-PURGEProblem for Large Databases. In Proceedingsof the 1995 ACM-SIGMOD Conference, 127138.
New York: Association for Computing Machinery.
Holsheimer, M.; Kersten, M. L.; Mannila, H.; and
Toivonen, H. 1996. Data Surveyor: Searching the

Nuggets in Parallel. In Advances in Knowledge Dis-
covery and Data Mining, 
eds. U. Fayyad, G. Piatet-sky-Shapiro, P. Smyth, and R. Uthurusamy,

447471. Menlo Park, Calif.: AAAI Press.Horvitz, E., and Jensen, F. 1996. 
Proceedings of the
Twelfth Conference of Uncertainty in Articial Intelli-

gence. San Mateo, Calif.: Morgan Kaufmann.Jain, A. K., and Dubes, R. C. 1988. Algorithms forClustering Data. Englewood Cliffs, N.J.: Prentice-Hall.Kloesgen, W. 1996. A Multipattern and Multistrate-
gy Discovery Assistant. In 
Advances in KnowledgeDiscovery and Data Mining, 
eds. U. Fayyad, G. Piatet-sky-Shapiro, P. Smyth, and R. Uthurusamy,

249271. Menlo Park, Calif.: AAAI Press.Kloesgen, W., and Zytkow, J. 1996. Knowledge Dis-
covery in Databases Terminology. In 
Advances inKnowledge Discovery and Data Mining
, eds. U. Fayyad,G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy,

569588. Menlo Park, Calif.: AAAI Press.Kolodner, J. 1993. 
Case-Based Reasoning.San Fran-cisco, Calif.: Morgan Kaufmann.Langley, P., and Simon, H. A. 1995. Applications of
Machine Learning and Rule Induction. Communica-tions of the ACM38:5564.Major, J., and Mangano, J. 1995. Selecting among
Rules Induced from a Hurricane Database. Journal
of Intelligent Information Systems 
4(1): 3952.Manago, M., and Auriol, M. 1996. Mining for OR.ORMS Today
(Special Issue on Data Mining), Febru-ary, 2832.
Mannila, H.; Toivonen, H.; and Verkamo, A. I.
1995. Discovering Frequent Episodes in Sequences.
In Proceedings of the First International Confer-

ence on Knowledge Discovery and Data Mining

(KDD-95), 210215. Menlo Park, Calif.: AmericanU. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R.
Uthurusamy, 7395. Menlo Park, Calif.: AAAI Press.
Cheng, B., and Titterington, D. M. 1994. Neural
NetworksA Review from a Statistical Perspective.
Statistical Science9(1): 230.Codd, E. F. 1993. Providing 
OLAP(On-Line Analyti-cal Processing) to User-Analysts: An IT Mandate. E.
F. Codd and Associates.
Dasarathy, B. V. 1991. Nearest Neighbor (NN)
Norms: NN Pattern Classication Techniques.

Washington, D.C.: IEEE Computer Society.
Djoko, S.; Cook, D.; and Holder, L. 1995. Analyzing
the Benets of Domain Knowledge in Substructure
Discovery. In Proceedings of KDD-95: First Interna-
tional Conference on Knowledge Discovery and
Data Mining, 7580. Menlo Park, Calif.: American
Association for Articial Intelligence.Dzeroski, S. 1996. Inductive Logic Programming forKnowledge Discovery in Databases. In 
Advances inKnowledge Discovery and Data Mining,
eds. U.Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R.
Uthurusamy, 5982. Menlo Park, Calif.: AAAI Press.
Elder, J., and Pregibon, D. 1996. A Statistical Per-
spective on KDD. In Advances in Knowledge Discov-ery and Data Mining,
eds. U. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, 83116.
Menlo Park, Calif.: AAAI Press.Etzioni, O. 1996. The World Wide Web: Quagmire
or Gold Mine? Communications of the ACM(SpecialIssue on Data Mining). November 1996. Forthcom-ing.Fayyad, U. M.; Djorgovski, S. G.; and Weir, N. 1996.
From Digitized Images to On-Line Catalogs: DataMining a Sky Survey. 
AI Magazine17(2): 5166.Fayyad, U. M.; Haussler, D.; and Stolorz, Z. 1996.
KDD for Science Data Analysis: Issues and Exam-ples. In Proceedings of the Second International
Conference on Knowledge Discovery and DataMining (KDD-96), 5056. Menlo Park, Calif.: Amer-ican Association for Articial Intelligence.Fayyad, U. M.; Piatetsky-Shapiro, G.; and Smyth, P.
1996. From Data Mining to Knowledge Discovery:
An Overview. In 
Advances in Knowledge Discoveryand Data Mining, eds. U. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, 130. Men-
lo Park, Calif.: AAAI Press.Fayyad, U. M.; Piatetsky-Shapiro, G.; Smyth, P.; and
Uthurusamy, R. 1996. 
Advances in Knowledge Dis-
covery and Data Mining.
Menlo Park, Calif.: AAAIPress.Friedman, J. H. 1989. Multivariate Adaptive Regres-sion Splines. Annals of Statistics 19:1141.Geman, S.; Bienenstock, E.; and Doursat, R. 1992.Neural Networks and the Bias/Variance Dilemma.
Neural Computation4:158.Glymour, C.; Madigan, D.; Pregibon, D.; and
Smyth, P. 1996. Statistics and Data Mining.
Com-munications of the ACM(Special Issue on Data Min-ing). November 1996. Forthcoming.Glymour, C.; Scheines, R.; Spirtes, P.; Kelly, K. 1987.
Discovering Causal Structure. 
New York: Academic.
Guyon, O.; Matic, N.; and Vapnik, N. 1996. Discov-
Articles
52AI MAGAZINE
Association for Articial Intelligence.Matheus, C.; Piatetsky-Shapiro, G.; and McNeill, D.1996. Selecting and Reporting What Is Interesting:The KERApplication to Healthcare Data. In Ad-vances in Knowledge Discovery and Data Mining,
eds.U. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R.

Uthurusamy, 495516. Menlo Park, Calif.: AAAI
Press. Pearl, J. 1988. Probabilistic Reasoning in Intelligent
Systems.San Francisco, Calif.: Morgan Kaufmann.Piatetsky-Shapiro, G. 1995. Knowledge Discovery
in Personal Data versus PrivacyA Mini-Sympo-sium. IEEE Expert 
10(5).Piatetsky-Shapiro, G. 1991. Knowledge Discovery
in Real Databases: A Report on the IJCAI-89 Work-

shop. AI Magazine11(5): 6870. Piatetsky-Shapiro, G., and Matheus, C. 1994. TheInterestingness of Deviations. In Proceedings of
KDD-94, eds. U. M.  Fayyad and R. Uthurusamy.

Technical Report WS-03. Menlo Park, Calif.: AAAI
Press.Piatetsky-Shapiro, G.; Brachman, R.; Khabaza, T.;
Kloesgen, W.; and Simoudis, E., 1996. An Overview

of Issues in Developing Industrial Data Mining and
Knowledge Discovery Applications. In Proceedings
of the Second International Conference on Knowl-edge Discovery and Data Mining (KDD-96), eds. J.

Han and E. Simoudis, 8995. Menlo Park, Calif.:
American Association for Articial Intelligence.Quinlan, J. 1992. C4.5: Programs for Machine Learn-
ing. San Francisco, Calif.: Morgan Kaufmann.Ripley, B. D. 1994. Neural Networks and Related
Methods for Classication. Journal of the Royal Sta-
tistical Society B.56(3): 409437. Senator, T.; Goldberg, H. G.; Wooton, J.; Cottini, M.
A.; Umarkhan, A. F.; Klinger, C. D.; Llamas, W. M.;

Marrone, M. P.; and Wong, R. W. H. 1995. The Fi-

nancial Crimes Enforcement Network AI System
(FAIS
): Identifying Potential Money Launderingfrom Reports of Large Cash Transactions. 
AI Maga-zine16(4): 2139.Shrager, J., and Langley, P., eds. 1990. 
Computation-al Models of Scientic Discovery and Theory Forma-
tion.San Francisco, Calif.: Morgan Kaufmann.Silberschatz, A., and Tuzhilin, A. 1995. On Subjec-
tive Measures of Interestingness in Knowledge Dis-covery. In Proceedings of KDD-95: First Interna-
tional Conference on Knowledge Discovery and

Data Mining, 275281. Menlo Park, Calif.: Ameri-
can Association for Articial Intelligence.Silverman, B. 1986. Density Estimation for Statisticsand Data Analysis. New York: Chapman and Hall.
Simoudis, E.; Livezey, B.; and Kerber, R. 1995. Using
Recon for Data Cleaning. In Proceedings of KDD-95:
First International Conference on Knowledge Discov-ery and Data Mining, 275281. Menlo Park, Calif.:
American Association for Articial Intelligence.Smyth, P.; Burl, M.; Fayyad, U.; and Perona, P.
1996. Modeling Subjective Uncertainty in Image
Annotation. In Advances in Knowledge Discovery and
Data Mining, 517540. Menlo Park, Calif.: AAAIPress.Spirtes, P.; Glymour, C.; and Scheines, R. 1993.
Causation, Prediction, and Search. 
New York:
Springer-Verlag.
Stolorz, P.; Nakamura, H.; Mesrobian, E.; Muntz, R.;
Shek, E.; Santos, J.; Yi, J.; Ng, K.; Chien, S.; Me-

choso, C.; and Farrara, J.  1995. Fast Spatio-Tempo-

ral Data Mining of Large Geophysical Datasets. In
Proceedings of KDD-95: First International Confer-
ence on Knowledge Discovery and Data Mining,

300305. Menlo Park, Calif.: American Association
for Articial Intelligence.Titterington, D. M.; Smith, A. F. M.; and Makov, U.
E. 1985. Statistical Analysis of Finite-Mixture Distribu-
tions.Chichester, U.K.: Wiley.
U.S. News. 1995. Basketballs New High-Tech Guru:
IBM Software Is Changing Coaches Game Plans.
U.S. News and World Report, 
11 December.Weigend, A., and Gershenfeld, N., eds. 1993. 
Pre-
dicting the Future and Understanding the Past. 
Red-wood City, Calif.: Addison-Wesley.
Weiss, S. I., and Kulikowski, C. 1991. 
Computer Sys-tems That Learn: Classication and Prediction Meth-

ods from Statistics, Neural Networks, Machine Learn-

ing, and Expert Systems. San Francisco, Calif.:Morgan Kaufmann.Whittaker, J. 1990. 
Graphical Models in Applied Mul-tivariate Statistics.New York: Wiley.
Zembowicz, R., and Zytkow, J. 1996. From Contin-
gency Tables to Various Forms of Knowledge in

Databases. In Advances in Knowledge Discovery and
Data Mining, eds. U. Fayyad, G. Piatetsky-Shapiro, P.
Smyth, and R. Uthurusamy, 329351. Menlo Park,

Calif.: AAAI Press.Usama Fayyad is a senior re-searcher at Microsoft Research.

He received his Ph.D. in 1991
from the University of Michigan
at Ann Arbor. Prior to joining Mi-

crosoft in 1996, he headed the
Machine Learning Systems Group
at the Jet Propulsion Laboratory

(JPL), California Institute of Tech-
nology, where he developed data-mining systems

for automated science data analysis. He remains

afliated with JPL as a distinguished visiting scien-
tist. Fayyad received the JPL 1993 Lew Allen Award

for Excellence in Research and the 1994 National

Aeronautics and Space Administration Exceptional
Achievement Medal. His research interests include

knowledge discovery in large databases, data min-

ing, machine-learning theory and applications, sta-

tistical pattern recognition, and clustering. He was
program cochair of KDD-94 and KDD-95 (the First
International Conference on Knowledge Discovery

and Data Mining). He is general chair of KDD-96,
an editor in chief of the journal Data Mining andKnowledge Discovery, 
and coeditor of the 1996 AAAIPress book Advances in Knowledge Discovery and Da-
ta Mining.Articles
FALL 1996   53
cal Engineering Departments at Caltech (1994) andregularly conducts tutorials on probabilistic learn-

ing algorithms at national conferences (including
UAI-93, AAAI-94, CAIA-95, IJCAI-95). He is general
chair of the Sixth International Workshop on AI

and Statistics, to be held in 1997. Smyths research

interests include statistical pattern recognition, ma-
chine learning, decision theory, probabilistic rea-

soning, information theory, and the application of

probability and statistics in AI. He has published 16
journal papers, 10 book chapters, and 60 confer-
ence papers on these topics.Gregory Piatetsky-Shapiro 
is aprincipal member of the technical
staff at GTE Laboratories and the
principal investigator of the
Knowledge Discovery in Databas-

es (KDD) Project, which focuses
on developing and deploying ad-
vanced KDD systems for business
applications. Previously, he
worked on applying intelligent front ends to het-

erogeneous databases. Piatetsky-Shapiro received

several GTE awards, including GTEs highest tech-

nical achievement award for the KERsystem forhealth-care data analysis. His research interests in-

clude intelligent database systems, dependency
networks, and Internet resource discovery. Prior to

GTE, he worked at Strategic Information develop-
ing nancial database systems. Piatetsky-Shapiro re-
ceived his M.S. in 1979 and his Ph.D. in 1984, both
from New York University (NYU). His Ph.D. disser-

tation on self-organizing database systems received
NYU awards as the best dissertation in computer

science and in all natural sciences. Piatetsky-
Shapiro organized and chaired the rst three (1989,
1991, and 1993) KDD workshops and helped in de-
veloping them into successful conferences (KDD-95
and KDD-96). He has also been on the program

committees of numerous other conferences and
workshops on AI and databases. He edited and
coedited several collections on KDD, including two
booksKnowledge Discovery in Databases (AAAIPress, 1991) and Advances in Knowledge Discovery in
Databases (AAAI Press, 1996)and has many otherpublications in the areas of AI and databases. He is
a coeditor in chief of the new Data Mining andKnowledge Discovery 
journal. Piatetsky-Shapirofounded and moderates the KDD Nuggetselectronicnewsletter (kdd@gte.com) and is the web master for
Knowledge Discovery Mine (<http://info.gte.com/

~kdd /index.html>).Padhraic Smyth received a rst-class-honors Bachelor of Engi-

neering from the National Uni-
versity of Ireland in 1984 and an
MSEE and a Ph.D. from the Elec-
trical Engineering Department at

the California Institute of Tech-

nology (Caltech) in 1985 and
1988, respectively. From 1988 to
1996, he was a technical group leader at the Jet

Propulsion Laboratory (JPL). Since April 1996, he

has been a faculty member in the Information and
Computer Science Department at the University of
California at Irvine. He is also currently a principal

investigator at JPL (part-time) and is a consultant to
private industry. Smyth received the Lew Allen

Award for Excellence in Research at JPL in 1993

and has been awarded 14 National Aeronautics and
Space Administration certicates for technical in-
novation since 1991. He was coeditor of the book
Advances in Knowledge Discovery and Data Mining

(AAAI Press, 1996). Smyth was a visiting lecturer in
the Computational and Neural Systems and Electri-Articles
54AI MAGAZINE
AAAI 97Providence, Rhode IslandJuly 2731, 1997Title pages due January 6, 1997 
Papers due January 8, 1997Camera copy due April 2, 1997ncai@aaaai.orghttp://www.aaai.org/
Conferences/National/1997/aaai97.htmlHypertrophic pachymeningitis is a rare disorder characterizedby marked inflammatory hypertrophy of the dura mater, withsubsequent neurological deficits resulting from the compressionof adjacent structures. Based on anatomic site, cases ofhypertrophic pachymeningitis can be subdivided into spinal,intracranial and the much less frequent craniospinalpachymeningitis.1-3Hypertrophic pachymeningitis involving the spinal meningeswas first described in the nineteenth century by Charcot andJoffroy and the intracranial form was reported shortly thereafter(cited in Ashkenazi et al4and Parney et al5). HypertrophicABSTRACT:Purpose:We report the treatment and follow-up, including MRI, of two patients with idiopathic hypertrophicpachymeningitis and review the English language literature, with emphasis on management and outcome in this rare disorder.Methodsand Materials:The files of two patients were reviewed, with relevant histopathology and imaging (MRI). The first patient has beenfollowed for sixteen years (the longest MRI-documented postoperative course reported for this condition) and the second for two years.The English language literature was reviewed, including a summary of all reported patients that have been followed with MRI or CTimaging. Results:Despite extensive investigation, no underlying etiology was determined in either patient. Histopathological studiesrevealed a chronic inflammatory dural infiltrate in both patients, with granulomas in the first but not the second patient. The first patientunderwent surgery twice and has remained stable for sixteen years, despite persistent neurologic deficits. The second patient wasmanaged with dexamethasone after a surgical biopsy, and experienced complete resolution of all neurological deficits and abnormalitiesseen with MRI. Conclusions:Although prompt and extensive surgery has been recommended for this condition, the results from oursecond patient indicate that complete remission can be achieved in some patients with biopsy and steroid therapy.This also supports theview that autoimmune mechanisms underlie idiopathic hypertrophic pachymeningitis. The first patient illustrates that extensivelaminectomies may be an effective therapeutic option but chronic discomfort may result. If extensive surgery must be performed,laminoplasty should be done because of the potential for reduced pain and improved long-term spinal stability.RSUM: Pachymningite hypertrophique idiopathique:  propos de deux cas et revue de la littrature.But: Nous rapportons le traitement etle suivi, incluant la RMN, de deux patients ayant prsent une pachymningite hypertrophique idiopathique et nous revoyons la littrature de langueanglaise en insistant sur le traitement et lvolution de cette maladie rare. Mthodes et sujets:Les dossiers ainsi que lanatomopathologie et limagerie(RMN) de deux patients ont t rviss. Le premier patient est suivi depuis seize ans (le suivi postopratoire le plus long document par RMN rapportdans cette maladie) et le deuxime est suivi depuis deux ans. La littrature de langue anglaise a t rvise, incluant un sommaire de tous les casrapports qui ont t suivis par RMN ou CTscan. Rsultats:Malgr une investigation pousse, aucune tiologie na pu tre dtermine dans chacun deces cas. Les tudes anatomopathologiques ont rvl une infiltration inflammatoire chronique de la dure-mre, avec des granulomes dans le premier casseulement. Le premier patient a subi deux interventions chirurgicales et il est demeur stable pendant seize ans, malgr des dficits neurologiquespersistants. Le second patient a reu de la dexamthasone aprs la biopsie chirurgicale et a prsent une rsolution complte de tous les dficitsneurologiques et des anomalies observes  la RMN. Conclusions: Bien quon recommande de procder rapidement  une chirurgie extensive danscette affection, les rsultats chez notre second cas indiquent quune rmission complte peut tre obtenue chez certains cas par la biopsie et lacorticothrapie. Ceci est en faveur de lhypothse dun mcanisme autoimmun dans la pachymningite hypertrophique idiopathique. Le premier casillustre que les laminectomies extensives peuvent tre une option thrapeutique efficace, mais quun inconfort chronique peut sen suivre. Si unechirurgie extensive doit tre effectue, une laminoplastie devrait tre faite pour minimiser la douleur et assurer une meilleure stabilit de la colonnevertbrale  long terme.Can. J. Neurol. Sci. 2000; 27: 333-340THE CANADIAN JOURNALOF NEUROLOGICALSCIENCES333Idiopathic HypertrophicPachymeningitis: AReport of TwoPatients and Review of the LiteratureAaron S. Dumont, Arthur W. Clark, Robert J. Sevick and S. Terence Myles From the Departments of Clinical Neurosciences (ASD, AWC, RJS, STM), Pathology(AWC), Radiology (RJS), Surgery (STM), and Anatomy (RJS), Faculty of Medicine,University of Calgary, Calgary,Alberta, Canada.RECEIVEDJULY13, 1999. ACCEPTEDINFINALFORMAUGUST8, 2000.Reprint requests to:Arthur W. Clark, Department of Clinical Neurosciences, Faculty ofMedicine, University of Calgary, 1403 - 29th Street NW, Calgary,Alberta T2N 2T9Canada.CASE REPORTpachymeningitis has often been attributed to specific etiologies,such as tuberculosis or syphilis. Most recent cases, however,have been reported as idiopathic, despite intensive investigationto implicate other etiologies. The widespread application ofgadolinium enhanced MRI has facilitated the diagnosis andfollow-up of patients and increased the frequency of reportedcases.6-8We present two cases of idiopathic hypertrophicpachymeningitis; one of craniospinal and the second of spinaldistribution, with MRI findings, pathology, and postoperativefollow-up. These patients illustrate very different approaches totreatment. The first patient underwent surgery twice, initially forbiopsy and subsequently for a second biopsy and decompression.Despite persistent pain and numbness, the patients neurologicalfunction has remained stable 16 years after his initial surgery.Toour knowledge this is the longest postoperative follow-upreported for this condition. The second patient was treated withdexamethasone after biopsy of the affected dura, with completeresolution of symptoms and MRI changes. Prompt and extensivesurgery has been recommended for this condition but experiencewith our second patient indicates that complete remission can beachieved with biopsy and steroid therapy.This is consistent withthe view that hypertrophic pachymeningitis involves anunderlying autoimmune etiology.1,5,6,8-10PATIENTREPORTSPatient 1In January 1981 this 61-year-old man presented with diplopia and atwelfth cranial nerve palsy. He later developed pain in his upper cervicalregion. Amyelogram revealed a complete block at the cervical level.Surgery revealed serous material overlying and encasing the spinalcord and pathologic evaluation of a biopsy disclosed reactive tissue. Hewas treated with steroids and improved, with resolution of allneurological problems except the 12th cranial nerve palsy.In June 1982, he suddenly developed ataxia and weakness in hislower extremities. Physical examination in July revealed sensory lossand upper motor neuron signs below T6. Myelograms revealed adhesive,obliterative arachnoiditis in the lumbar area, complete blocks at T9 andT2, and relative obstruction at C5. CSF contained 34 white blood cellsand 106 mg of protein. There was no documented fever or leucocytosis.Re-exploration was undertaken due to the recurrence of the blockand failure to establish a diagnosis. Laminectomy, with decompressionand exploration of T3 to T5, revealed a thick, nonpulsatile dura andadherent epidural material. The arachnoid was pulsatile but containedopacities throughout.Pathologic examination revealed a markedly thickened dura with anodular chronic inflammatory infiltrate. Small granulomas were notedwith thick rims of epithelioid cells surrounding central collections ofpolymorphonuclear cells. Giant cells were present in a few granulomas.Lymphocytes, histiocytes, plasma cells and occasional polymorpho-nuclear cells surrounded the granulomas. Stains for bacteria includingtubercle bacilli and spirochetes, fungi and protozoa were completelynegative. The histological diagnosis was chronic granulomatousinflammation of dural and epidural tissues of unknown etiology.The patient gradually improved, but in April 1991 experiencedincreased pain in the posterior mid-thoracic region. An MRI revealedlocalized spinal cord atrophy at upper thoracic levels (T2-T4) andincreased cord signal on T2-weighted images from this level distally,consistent with myelomalacia or ischemic/demyelinating changes.Gadolinium-enhanced axial images throughout the thoracic spinerevealed no abnormal enhancement.In 1997, about fifteen years after his second operation, the patientpresented with tightness across his shoulders, activity-induced crampingin the upper extremities and increased numbness in his legs. He was stillable to walk for several hours each day and his bowel and bladderfunction was normal. He had a moderate kyphosis and unchangedneurologic signs. Repeat MRI revealed an atrophic cord and probableposterior tethering at the site of his previous laminectomies. On T2-weighted images the thoracic spinal cord demonstrated focal T2 signalhyperintensity centrally but no definite syrinx.Patient 2A30-year-old woman presented in December 1996, complaining ofdifficulty urinating, with bilateral leg weakness and numbness. MRrevealed a dural lesion extending from approximately the C4 to T3 level(Figures 1 & 2). She subsequently underwent a T1 bilateral laminectomyand biopsy of the lesion. At operation, dark and slightly hemorrhagictissue was observed in the epidural space, dorsal to the dural tube. Thedura appeared thickened and bowed posteriorly. Histopathologicalexamination of the biopsied tissue revealed well-organized connectivetissue, infiltrated by a mixed inflammatory response, with giant cells,plasma cells, occasional eosinophils, and foci of B lymphocytes (Figures3 & 4). Discrete granulomas were not identified. Stains for organisms(bacteria, acid fast bacilli, spirochetes and fungi) were negative. Therewas no evidence of vasculitis. Investigation for lymphoma, infectious,and connective tissue disease was negative. The final diagnosis wasidiopathic spinal pachymeningitis.Postoperatively the patient was continued on dexamethasone, withgradual improvement in her neurologic function. She developed apulmonary embolus and required intensive care. New defects appearedon ventilation perfusion scans despite aggressive therapeutic measuresand an inferior vena cava filter was implanted. Her neurological functioncontinued to improve. She was able to ambulate with a walker and herbladder function returned to baseline. An MRI done before discharge inFebruary revealed almost complete resolution of the cervical andthoracic dural thickening. Asmall amount of residual enhancement wasseen but there was no evidence of cord compression. Arepeat MRI scan done five months after surgery revealed no duralthickening or spinal cord compression. The residual enhancement notedon the first postoperative MRI had completely disappeared. She wasseen one month later, at which time her numbness and weakness hadresolved. Another MRI scan done 12 months postoperatively revealed nonew findings.DISCUSSIONIdiopathic hypertrophic pachymeningitis is a rare but possiblyunderrecognized condition. There are few reports of long-termfollow-up of treated patients.6 , 8 , 11 , 1 2Optimal therapy iscontroversial. Our patients initially underwent biopsy and steroidtherapy, with improvement in neurological function. In the firstpatient, an early relapse has been followed by more than 15 yearsof stable neurological function. Increased thoracic pain andsubjective numbness were likely the result of surgical treatment,as radiologic and neurologic signs of progression were absent.Thus our first patient, with the longest follow-up reported,suggests a relatively favorable long-term prognosis for somepatients with this disorder.The diagnosis of idiopathic hypertrophic pachymeningitisdepends on excluding causative diseases, particularly thosewhich call for specific treatment. The natural history is poorlydefined. Areview of the English and Japanese literature onidiopathic hypertrophic spinal pachymeningitis led to theconclusion that patients with inflammatory signs (fever,increased sedimentation rate, leukocytosis, or increased C-reactive protein) had a poorer prognosis than patients withoutinflammatory signs.13THE CANADIAN JOURNALOF NEUROLOGICALSCIENCES334Associated diseasesHypertrophic pachymeningitis can be associated withinfectious agents, autoimmune disorders, and otherp r o c e s s e s .2 , 4 , 1 4 - 1 7It should be distinguished from duralhypertrophy without inflammation18and from enhancement ofthe dura on neuroimaging without documented hypertrophy orinflammation.19Sarcoidosis can present with findings similar tohypertrophic pachymeningitis.20-23LE JOURNALCANADIEN DES SCIENCES NEUROLOGIQUESVolume 27, No. 4  November 2000335Figure 1:Patient 2. Sagittal (A) and axial (B) T2-weighted fast spin-echo images of the lower cervical and upper thoracicspine show markedly thickened dura which appears hypointense (arrows), causing mild spinal cord compression.Figure 2:Patient 2. Sagittal T1-weighted images before (A) and after administration of intravenous Gd-DTPA(B). On thepre-contrast image, spinal cord and dura are isointense and difficult to distinguish. The abnormally thickened dura showsmarked contrast enhancement. The enhancement is more marked peripherally with some relative signal hypointensitycentrally (arrow). ABABInfectious agents reportedly associated with hypertrophicpachymeningitis include syphilis,24,25tuberculosis,26,27HTLV-I28and fungi.2 9 - 3 1Pachymeningitis may be the presentingmanifestation of adjacent ear or sinus infections.3 2 , 3 3P C Rdiagnostic methods applied to CSF may be helpful in patientswhere tuberculosis is suspected but routine tests are negative.5,34Hypertrophic pachymeningitis has been associated withvarious autoimmune processes including rheumatoidarthritis,35,36orbital pseudotumor,37multifocal fibrosclerosis,38,39mixed connective tissue disease1 0and We g e n e rsgranulomatosis.40Such association supports an autoimmunepathogenesis for idiopathic cases.1,5,6,8-10THE CANADIAN JOURNALOF NEUROLOGICALSCIENCES336Figure 3:Patient 2. Periodic acid Schiff (PAS)-stained section showing pathologic features from thesurgically excised dura. Markedly thickened dura mater is infiltrated by inflammatory cells (X80)Figure 4:Patient 2. At higher magnification the inflammatory infiltrate is shown to includegiant cells, plasma cells, occasional eosinophils, and lymphocytes (PAS, X800).LE JOURNALCANADIEN DES SCIENCES NEUROLOGIQUESVolume 27, No. 4  November 2000337Table: Idiopathic hypertrophic pachymeningitis - Response to TreatmentAuthor(Ref No)AgeSexDescriptionTreatmentOutcomeKanamori et al1128MSpinal:Steroids (1989)Remission x2 yearsT5L2Expansivelaminoplasty (1991)Improved, then worse at 4 monthsSteroidsImproved; stable at 4 years postop but lesion extended to T2Mikawa et al1358FSpinal:Steroids;Fluctuating course with improvement after third operationT6T10Laminectomy(x3), dura resection (x2)Adler et al947MSpinal:SteroidsStable one week, then rapidly worse C3T11Laminectomies; longitudinalFluctuating course with postop improvements; dura incision & biopsy; stable with deficits at 3 years after third operationsteroids; azathioprineAshkenazi et al465FSpinal:Laminectomy,Asymptomatic at 1 yrT1T5dura excision46FSpinal:Laminectomy,UnavailableT9T10dura excisionDigman et al6170FSpinal: (Antibiotics forProgression, died 4 months later due to respiratory failureAll levelsconcurrent infections)Rosenfield et al4125MSpinal: Laminectomy, steroidsMarked improvement of symptoms at l yrC4C7Kao et al4234MSpinal:Multiple, transverse Symptomatic improvement at 18 monthsC7T2durotomies28MSpinal:Multiple, transverse Symptomatic improvement at 16 monthsC7T2durotomiesBotella et al155FCranio cervicalVPShuntSymptoms decreasedSteroidsLittle improvementDura excisionImprovement but recurrence; died of pneumoniaDumont et al 30FSpinal C4T3Laminectomy, biopsy,Complete resolution(this report)steroids61MCraniospinalSteroids,Improved, recurrent pain at 10 years, spinal cord atrophy,(laminectomy & biopsy x2)further problems at 15 years but neurologically stableFriedman et al 265FCraniospinalSurgeryImprovement, no follow-up time givenGoyal et al1528MIntracranialSteroidsProgression at 2 years62FIntracranialSteroidsCranial nerve palsy improvement; MRI improvement at 3 months19FIntracranialSteroidsUnavailableKitai et al656FIntracranialSteroidsSymptoms decreased; dura decreased in size at 3 yearsPhanthumchinda et al4823MIntracranialSteroidsNo symptoms at 1 yr30FIntracranialSteroidsMRI improvement at 1 yr42MIntracranialSteroidsSymptoms decreased at 10 monthsTanaka et al845MIntracranialSteroidsMRI and symptomatic improvement at over 2.5 yrsJacobson et al4578MIntracranialSteroids,Modest improvement of symptoms; MRI improvement at 18 azathioprinemonthsNishio et al5936FIntracranialNoneSpontaneous resolutionKioumehr et al4735MIntracranialSurgery to release Died of postoperative complicationsobstructionHamilton et al1655FIntracranialSteroidsSome improvementAzathioprineMarked improvement of symptons at 6 mos68FIntracranialSteroids and azathioprineImprovement of symptoms31MIntracranialSteroidsSymptom controlMethotrexateNo benefitChloroquine and radiationMarked improvement at 4 monthsClinical manifestationsIt was the spinal form of hypertrophic pachymeningitis whichwas first described by Charcot and Joffroy (cited in Ashkenazi etal4). They divided the clinical presentation into three distinctstages: 1) intermittent radicular pain that eventually becamecontinuous; 2) muscle weakness and atrophy; 3) spastic paralysisand loss of sphincter control. However, leg weakness ornumbness, sometimes associated with bladder dysfunction, andevolving over two weeks to a year, is a characteristic presentingcomplaint (our patient 2;4,11,13). Radicular signs and symptomsconfined to the upper extremities may occur.41Signs may evolveover a longer time interval.42The cranial form of hypertrophicpachymeningitis frequently presents with headache, cranialneuropathies and ataxia.43Radiologic findingsIn general, hypertrophic pachymeningitis lesions appearhypointense relative to brain or spinal cord on T1- and, to agreater extent, on T2-weighted images. Contrast-enhanced MRImay show more intense enhancement at the periphery of thelesions with central signal hypointensity, as in our patient 2,possibly due to greater enhancement in an active zone ofinflammation peripherally than in a central zone of densef i b r o s i s .4 4Nonetheless, pathologic confirmation remainsessential. It is often only after histopathologic examination ofdura mater that the diagnosis can be entertained and a search forpotential causes carried out. PathologyGrossly observable thickening or inflammation of the duramater may occur in association with meningioma, craniopharyn-gioma, lymphoma, metastatic carcinoma, and other tumors.Infectious processes involving the sinuses, middle ear, epidural,or subdural locations can produce inflammation and thickeningof the dura. Hence the clinical and radiologic findings andobservations at surgery are essential for interpretation of thepathology; and a search for infectious causes is mandatory.H i s t o l o g i c a l l y, distinction between inflammation and aneoplastic proliferation of lymphoid, plasmacytic, or histiocyticelements is fundamental. Most cases of idiopathic pachy-meningitis are characterized by a nonnecrotizing chronicinflammatory infiltrate of lymphocytes, plasma cells, andoccasional histiocytes, giant cells, polymorphonuclear cells, oreosinophils.1,4,6,16,17,41,45-48Granulomas, necrosis and vasculitisare less frequently identified.2,12-14,49Dural involvement has been reported in neurosarcoidosis butinvolvement of the leptomeninges and substance of the centralnervous system are more typical.20-22,50Even when involvementis preferentially dural, there is usually an obvious mass, typicallysubdural, evoking meningioma; but the pattern may be sarcoiden-plaque51,52or pachymeningitis.22,53-55Biopsy of affected dura in sarcoid usually reveals thecharacteristic noncaseating granuloma. In contrast, biopsy ofinvolved dura in rheumatoid arthritis usually reveals onlynonspecific inflammation, and the association ofpachymeningitis with rheumatoid arthritis relies on other clinicalevidence.35,36Rheumatoid nodules may involve dura mater insymptomatic pachymeningitis but, to our knowledge, they havebeen discovered only at autopsy.35ManagementThe Table summarizes methods and outcomes of treatment inTHE CANADIAN JOURNALOF NEUROLOGICALSCIENCES338Author(Ref No)AgeSexDescriptionTreatmentOutcomeMamelak et al4967FIntracranialSteroidsProgression50FIntracranialSurgical resection (x2), Improvement, then recurrenceSteroids75MIntracranialSteroidsImprovement of symptoms; died of cardiopulmonary arrestMasson et al1258FIntracranialVPshunt, Steroids,Little improvement, dead 10 yrs laterRadiotherapy, azathioprine20FIntracranialSteroids and azathioprineAsymptomatic at 6 years57MIntracranialSteroids, radiotherapy,Little improvement, dead 5 yrs laterazathioprine41MIntracranialAnticonvulsants, VPShuntAsymptomatic at 10 years50FIntracranialAsymptomatic at 1 yearLam et al6064FIntracranialSteroidsSymptoms improved at 11 months41MIntracranialSteroidsSymptoms improved at 7 months70MIntracranialSteroidsSymptoms improved at 12 monthsShintani et al772MIntracranialSteroidsSymptoms improvedNo change in MRI at 2 monthsWilling and Broghamer5 835FIntracranialSteroidsUnavailableMartin et al5720FIntracranialSteroids, then azathioprineAsymptomatic at 15 months58FIntracranialSteroids and radiotherapyProgression58MIntracranialSteroids and azathioprineSymptomatic and MRI improvement at 17 monthsKobayashi et al5640MIntracranialSteroidsLittle improvement, died soon after68MIntracranialSteroids, VPshuntLittle improvement, died of pneumoniapatients with hypertrophic pachymeningitis in which MRI or CTdocumentation was available. Most of these patients wereidiopathic. Since management differs for the cranial (ICHP) andspinal (ISHP) forms, we separately address their mangement here.As illustrated in the Table, therapeutic strategies for ICHPhave included steroids,6-8,12,15,16,45,48,49,56-58azathioprine,12,16,45,57m e t h o t r e x a t e ,1 6c h l o r o q u i n e ,1 6r a d i o t h e r a p y,1 2 , 1 6 , 5 7v e n t r i c u l o-peritoneal shunts,1 2 , 5 6antiepileptic drugs,1 2s u rg e r y4 9a n dobservation.59Corticosteroids can decrease the thickness of duraas documented with MRI,6,8,15,45,46,48,57and can result in dramaticreductions of symptoms and complete remission in somep a t i e n t s .6 - 8 , 1 2 , 1 5 , 1 6 , 4 5 , 4 6 , 4 8 , 4 9 , 5 7 , 6 0Patients may become steroid-dependent.46Azathioprine may be introduced in an effort to taperthe corticosteroids.16Early results from the use of azathioprinehave been promising9 , 1 2 , 1 6 , 4 5 , 5 7but the efficacy of chronicazathioprine therapy remains poorly documented. Radiotherapy probably confers no benefit,1 2 , 5 7d e s p i t eoccasional reports of improvement.16Surgical techniques andventriculoperitoneal shunts have been used with variablesuccess.12,47,49,56An empiric trial of antituberculous therapy maybe warranted in selected patients.5For ISHP, radical surgical treatment has been recommendedas the only therapy.43Others have emphasized early surgicalexcision of thickened dura1 3or surgical decompression bylaminectomy and excision of the involved dura.11 , 6 1Immunosuppressive therapy may obviate the need for extensivesurgery. Corticosteroids may achieve symptomatic control9,11and reduction in dural thickness,13which can be virtuallycomplete, as in our second patient. Decompression may be necessary but our first patientdemonstrates that extensive laminectomies can result in chronicdiscomfort. If decompressive surgery is necessary,laminoplasties should be performed instead of laminectomies,because of the enhanced spinal stability and reduced pain aftersurgery.11For the management of ISHP, we advocate surgical biopsyand the use of corticosteroids. Azathioprine may be integratedinto the therapeutic regimen as noted above in the treatment ofICHP. If stabilization of neurological function is not achieved,surgical intervention with laminoplasty and excision of involveddura should be done.The need for long-term prospective studies of patients withIHPis apparent. Because of the rarity of the disorder, a NorthAmerican or worldwide registry of patients may be warranted.E fforts to delineate the epidemiology and underlyingautoimmune components of the pathogenesis should beemphasized.REFERENCES1.Botella C, Orozco M, Navarro J, Riesgo P. Idiopathic chronichypertrophic craniocervical pachymeningitis: case report.Neurosurgery 1994; 35:1144-1149.2.Friedman D, Flanders A, Tartaglino L. Contrast-enhanced MRimaging of idiopathic hypertrophic craniospinal pachymeningitis.AJR Am J Roentgenol 1993; 160:900-901.3.Uemura K, Matsummura A, Kobayashi E. Idiopathic chronichypertrophic craniocervical pachymeningitis: case report.Neurosurgery 1995; 37:358.4.Ashkenazi E, Constantini S, Pappo O, et al. Hypertrophic spinalpachymeningitis: report of two cases and review of the literature.Neurosurgery 1991; 28:730-732.5.Parney IF, Johnson ES, Allen PBR. Idiopathic cranialhypertrophic pachymeningitis responsive to antituberculoustherapy: case report. Neurosurgery 1997; 41:965-971.6.Kitai R, Sato K, Kubota T, et al. Hypertrophic cranial pachy-meningitis involving the pituitary gland: a case report. SurgNeurol 1997; 48:58-63.7.Shintani S, Shiigai T, Tsuruoka S. Hypertrophic cranialpachymeningitis causing unilateral blindness: MR findings. ClinNeurol Neurosurg 1993; 95:65-70.8.Tanaka M, Suda M, Ishikawa Y, et al. Idiopathic hypertrophiccranial pachymeningitis associated with hydrocephalus andmyocarditis: remarkable steroid-induced remission ofhypertrophic dura mater. Neurology 1996; 46:554-556.9.Adler JR, Sheridan W, Kosek J, Linder S. Pachymeningitisassociated with a pulmonary nodule. Neurosurgery 1991; 29:283-287.10.Fujimoto M, Kira J, Murai H, et al. Hypertrophic cranialpachymeningitis associated with mixed connective tissue disease;a comparison with idiopathic and infectious pachymeningitis.Intern Med 1993; 32:510-512.11.Kanamori M, Matsui H, Terahata N, Tsuji H. Hypertrophic spinalpachymeningitis: a case report. Spine 1997; 22:1787-1790.12.Masson C, Henin D, Hauw JJ, et al. Cranial pachymeningitis ofunknown origin: a study of seven cases. Neurology 1993;43:1329-1334.13.Mikawa Y,Watanabe R, Hino Y, Hirano K. Hypertrophic spinalpachymeningitis. Spine 1994; 19:620-625.14.Feringa ER, Weatherbee L. Hypertrophic granulomatous cranialpachymeningitis causing progressive blindness in a chronicdialysis patient. J Neurol Neurosurg Psychiat 1975; 38:1170-1176.15.Goyal M, Malik A, Mishra NK, Gaikwad SB. Idiopathichypertrophic pachymeningitis: spectrum of disease.Neuroradiology 1997; 39:619-623.16.Hamilton SR, Smith CH, Lessell S. Idiopathic hypertrophic cranialpachymeningitis. J Clin Neuro Ophthalmol 1993; 13:127-134.17.Sharma V, Newton I, Wahal KM. Idiopathic hypertrophicpachymeningitis - an uncommon cause of cord compression.Indian J Pathol Microbiol 1992; 35:133-136.18.Tamaki N, Kojima N, Tanimoto M, et al. Myelopathy due to diffusethickening of the cervical dura mater in Maroteaux-Lamysyndrome: report of a case. Neurosurgery 1987; 21:416-419.19.Hochman LS, Naidich TP, Kobetz SA, Fernandez-Maitin A. Aspontaneous intracranial hypotension with pachymeningealenhancement on MRI. Neurology 1992; 42:1628-1630.20.Delaney P. Neurologic manifestations in sarcoidosis. Ann InternMed 1977; 87:336-345.21.Healton EB, Zito G, Chauhan P, Brust JCM. Intracranial subduralsarcoid granuloma. J Neurosurg 1982; 56:728-731.22.Ranoux D, Devaux B, Lamy C, et al. Meningeal sarcoidosis,pseudo-meningioma, and pachymeningitis of the convexity. JNeurol Neurosurg Psychiat 1992; 55:300-303.23.Sethi KD, Gammal TE, Patel BR, Swift TR. Dural sarcoidosispresenting with transient neurologic symptoms. Arch Neurol1986; 43:595-597.24.Agdal N, Hagdrup HK, Wantzin GL. Pachymeningitis cervicalishypertrophica syphilitica. Acta Derm Venereol 1980; 60:184-186.25.Moore A P, Rolfe EB, Jones EL. Pachymeningitis cranialishypertrophica. J Neurol Neurosurg Psychiat 1985; 48:942-944.26.Callebaut J, Dormont D, Dubois B, et al. Contrast-enhanced MRimaging of tuberculosis pachymeningitis cranialis hypertrophica:case report. AJNR Am J Neuroradiol 1990; 11:821-822.27.Yamashita K, Suzuki Y, Yoshizumi H, et al. Tu b e r c u l o u shypertrophic pachymeningitis involving the posterior fossa andhigh cervical region. Neurol Med Chir 1994; 34:100-103.28.Kawano Y, Kira J. Chronic hypertrophic cranial pachymeningitisassociated with HTLV-I infection. J Neurol Neurosurg Psychiat1995; 59:435-437.29.Murai H, Kira J, Kobayashi T, et al. Hypertrophic cranialpachymeningitis due to A s p e rgillus flavus. Clin NeurolNeurosurg 1992; 94:247-250.30.Schiess RJ, Coscia MF, McClellan GA. Petriellidium boydiiLE JOURNALCANADIEN DES SCIENCES NEUROLOGIQUESVolume 27, No. 4  November 2000339pachymeningitis treated with miconazole and ketoconazole.Neurosurgery 1984; 14:220-224.31.Selby R. Pachymeningitis secondary to Allescheria boydii. JNeurosurg 1972; 36:225-227.32.Leiberman A, Tovi G, Hirsch M. Pachymeningitis presentingfeature of posterior sinus infection. Eur J Pediatr 1986; 144:583-585.33.Nemzek W, Postma G, Poirier V, Hecht S. MR features ofpachymeningitis presenting with sixth-nerve palsy secondary tosphenoid sinusitis. AJNR Am J Neuroradiol 1995; 16:960-963.34.Newton RW.Tuberculosis meningitis. Arch Dis Child 1994; 70:364-366.35.Bathon JM, Moreland LW, DiBartolomeo AG. Inflammatory centralnervous system involvement in rheumatoid arthritis. SeminArthritis Rheum 1989; 18:258-266.36.Hauge T, Magnaes B, Loken AC, de Graaf AS. Treatment ofrheumatoid pachymeningitis involving the entire thoracic region.Scand J Rheumatol 1978; 7:209-211.37.Olmos PR, Falko JM, Rea GL, et al. Fibrosing pseudotumor of thesella and parasellar area producing hypopituitarism and multiplecranial nerve palsies. Neurosurgery 1993; 32:1015-1021.38.Berger JR, Snodgrass S, Glaser J, et al. Multifocal fibrosclerosiswith hypertrophic intracranial pachymeningitis. Neurology 1989;39:1345-1349.39.Kitano A, Shimomura T, Okada A, Takahashi K. Multifocalfibrosclerosis with intracranial pachymeningitis. Intern Med1995; 34:267-271.40.Nishino H, Rubino FA, Parisi JE. The spectrum of neurologicinvolvement in We g e n e rs granulomatosis. Neurology 1993;43:1334-1337.41.Rosenfield JV, Kaye AH, Davis S, Gonzales M. Pachymeningitiscervicalis hypertrophica. J Neurosurg 1987; 66:137-139.42.Kao K, Huang C, Shan D, et al. Non-obstructive idiopathicpachymeningitis cervicalis hypertrophica. J Neurol NeurosurgPsychiat 1986; 49:1441-1444.43.Guidetti B, LaTorre E. Hypertrophic spinal pachymeningitis. JNeurosurg 1966; 26:496-503.44.Friedman DP, Flanders AE. Enhanced MR imaging of hypertrophicpachymeningitis. AJR 1997; 169:1425-1428.45.Jacobson DM, Anderson DR, Rupp GM, Warner JJ. Idiopathichypertrophic cranial pachymeningitis: clinical-radiological-pathological correlation of bone involvement. J Neuro-ophthalmol 1996; 16:264-268.46.Kadoya C, Soejima T,Yamada H, Yokota A. Pachymeningo-encephalitis: case report. Neurosurgery 1993; 33:131-135.47.Kioumehr F,Au A, Rooiholamini SA, et al. Idiopathic hypertrophiccranial pachymeningitis: a case report. Neuroradiology 1994;36:292-294.48.Phanthumchinda K, Sinsawaiwong S, Hemachudha T,Yodnophaklao P. Idiopathic hypertrophic cranialpachymeningitis: an unusual cause of subacute and chronicheadache. Headache 1997; 37:249-252.49.Mamelak AN, Kelly WM, Davis RL, Rosenblum ML. Idiopathichypertrophic cranial pachymeningitis: report of three cases. JNeurosurg 1993; 79:270-276.50.Huang H, Haq N. Spinal leptomeningeal sarcoidosis.Neuroradiology 1987; 29:100.51.Osenbach RK, Blumenkopf B, Ramirez H, Gutierrez J. Meningealneurosarcoidosis mimicking convexity en-plaque meningioma.Surg Neurol 1986; 26:387-390.52.Tasdemiroglu E, Nazek M, Zuccarello M. Sarcoidosis en-plaque.Report of a case and review of the literature. Neurosurg Rev1997; 20:269-273.53.Keime F, DeRecondo A, Mikol J, et al. Sarcoid pachymeningitis:importance of meningeal biopsy (French). Ann Med Interne(Paris) 1996; 147:120-122.54.Powers JM. Sarcoidosis of the tentorium with cortical blindness. JClin Neuro ophthalmol 1985; 5:112-115.55.Belec L, Cohen L, Dormont D, et al. Pachymeningitis complicatingsarcoidosis: monitoring the course under corticosteroid therapywith magnetic resonance imaging (French). Rev Neurol 1991;147:743-747.56.Kobayashi N, Hongo K, Kawauchi M, et al. Chronic meningitiswith marked unilateral tentorial pachymeningitis. Surg Neurol1985; 23:529-535.57.Martin N, Masson C, Henin D, et al. Hypertrophic cranialpachymeningitis: assessment with CTand MR imaging. AJNRAm J Neuroradiol 1989; 10:477-484.58.Willing SJ, Broghamer W. Internal carotid artery occlusion due toidiopathic cranial pachymeningitis. AJNR Am J Neuroradiol1992; 13:1594-1596.59.Nishio S, Morioka T,Togawa A, et al. Spontaneous resolution ofhypertrophic cranial pachymeningitis. Neurosurg Rev 1995;18:201-204.60.Lam BL, Barrett DA, Glaser DS, et al. Visual loss from idiopathicintracranial pachymeningitis. Neurology 1994; 44:694-698.61.Digman KE, Partington CR, Graves VB. MR imaging of spinalpachymeningitis. J Comput Assist Tomogr 1990; 14:988-990.THE CANADIAN JOURNALOF NEUROLOGICALSCIENCES340Avoidance of Power Draining in Mobile Ad-HOC Network over Vampire Attack Anam
ika Garg
#1, Mayank K Sharma
#2 PG scholar, Department of CSE, RGPV University 
Asst. Professor, Department of CSE, RGPV University 
Abstract 
MANET stands for Mobile ad hoc network which is 
independent of any fixed infrastructure or centralized 
administration. In exacting, a MANET has no base stations: a 
node communicates directly with nodes within wireless range 

and indirectly with all other nodes using a dynamically-
computed, multi-hop route via the other nodes of the MANET 
For the communication purpose every node contain certain 
energy and used this energy for the communication. This work 
presents a model for evaluating the energy consumption 
behavior of a mobile ad hoc network in the presence of 
vampire attack which is energy draining attack. For the 

simulation NS2.35 used having energy model. For the 
detection and prevention Intrusion Detection technique used 
which is based on the highest energy level of the node. 
Detection technique consider node to be malicious if node 
contains highest energy level of the other nodes.   
Keywords MANET, Denial of Service, Security, Routing, Ad-
Hoc Networks, Sensor Networks, Wireless Networks.
 I. INTRODUCTION
 Mobile ad-hoc Networks is one of growing technology in 
wireless networks field. It may be defined as temporarily or 
infrastructure less topology based collection of autonomous 
nodes. Mobile nodes have self-manageable characteristic 

which help to establish and communicate automatically. 
Furthermore, Dynamic topology na
ture gives facility to join 
or leave network any time. 
 Mobile nodes are collection of
 battery, processor, memory 
storage and suit of transducer. Here, battery may be 
chargeable or fixed nature depends requirement and node 

type. Subsequently, life of node is directly proportional 
with capacity of battery. Degradation in battery leads to 
degradation in node life. The complete phenomena impact 

on overall network performance and degrade the packet 
delivery accordingly. 
 Communication among nodes depends on transmission 
range of mobile node. A wide range communication 
expects intermediate node or router to discover and transfer 
the packet. Here, each mobile node is capable to work as 

node or router or switch. Therefore, it does not require any 
external device to connect mobile nodes or mobile 
networks. 
 Due to wide range applications require distant 

communication and route discovery on intermediate nodes. 

The nodes discover multi-hop 
routes to each other by 
exchanging topology information in the form of control 

messages. Ad-hoc networks are suitable for areas where it 

is not possible to set up a fixed infrastructure such as 
military, transport, aviation and commercial. Since the 
nodes communicate with each other without an 
infrastructure, they provide the connectivity by forwarding 

packets over themselves.
 Routing protocols are used to discover routes from source 

to destination. It may be classified into two category; 

Proactive Routing Protocol or Reactive Routing Protocols. 
Proactive routing protocols discover routes before their 
demand. Where, Reactive routing protocols are on-demand 

routing protocols. Here, reactiv
e routing protocols are more 
suitable for mobile ad-hoc networks due to their on-demand 

route discovery nature. 
 The complete phenomena require lots of energy for route 
discovery and packet transmission. A heavy route discovery 
process or information processing will quickly degrade the 

battery capacity. The complete work observes that, energy 
consumption is the key challenge in mobile ad-hoc 
networks and it should be as possible as least amount.  
 II.RELETED WORKEnergy conservation is an important issue in ad hoc 
networks as nodes are usually battery powered. Even 
though a node may not have any message of its own to 
transmit, its battery is drained when it acts as a router and 
forwards packets for other nodes. In transmit mode a 
mobile unit uses its maximum power level to transfer 
packet to another node. Communication involves the cost of 
sending data along with control packet from source, routed 
through intermediate node and cost at receiving at 
destination end.
 To understand the concept and impact of power draining on 
MANET, work consider certain research work which is 
explain below:
 Zubair A. Baiget. al[1] pr
oposed a solution where it 

concludes that Distributed Denial of Service attack is most 

popular attack for power drai
ning. They also discussed 
about various attack models and impact. They designed 

pattern recognition problem to detect DDOS attack. 

Proposed method improves performance on basis of timely 
and energy-efficient manner. 
 S. N. Premnathet. al. [3] pro
posed a advanced DDOS attack 
They also propose a special 
attack named as SDP Attack 
(Service Discovery Protocol based attack) to make Heavy 
Power Drain on node.SDP sends new service request 

similar to SYN Flooding to create conjunction and 
overwhelming the nodes. SDP attack effectively make it 
inoperable and reducing the battery life by as much as 97%. 
 Ambili M. A, BijuBalakrishnan [4] proposed energy 
draining in WSN and introduced 
energy Based Intrusion 
Detection System. Intrusion Detection method detect 
vampire node on the basis of energy level of the node. All 
nodes that present in the network will have same energy 
level and during the communication little changes will take 
place. Intrusion Detection based on the fact that the 
melicious node will have more power than other node 
because it will consumes othe
r nodes energy so it will ave 
high energy level. Thus all nodes energy level measured 
and the node which have abnormally high energy, 
considered as vampire node. 
 Chahana B. Thakur ,V.B.Vaghela [5] describe vampire 
attack and its types and introduced flag based technique to 

detect and prevent vampire attack
. Flag set to header to it 
cannot take much space and can
 prevent from the repeated 
path as for the carousel attack. 
 III.
 SECURITY
 THREATS
 Security threats come when ba
sic security breaks down and 
this can be any form like ener
gy based security threats. 
 Attacks are classified into two categories:
  Active Attack
  Passive Attack DOS attack is considered to be active attacks which prevent 

the normal use of communication services.
 DOS attacks in wireless network not only cause damage to 
the victim node but decrease the performance of the entire 
network because nodes have a limited battery power and the 
network can be congested due to limited bandwidth.  
 Some recognized attacks are as follows:
 A. Black Hole Attack 
In this attack malicious node drops the entire packet 
transmitted by the sender.
 B. Gray Hole Attack
 Grey Hole attack is similar to 
black hole attack except that 
the malicious node drops selective packets not all the 
packets are dropped.
 C. Worm Hole Attack
 In this attack two malicious nodes participates for 

establishing the tunnel. One node takes the data packet from 
the sender and sends to the other participant (malicious 
node). In this way two malicious node creates tunnel.
 D. Sybil Attack
 In this attack one malicious nodes recognize the network 

and acquires multiple fake identities and pretends to be 

multiple, distinct nodes in the system. 
 E. Vampire Attack
 It is energy darning attack where malicious node consumes 

other nodes energy and downs all the nodes energy.
  Mobile ad-hoc network particular vulnerable to DoS attack. 
Attacks can be of two types, a resource depletion attack and 
a routing disruption attack. [5] A routing disruption attack 
tries to alter the routing path e.g.: Worm hole, Sybil attack, 

etc. whereas the resource depl
etion attack only focuses on 
the battery power and memory
. The most permanent DoS 
attack is to entirely deplete nodes' batteries. 
  This is an instance of a resour
ce depletion attack, with the 
battery power as the resource 
of interest. These attacks are 
commonly called as Vampire attacks. 
 IV. VAMIRE
 ATTACK
 Vampire attack is energy draining attack where messages 

send by the malicious node which causes more energy 
consumption. This energy consumption is very high and 

leading to slow depletion of network nodes battery life.
 [2] Vampire attacks are not protocol-specific, in that they 

do not rely on design properties or implementation faults of 

particular routing protocols, but rather exploit general 
properties of protocol classes 
such as link-state, distance 
vector, source routing and geographic and beacon routing. 
Neither do these attacks rely on flooding the network with 
large amounts of data, but rather try to transmit as little data 
as possible to achieve the larg
est energy drain, so it takes 
large energy to transmit the data and consumes the node 

energy. Since Vampires use protocol-compliant messages, 

these attacks are very diffic
ult to detect and prevent.
 The vampire attacks can be cla
ssified has two types. There 
are: one is Carousel attack 
and other is Stretch attack.
 1)  
Carousel Attack In the Carousel attack, [4] at
tackers introduce some packet 

within a route as a sequence of loops, such that the same 

node appears in the route of
 communication many times. 
This attack increases the routing length and delay very 

much in the net- works and also inadequate by the number 

of allowable entries in the resource route. 
 Series of loops made in this way, such that the same node 

appears in the route many times. [3] This strategy can be 

used to increase the route length beyond the number of 
nodes in the network, only limited by the number of 
allowed entries in the source route.
 Route looping causes high energy consumption which 
drains energy level of the whole network. 
    
 
   
 
 
     Source                                               Destination
 Honest Path 1-2-3-4-5-6
 Carousel Attack Introduce path 1-2-3-4-5-4-3-2-3-4-5-6
 Introduce Loop between 2-3-4-5 
 Fig. Example of Carousel Attack 
 2)  Stretch Attack 
Second attack also [4] targe
ting resource steering, attackers 
construct falsely long routes, 
potentially traversing every 
node in the network. And al
so stretch attack, increases 
packet length, causing packets to be processed by a number 
123456of nodes that is self-governing of hop count down the 
straight path stuck between the 
challenger and packet target.
 For this type of attack malicious node constructs artificially 

long source routes, causing pack
ets to traverse a larger than 
optimal number of nodes. [3] The honest path is very less 
distant but the malicious path is very long to make more 
energy consumption.
      
   
   
   
Source                               Destination 
Honest Path 1-2-3-4-5-6
 Stretch Attack Introduce path 1-2-3-4-7-8--9-5-6
 Fig. Example of Stretch Attack
  Impact of Vampire Attack
 Ad-hoc network communication based on the nodes energy 

level. Vampire attack based on the poser draining of 

network. Presence of vampire attack leads to the failure of 
the node and the network also due to the energy 
consumption of nodes. Data loss will also occur.  
 V. SIMULATION TOOL Work done performed with the help of ns2.35 network 

simulator. This software is a discrete event driven simulator 
and is developed at UC Berkelry as a part if the VINT 
project. It is distributed as open source software. 
 NS2 is built using object oriented language C++ and OTcl 
(Object oriented varient of Tool Command Language). NS-
2 interprets the simulation scripts written in Otcl. User 

writes his simulation as an Otcl script. Some parts of NS2 
are written in C++ for efficiency reason. It also use various 
tools like Nam which is Network animator to visualize the 

running network scenario. Xgraph tool for plotting the 
graph for result analysis. 
 VI. PERFORMANCE
 METRICS
 In order to evaluate the performance, several metrics are 
used, such as packet delivery fraction, end-to-end delay, 

throughput, and power consumption. The most important 
metrics for best-effort traf
fic are the packet delivery 
fraction and the end-to-end delay. However, these metrics 
are not entirely independent as a shorter delay does not 
necessarily mean a higher packet
 delivery fraction, as only 
successfully delivered packet
s are used to measure the 
delay. A lower packet delivery fraction and longer delay 

may, however, cause a larger overhead. 
 F. A. Packet Delivery Fraction 
The ratio of the number of data packets sent by the CBR 

sources and the number of da
ta packets received by the 
CBR sinks at their destination. It shows how reliable a 
protocol is and the higher the PDF, the better the results. 
 G. B. End-to-end Delay 
The average length of time, measured in seconds, necessary 

to deliver a set of data from the source to the destination 
node. 
 It includes all potential delays resulting from queuing at the 
interface queue, propagation and transfer times, and 
retransmission delays at the MAC layer. 
 H. C. Throughput 
The number of bytes successfully transferred to the 

destination during a specific amount of time (s). 
 I. D. Power Consumption 
The average power consumption per node is calculated 
based on the ratio of the total energy consumed by all nodes 

in the network divided by the total number of nodes. 
 TABLE
 I SIMULATION
 PARAMETER 
Channel Channel/WirelessChannel Propagation Propagation/TwoRayGround
 Network Interface Phy/WirelessPhy
 Platform
 Ubuntu 13.10 / 15.04 NS Version Ns-allinone-2.35 MAC Mac/802_11 Interface Queue
 Queue/ DropTail / PriQueue
 Link Layer LL Antenna Antenna/OmniAntenna
 Interface Queue 
Length 50 No. of Nodes
 20 Simulation area size
 750*750 Traffic Pattern
 CBR Sessions
 CBR Packet Size
 512 bytes Simulation Duration 130 seconds  TABLE
 IIIII 
PERFORMANCE
 EVALUATION 
 VII.
 CONCLUSION
 This paper introduces energy draining attack and the 

proposed method to detect and remove it by using intrusion 
detection method. Intrusion detection method detect 
malicious node on the basis of energy level of nodes and 

remove that node from the current path. Thus the securing 
of network nodes energy is carried out and finding alternate 
path for route after detection is done. The simulation result 

proves the improvement in the energy consumed and 
throughput and decreases hop count and save the nodes 
energy. By avoiding vampire attack this work will increase 

life of the node and the network. It will be very crucial in 
  Normal AODV 
AODV with Vampire Attack 
Modified AODV 
Throughput(b/s) 
84.78 25.29 52.72 Total Energy 
200.813 259.39 215 Protocol Energy 
50.203 64.849 53.813 312478956REFERENCES [1]  Getsy S Sara1, S.NeelavathyPari, D. Sridharan,EVALUATION 
AND COMPARISON OF EMERGING ENERGY EFFICIENT 
ROUTINGPROTOCOLS IN MANET 
in proceedings published in 
ICTACT JOURNAL OF COMMUNICATION TECHNOLOGY, 
MARCH 2010.
 [2]  V.Sharmila1, Mr. K. MuthuRamalingam Energy Depletion Attacks: 
Detecting and Blocking in Wireless 
Sensor Network IJCSMC, Vol. 
3, Issue. 8, August 2014
 [3]  P.Suthahar, IIR.Bharathinternational Defending against Energy 
Draining attack in Wireless Ad-Hoc Sensor Networks  Journal of 

Advanced Research in Computer Science & Technology (IJARCST 

2014) Vol. 2 Issue Special 1 Jan-March 2014.
 [4] Ambili M. A, BijuBalakrishnan Vampire Attack : Detection and 
Elimination in Wsn.
 [5]  ChahanaB. Thakur ,V.B.Vaghe
la Detection and Elimination of 
Vampire Attack in Mobile Ad hoc Network INDIAN JOURNAL 
OF APPLIED RESEARCH Volume - 5 | Issue - 1 | Jan Special Issue 
- 2015 | ISSN - 2249-555X
 [6]  Eugene Y. Vasserman , Nicholas
 Hopper, Vampire attacks: Draining 
life from wireless ad-hoc sensor networks.2011.
 [7]  ImadAad, Jean- Pierre Hubaux, 
and Edward W.Knightly, Denail of 
service resilience in ad hoc networks, mobicom,2004 
 
 
   
      
   
   
   
 
   
   
   
   
     A VECTOR SPACE MODEL FOR INFORMATION RETRIEVAL: A MATLAB APPROACH  A. B. Manwar 
Department of Computer Science, 
S.G.B. Amravati University, Amravati MS, India 
avinash.manwar@gmail.com
 Hemant S. Mahalle Department of Computer Science, 
P. N. College, Pusad, Dist. Yavatmal  MS, India 
mahalle_hemant@yahoo.co.in
 K. D. Chinchkhede Department of Applied Electronics, 
S.G.B. Amravati University, Amravati MS, India  
krish.chinchkhede@gmail.com
 Dr. Vinay Chavan 
Department of Computer Science, 
S. K. Porwal College, Kamptee, Nagpur MS, India 
drvinaychavan@yahoo.co.in
    
Abstract 
By and large, three classic framework models have been used in the process of retrieving information: Boolean, 
Vector Space and Probabilistic. Boolean model is a light weight model whic
h matches the query with precise 
semantics. Because of its boolean nature, results may be tides, missing partial matching, while on the contrary, 

vector space model, considering term-frequency, inve
rse document frequency measures, achieves utmost 
relevancy in retrieving documents in information retrieva
l. This paper implements and discusses the issues of 
information retrieval system with
 vector space model using MATLAB 
on Cranfield data collection of 
aerodynamics domain. 
Keywords
: tf; idf; vector space model; cosine similarities; 
term-document; term-query matrices; dot products. 
1. Introduction 
Enormous amount of text material is increasing at e
xponential rate, especially with the increasing use and 
applications of Internet. Day by day it is becoming very 
difficult to retrieve the relevant information. Various 
approaches have been used by the researchers to get 
over the relevancy factor in information retrieval. 
An information retrieval model is a quadruple consistin
g of document collection, set of queries, framework 
model and a ranking function associated with query-document. A framework model may be boolean, vector 
space or probabilistic. Boolean model matches query with
 precise semantics in the document collection by 
boolean operations with operators AND, OR, NOT. It predicts either relevancy or non-relevancy of each 
document, leading to the disadvantage of retrieving very few or very large documents. The boolean model is the 
lightest model having inability of partial matching which lead
s to poor performance in retrieval of information. 
Vector space model is introduced by G. Salton in late 19
60s in which partial matchi
ng is possible. Non-binary 
weights are used to weight the index terms in queries and in documents. These words are used for calculating 

degree of similarity between each docu
ment and the query. The ranked docume
nt set in the decreasing order of 
degree of similarity thus obtained is precise than the result of boolean model. Index term weights can be 
calculated in many different ways. The work by Salton and McGill [1] reviews various term-weighting 
techniques. Although there is contention as to whether 
probabilistic model outperforms the vector model, Salton 
 and Buckley [2] showed that the vector model is expected to outperform the probabilistic model with general 
collections [3]. This paper implements and discusses the issues of info
rmation retrieval system with vector space model using 
MATLAB on Cranfield data collection of aerodynamics domain.  
The next section deals with brief review of related 
work of vector space model in information retrieval. 
2. Related work 
Maron and Kuhns [4] in early 1960, described probabilistic indexing technique in a mechanized library system 

yielding probable relevance. Afterword in 1983, Salton and McGill wrote a book [1] which discusses thoroughly 
the three classic models in information retrieval namely, the boolean, the vector, and the probabilistic models. 
The book by van Rijsbergen [5] covers the discussion on
 three classic models and ma
jority of the associated 
technology of retrieval system. Frakes and Baeza-Yates [6] edited the book on information retrieval which 
mainly deals with the data structures used in general in
formation retrieval systems. Also, it includes the issue of 
relevance feedback as well as some query modification techniques [7] and Boolean operations and their 
implementations [8]. Verhoeff, Goffman, and Belzer [9] described the shortfall of boolean queries for 
information retrieval. The concept of using boolean formalism in other frameworks had been the great interest 
area of the researchers. Lee et al proposed a thesau
rus-based boolean retrieval system for ranking [10]. 

Vector space model has been the most popular model in
 information retrieval among the research vicinity 
because of the research outcome in indexing, term va
lue specification in automatic
 indexing carried out by 
Salton and his associates [11, 12]. Most of this research deals with experiments in automatic document 

processing and different term weighting approaches for automatic retrieval [2, 13]. In 1972, Karen Sparck Jones 
introduced the concept of inverse doc
ument frequency, a measure of specifi
city [14, 15] and Salton and Yang 
uses it for automatic indexing to improve retrieval [12]. Raghavan and Wong [16] analyses vector space model 
critically with the conclusion that the vector space model is useful an
d which provides a formal framework for 
the information retrieval systems.   

The next section gives a description of
 the most influential vector space m
odel in modern information retrieval 
research.  
3. Vector Space Model The drawback of binary weight assi
gnments in boolean model is remediat
ed in the vector space model which 
projects a framework in which partial matching is possible [11, 13]. Non-binary weights for index terms in 
queries and documents are used in the calculation of degree of similarity. Decreasing order of this degree of 
similarity for the retrieved documents gives the ranked documents with partial match. 
For the vector model, the weight
 
 associated with a pair
  is positive and non-binary. Further, the 
index terms in the query are also weighted. Let
 
 be the weight associated with the pair
, where
 

. Then, the query vector
 
 is defined as
 

  where
 t is the total number of index 
terms in the system. The vector for a document
 dj  is represented by
 


.   The vector model proposes to evaluate th
e degree of similarity of the document 
dj with regard to the query 
q as the correlation between the vectors 
 and 
. This correlation can be measured by the cosine of the angle
 between these two vectors as,  
                                                

                                                                                  






  where 
and 
 are the norms of the document 
and query vector
s. The factor 

 does not affect the ranking 
(i.e., the ordering of the documents) because 
it is the same for all documents. The factor 
 provides 
normalization in the space of the documents.  
Since 
 and  
, 
 varies from 0 to +1, the vector m
odel ranks the documents according to 
their 
degree of similarity 
to the query. A document might be retrieved even if it matches the query only 
partially.
 [3, page 27-28]. 
   In the vector space model, the frequency of a term 
ki inside a document 
dj referred to 
as the 
tf factor and 
provides one measure of how well that term describes the document contents. Furthermore, the inverse of the 
frequency of a term 
ki among the documents in the collection referred to as the 
inverse document frequency 
or the 
idf  
factor.   The normalized frequency
 fi,j of term
 ki in document
 dj is given by 
 
      
(1)
 where the maximum is computed over all terms which are mentioned in the text of the document
 dj. The  idfi, inverse document frequency for
 ki, be given by 
 
     
(2) 
The best known term-weighting schemes use weights which are given by 
 


    (3) 
 Such term-weighting strategies are called
 tf-idf 
schemes [3, page 29-30].
  The success of vector space model lies in
 its term-weighting scheme, its partia
l matching strategy and similarity 
measure. Mutual independence of index terms has said to be disadvantage of vector space model but practically, 
consideration of term dependencies is not fruitful. From 
the research consequence in the field, it seems that the 
vector model is either superior or almo
st as good as the 
known alternatives. 
4. Experimental Evaluations 
4.1.
  Dataset for information retrieval system 
We used a Cranfield collection having 1398 abstracts related to aerodynamics domain which is obtained from 
[17]. The collection contains a compressed version of document text, relation giving relevance judgements, text 
of 225 queries, indexed documents and indexed queries. Also, we used stop-list obtained from the collection 
source. 4.2.
  Preprocessing 
The compressed version of document text has been preprocessed to obtain a set of 1398 individual abstract files. 
A PHP script has been written for this purpose. Stemming ha
s not applied for the reasons of i) losing context of 
search, ii) may reduces precision and iii) can not be applied 
to proper nouns. To have a 
clear view of relevancy, 
instead of using given set of query and relevance judgement
s, we have created our set of queries from titles of 
1398 documents, forming a query set of 1398 entities.  
4.3.
 Implementation 
A MATLAB is used to implement a vector space model 
for information retrieval; 
the complete process is 
depicted in fig. 1. 
   
   
   
  Fig. 1. Implementation of vector space model for information retrieval. 
   As shown in block diagram it consists of three stages:  
1. Generation of Term-Frequency matrix 
It is term-frequency matrix of all unique terms in document 
 with 

. The term document 
matrix (FM) is 

 matrix with 
 unique terms in dictionary             

 and 
 documents. The elements of FM are represented as  

 in which each element indicates the frequency 
of 
 term in 
 document.  
The Cranfield data collectiont is preprocessed to convert into individual 1398 text files. A SMART 
stop word text file which is available with the dataset is used for the removal of stop words from the 
data collection of 1398 files. Also, non-embedding special characters and numerals have been removed 
from these files. 79,728 words ha
ve been collected which are then 
processed to find the frequency of 
unique words in each documents. The dictionary of 
unique words is of 7805 
words. Thus the term-
frequency matrix is of size 7805x1398. 

 2. Generation of Query matrix 
The title of each abstract, after removing stop-list 
words and non-embedding special characters is used 
as query, which contributes to the set of 1398 unique queries represented as 
. Here, we have taken 
queries as titles of the document instead of the dataset queries so as to judge the relevancy more 
profoundly. The generated matrix for 1398 queries is 

  3. Term-weight calculations and result 
A term-frequency matrix is processed to get the term weights considering 
tf-idf
 scheme. These term 
weights are calculated by the formula

, where 
 
  is the term-weight i.e. weight of term 
 in document 
, 
  is the frequency of term 
 in document 
,   is the inverse document frequency representing the terms appearing in many 
documents and is calculated by the formula 
, where  is the total number of 
documents and 
 is the number of documents containing term 
,   is the Euclidean length obtaining by taking square root of sum of squares of 
individual terms per document. 
 Query matrix of size 

is also divided by their corresponding
 Euclidean 
lengths to obtain the 
normalized weights. For query
 , the transpose of query matrix is 
multiplied by the term-weight matrix. 
The final result is obtained by ordering the weights in a result matrix in decreasing manner of their weights.
 4.4.
  Testing phase 
The vector space model (VSM) implemented above is tested thoroughly in different ways, the details of 
experimentation is explained below: 

Fig. 2 shows the distribution of index terms in dictionary for individual documents. The dictionary consists 

of 7805 unique terms. 
 Fig. 2. Index terms in a dictionary 
Fig. 3 shows frequency count of each unique term in dic
tionary distributed in complete dataset. Some of the 
unique terms such as (flow, 2059), (pressure, 1245), (boundary, 1076), (results, 897), with high 

frequency in entire documents is shown. 
  Fig. 3. Frequency count of each 
unique term among data collection 
 Further looking into the dataset, as shown in fig. 4, shows subset of 380 documents out of 1398 abstracts. In 

document 329 the total terms are 342 but the unique terms are only 162. 
   
 
 
 
     Fig. 4. Document-wise total and unique terms 
 Table I is the subset of 20 queries out of 1398 tested queries against the vector 
space model for retrieval of 
relevant documents. In the present study the most relevant document is one whose ID matches with query 

ID.   

Table I is the subset of 20 queries out of 1398 tested queries against the vector 
space model for retrieval of 

relevant documents. In the present study the most relevant document is one whose ID matches with query 

ID.   
 Table 1. Subset of 20 queries 
 Q.ID 
Details of  Queries 
1  experimental investigation of the aer
odynamics of a wing in a slipstream .  
2  simple shear flow past a flat plate in an incompressible fluid of small viscosity .  
3  the boundary layer in simple sh
ear flow past a flat plate .  
4  approximate solutions of the incompressible laminar 
boundary layer equations for a plate in shear flow .  
5  one-dimensional transient heat conduction in
to a double-layer slab subjected to a lin
ear heat input for a 
small time interna
l . 6  one-dimensional transient heat flow in a multilayer slab .  
7  the effect of controlled three-
dimensional roughness on boundary layer transition at supersonic speeds .  
8  measurements of the effect of two-dimensional and thr
ee-dimensional roughness elements on boundary layer transition .  
9  transition studies and skin friction measurements on an
 insulated flat plate at a mach number of 5.8 .  
10  the theory of the impact
 tube at low pressure .  
11  similar solutions in compressi
ble laminar free mixing problems .  
12  some structural and aerelastic considerations of high speed flight .  
13  similarity laws for stressing heated wings .  
14  piston theory - a new aerodyna
mic tool for the aeroelastician .  
15  on two-dimensional panel flutter .  
16  transformation of the compressible turbulent boundary layer .  
17  remarks on the eddy viscosity in compressible mixing flows .  
18  the flow field in the diffuser of a radial compressor .  
19  an investigation of the pressure distribu
tion on conical bodies in hypersonic flows .  
20  generalised-newtonian theory .  
   The result of experimentation is tabulated in Table 2 shown below: 
 
   
    Table 2. Tabulation of results of first ten queries 
    Further query wise details are depicted in subsequent Table 3 to Table 5 respectively. 
 Table 3. Frequency of terms in 
retrieved document set for query 1 
 Table 4. Frequency of terms in 
retrieved document set for query 2 
  Table 5. Frequency of terms in 
retrieved document set for query 3 
    Table 3 depicts the result obtained for query 1 with query terms and their occurrences in the documents. First 18 
documents are listed. Similar results are shown in ta
ble 4 and table 5 for queries 2 and 3 respectively. 
5. Results and discussion 
Even after taking the titles of the abstract documents as a query set, the final result of retrieval is 89.41%. 148 

queries have not shown result as first retrieved document; however, within the range of first two retrieved 
documents, the result obtained is 94.99%. 2.22% of queries have not retrieved the correct result up to the range 
of first five documents.  

Inter-document characterization and document frequency plays vital role in building ranks of the documents in 
vector space model. As such, te
rm frequency of the documents d
3, d389 and d
2 is 24, 42 and 110 respectively; and 
the frequency of terms 
flow
, plate
 and 
small
 in all documents is 2059, 421 and 306 respectively, which is 
much more higher than the average frequency- as 
a result, table IV shows the outcome of query q
2 as document 
d3; however, expected relevant document is d
2. Variety of the weight calculation formulas as suggested by 
Salton and Buckley [2] have been tested on this collection but we found that the standard 
tf-idf
 weighting 
scheme gives the best results. 
References 
[1] G. Salton and M. J. McGill. 
Introduction to Modern Information Retrieval.
 McGraw-Hill Book Co., New York, 1983. 
[2] G. Salton and C. Buckley. 
Term-weighting approaches in automatic retrieval.
 Information Processing and Management, 
24(5):513-523, 1988. 
[3] Christopher D. Manning, Prabhakar Raghavan, and Hi
nrich Schutze, 
Introduction to Information Retrieval,
 Cambridge 
University Press, New York, USA, 2008. 
[4] M. E. Maron and J. L. Kuhns. 
On relevance, probabilistic indexing and information retrieval
. Association for Computing 
Machinery, 7(3):216-244, 1960. 
[5] C. J. van Rijsbergen. 
Information Retrieval. Butterworths, London, 1979. 
[6] W. B. Frakes and R. Baeza-Yates. 
Information Retrieval: Data Structures and Algorithms
. Prentice Hall, Englewood Cliffs, NJ, 
USA, 1992. 
[7] D. Harman. 
Relevance feedback and other query modification techniques
. In W. B. Frakes and R. Baeza-Yates, editors, 
Information Retrieval: Data Structures
 and Algorithms, pages 241-263. Prentice 
Hall, Englewood Cliffs, NJ, USA, 1992.  
[8] S. Wartick. Boolean operations. In W. 
B. Frakes and R. Baeza-Yates, editors
, Information Retrieval: Data Structures and 
Algorithms
, pages 264-292. Prentice Hall, Englewood Cliffs, NJ, USA, 1992.  
[9] J. Verhoeff, W. Goffmann, and Jack Belzer. 
Inefficiency of the use of Boolean func
tions for information retrieval systems
. Communications of the ACM, 4(12):557-558, 594, December 1961. 
[10] J. H. Lee, W. Y. Kim, and Y. H. Lee. 
Ranking documents in thesaurus-
based Boolean retrieval systems
. Information Processing 
and Management, 30(1):79-91, 1993. 
[11] G. Salton and M. E. Lesk. 
Computer evaluation of indexing and text processing.
 Journal of the ACM, 15(1):8-36, January 1968. 
[12] Gerad Salton and C. S. Yang. 
On the specification of term values in automatic indexing
. Journal of Documentation, 29:351-372, 
1973. 
[13] G. Salton. The 
SMART Retrieval System  Experiment
s in Automatic Document Processing
. Prentice Hall Inc., Englewood 
Cliffs, NJ, 1971. 
[14] K. Sparck Jones. A statistical interpretation of term sp
ecificity and its application to retrieval
. Journal of Documentation, 
28(1):11-20, 1972. 
[15] K. Sparck Jones. A statistical interpretation of term sp
ecificity and its application to retrieval
. Information Storage and Retrieval, 
9(11):619-633, 1973. 
[16] V. V. Raghavan and S. K. M. Wong. A 
critical analysis of vector space model for information retrieval
. Journal of the American 
Society for Information Sciences, 37(5):279-287, 1986. 
[17] ftp server of Co
rnell University ftp://ftp.cs.cornell.edu/pub/smart/cran/
 for Cranfield collection. 
DOI:10.1046/j.1365-
2125.2003.01968.x
British Journal of Clinical Pharmacology
 2003 Blackwell Publishing LtdBr J Clin Pharmacol57:2127134127
Blackwell Science, LtdOxford, UKBCP
British Journal of Clinical Pharmacology0306-5251Blackwell Publishing 2003? 2003572127134Review Article
Data mining in pharmacovigilanceA. M. Wilson 
et al.CorrespondenceDr Anne E Holbrook, Director, Centre 

for Evaluation of Medicine, 105 Main 

Street East, Level P1, Hamilton, 
Ontario L8N 1G6, Canada.Keywordsadverse drug events, data mining, knowledge discovery in databases, 
pharmacovigilance 
Received24 April 2003
Accepted

28 July 2003
Application of data mining techniques in pharmacovigilance
Andrew M. Wilson,
1,2 Lehana Thabane
2,3 & Anne Holbrook1,2,31Division of Clinical Pharmacology, Department of Medicine, 
2Centre for Evaluation of Medicines and 
3Department of Clinical Epidemiology 
and Biostatistics, McMaster University, 105 Main Street East, Level P1, Hamilton, ON L8N 1G6, Canada
AimsTo discuss the potential use of data mining and knowledge discovery in databasesfor detection of adverse drug events (ADE) in pharmacovigilance.
MethodsA literature search was conducted to identify articles, which contained details of data

mining, signal generation or knowledge discovery in relation to adverse drug reactions
or pharmacovigilance in medical databases.
ResultsADEs are common and result in signicant mortality, and despite existing systems

drugs have been withdrawn due to ADEs many years after licensing. Knowledge
discovery in databases (KDD) is a technique which may be used to detect potential
ADEs more efciently. KDD involves the selection of data variables and databases,

data preprocessing, data mining and data interpretation and utilization. Data mining
encompasses a number of statistical techniques including cluster analysis, link anal-
ysis, deviation detection and disproportionality assessment which can be utilized to

determine the presence of and to assess the strength of ADE signals. Currently the

only data mining methods to be used in pharmacovigilance are those of dispropor-

tionality, such as the Proportional Reporting Ratio and Information Component, which

have been used to analyse the UK Yellow Card Scheme spontaneous reporting

database and the WHO Uppsala Monitoring Centre database. The association of

pericarditis with practolol but not with other b-blockers, the association of captopriland other angiotensin-converting enzymes with cough, and the association of ter-

fenadine with heart rate and rhythm disorders could be identied by mining the WHO

database.ConclusionIn view of the importance of ADEs and the development of massive data storage

systems and powerful computer systems, the use of data mining techniques in

knowledge discovery in medical databases is likely to be of increasing importance in
the process of pharmacovigilance as they are likely to be able to detect signals earlier
than using current methods.IntroductionAdverse drug events (ADEs) constitute a major public
health problem. They are estimated to account for up to

5% of hospital admissions [1], 28% of all emergency
department visits [2], and 5% of hospital deaths [3].Many ADEs, such as prescription errors, are preventable

[4], and methods have been suggested to reduce these

errors [5, 6]. However other ADEs are unknown at the
A. M. Wilson et al.
12857:2Br J Clin Pharmacoltime of marketing as premarketing studies are generally
small, of a short duration, do not detect late-onset or rare
adverse effects and, by excluding patients with comor-

bid disease, have limited generalizability. For example

the interaction between terfenadine and cytochromal

p450 enzyme inhibitors causing cardiac arrhythmias

was only recognized 7 years after licensing [7]. Further-
more, clinical trials typically underplay harm in their
reporting [8]. For example, increased risks of serious

cardiac events with rofecoxib were suggested by a

further analysis [9] of the VIGOR study [10], which

occurred a year after the marketing of this therapy.
In the current regulatory environment, where efca-
cious drugs are brought to market as soon as possible

[11], post-marketing drug surveillance (PMS) has

become increasingly important in order to characterize
cost-effectiveness and harm in real practice. Recent

examples of drug withdrawals, due to uncommon ADEs
after millions of patients were exposed [12], have rein-

forced the inadequacy of current methods of PMS.
Over recent years, with the development of large
electronic health data storage systems, powerful com-

puters and new statistical algorithms, there has been an

increased interest in data mining or knowledge discov-

ery initiatives from databases. These processes, which

were mainly used in the marketing industry, have gained

popularity in various elds such as web mining [13] and

information science [14, 15] but very little information

exists on their application in pharmacovigilance [16].
The purpose of this paper is to review the current use of

data mining in pharmacovigilance and provide an over-

view of the data mining process.
MethodsAn electronic search of MEDLINE from 1966 to 2002
identied articles which contained the keywords data-

mining, data mining, signal generation, exploratory

methods, exploratory tools, neural network, dispropor-

tionality, signal detection, higher than expected combi-

nation, signal, data interrogation, database interrogation,

Bayesian, cluster analysis, hypothesis generation,

knowledge discovery, symmetrical analysis, prescrip-

tion event monitoring and adverse drug effect, adverse

drug event, pharmacovigilance, drug side-effect, toxi-

cology, electronic medical records, EMR, large admin-

istrative data, linked administrative data, Medicaid,

MEMO, GPRD, Tayside, Saskatchewan, Unitedhealth

Group, Harvard Pilgrim Health Care. This search

resulted in 340 citations, the title and abstract of which
were reviewed. Sixty-four articles in the English lan-

guage were identied, of which 39 manuscripts were
considered relevant and reviewed. The relevant articles
will be discussed within the context of the data mining
process.What is data mining?There is some confusion in the terminology of datamining. Some authors refer to data mining as the process
of acquiring information, whereas others refer to data
mining as utilization of statistical techniques within the
knowledge discovery process. We will dene knowledge

discovery in databases (KDD) as the process of extract-

ing previously unknown, valid and actionable informa-

tion from large information sources or databases. The

process requires a denition of the project goals, dataset
acquisition, data cleaning and preprocessing, data min-
ing, data interpretation and utilization [17]. We will

dene data mining as the application of statistical
techniques, e.g. predictive modelling, clustering, link

analysis, deviation detection and disproportionality

measures, to databases. All of the above KDD steps are

required when embarking on a project from the begin-

ning. However, some researchers take a given database

and perform statistical analysis in order to mine it for

additional unrecognized information and therefore call
the process data mining.Many of the statistical techniques used in data mining
are the same as conventional methods of examining

data. However, their use within KDD differs in that there

is no prior hypothesis or null hypothesis, and power

calculations are not performed. For this reason, if no

signal is produced at the end of the analysis it is not
possible to tell whether this is because the signal does
not exist or because insufcient data are being analysed.

Essentially, once the aims have been dened and the

database is prepared for data mining, the statistical tech-
niques attempt to nd patterns, or associations or dis-
similarities between groups of data in order to generate
a signal or detect new information.
Steps of knowledge discovery in databases (Table 1)
Identication of objectives/goalsAlthough there is no detailed a priori hypothesis inKDD, and one should keep an open mind when explor-

ing for possible ADEs [18], it is important to have

an aim, as KDD is costly in terms of data collection and

management.Selection of variablesA clearly dened aim makes it easier to determine the

type of variables and data mining technique to be used.

For example, assessing drug-related birth defects would
require a different data set than drugdrug interactions.
Data mining in pharmacovigilance
Br J Clin Pharmacol57:2129
However, in order to maximize the chance of detecting
a signal, the most inclusive collection of relevant vari-

ables should be used. The World Health Organization

(WHO) Adverse Reaction Terminology and Medical

Dictionary for Regulatory Activities (MedDRA) are

examples of datasets used for pharmacovigilance [19],
but their list of variables is limited to those determined
by prior assessments of causation [20]. In terms of vari-

able availability, electronic medical records (see below)

hold potential in being the repository of the widest pos-
sible medical terminology. For example, new medical

conditions, such as Severe Acute Respiratory Syndrome,

will be used within electronic medical records before
they are incorporated into adverse reaction terminology

dictionaries.Selection of data sources/databasesAs data collection is expensive, data mining processes

are often performed on existing databases, for the pur-

poses of pharmacovigilance. The necessary size of the

dataset required is difcult to determine but will depend

on the data quality, the background frequency of the

event and the strength of the association of the event
with the drug. However, for even moderately rare events,

large databases are required. The characteristics of the

different large databases throughout the world are

described elsewhere [21] and discussed below (Table 2).
Spontaneous reporting databases
The spontaneousreporting of a possible adverse drug reaction by health-

care providers to governmental agencies or drug com-
panies (i.e. to the UK Yellow Card Scheme, http://
medicines.mhra.gov.uk/) is an important procedure in

PMS. However, as spontaneous reporting is a passive

system, inconsistent reporting is a limitation with more
frequent reporting for unusual reactions, reactions for
new drugs and serious reactions [22]. Furthermore, the

accuracy of the data contained within the reports is

uncertain. However, spontaneous reporting databases do

contain large amounts of data, for example the Food and

Drug Administration (FDA) spontaneous reporting data-

base contains over 2 million reports over a period of

35 years. These databases can therefore be mined to

obtained details of ADEs. For example, a retrospective

data mining of the FDA spontaneous reporting database

was shown to identify ADEs many years prior to col-
lecting reports alone [23], as is discussed further below.
Prescription event monitoring databases
Prescriptionevent monitoring (PEM) is used to detect ADEs by
collecting high-quality data from family doctors, on a

select group of patients exposed to a specic (new) drug,

for a limited period of time [24]. Heeley 
et al. [25]discuss the role of database exploration in order to detect

ADE signals from a PEM database, which contains 1
million reports of events from 78 PEM studies. They

point out that a limitation of PEM database mining is
the lack of an adequate control group, as the database
contains details of clusters of patients exposed to certain

drugs. For example, tolterodine did not show evidence

of hallucinations as an ADE because the control group

contained patients prescribed other drugs known to
Table 1The steps with rationale and examples, of the knowledge discovery in database (KDD) processStep of KDD processRationaleExample
Denition of the project goalsTo focus the project and determine the data set required To determine any ADE with a COX-2 inhibitors
Data set acquisitionSelect the data set based on goalsThese will include patient demographics, past history, details of all pre
scribing and nonprescribed drugs, all diagnosis and symptoms (including 
gastrointestinal and cardiovascular). Many more variables will be required 
to exclude bias in the analysis and to search for other ADE.Selection of data-base or databasesTo ensure the database contains at least the minimum required variablesThe combination of a linked administrative database and electronic medical recordsData cleaning and preprocessingTo validate seemingly erroneous data entries and calculate additional eldsHysterectomy in males or to calculate socio-economic status from postal codeData miningProcess the dataLink analysis or measure of disproportionality
Data interpretation and utilizationTo identify signal, perform further mining 
if required and to report the nding so 

that further studies can be performed 
Determine a high association with cardiovascular disease and COX-2, report 
this association via Dr Doctor letter and instigate a controlled observational 
study to prove the associationA. M. Wilson et al.
13057:2Br J Clin Pharmacolcause hallucinations. When the data from these patients
were removed, an ADE signal for tolterodine was dis-

covered [25].
Linked administrative databases
Large linked health
administrative databases, such as Medicaid in the USA

and the Ontario provincial databases, contain data on

millions of subjects and may also be used as a source
for data mining. The data are available at relatively small

additional costs and are not subject to recall or
interviewer bias. However, the completeness of details,

such as diagnoses, are questionable in many circum-

stances, and they tend to apply to elderly or low-income

populations only, so may not be representative of the

whole population.The Saskatchewan-linked administrative healthcare
utilization database and the Tayside Medicines Monitor-

ing (MEMO) are examples of linked medical health

databases, and both have been utilized to identify risks

of benzodiazepine therapy [26, 27].
Electronic medical records
Electronic medical records(EMRs) contain a large number of data elds, including

details such as the use of tobacco products, smoking and
nonprescription drugs, symptoms and signs, laboratory
data and social circumstances, on a smaller number of
patients and may also be used for data mining. Becauseof the large number and detail of the variables, which
can be combined to generate new diagnoses or adverse

events, hypotheses, which are not restricted to existing
diagnoses, can be explored. Although Honigman 
et al.[28] have investigated the use of EMR in detecting

known ADEs, there have been no studies of data mining

using EMR.Other databasesClinical trials databases and specialistdatabases such as overdose or toxicology databases may

also contain valuable information. Data mining has been

used to explore cardiovascular clinical trial databases

[29], the US Vaccine Adverse Event Reporting System

[30] and a large prescription database [31]. Signals for

liver-related ADEs have been reported from analysis of

a biochemistry laboratory database at a higher rate than
that reported by physicians [32]. Poison information

centres also record details of ADEs and may contribute

to the pharmacovigilance process [33].
Data preprocessingData preprocessing involves data sampling and data
quality verication to ensure the data are clean and well

described. Medical data can contain erroneous data such
as ages of 120 years rather than 20 years, and a docu-

mentation of hysterectomy in males. New variables of

interest to be used in the analysis can be generated fromTable 2Examples of databases used in knowledge discovery in databases for the purposes of pharmacovigilance
Type of databaseExampleAdvantagesDisadvantages
Spontaneous reporting 
databaseWHO Uppsala MonitoringCentreVery large Relatively cheapRequires association to be recognized and report to be 
submittedNot complete (under reporting)
Assess the whole populationNot able to assess incidence rates
No control group

Reporting bias
Prescription event monitoring databaseDrug Safety Research Unit,Southampton, UK 
Large Only a few select drugs
Good quality dataControl group may be inadequate

Identication of association not requiredIncidence rates can be calculatedLarge linked administrativedatabaseMedicaid, USA Very large Not representative of population
Canadian provincial linkedadministrative utilization
databaseRelatively cheapData set may not be accurate for all elds
Capture data of routine clinical careElectronic medical recordsCOMPETE, Hamilton, More extensive data set Small
CanadaExpensive

GPRDData mining in pharmacovigilance
Br J Clin Pharmacol57:2131
the data. For example, it may be possible to estimate
socio-economic status from the postal code.Data mining techniquesPredictive modelling
Predictive modelling is a tech-
nique used to develop a model to relate a dependent

variable with a set of independent variables in a manner
similar to multiple regression analysis. There are two

types of predictive modelling, namely classication, for

categorical dependent variables, and value prediction,

for continuous dependent variables. Classication is

appropriate if the goal is to predict group membership
of new records based on their characteristics (indepen-

dent variables). Using classication, the most inuential

variable is identied and used to split the data intogroups. This is then repeated with the next most inu-

ential variable until the data are fully characterized. For

example, it may be possible to determine a classica-tion criterion or rule that discriminates between differ-

ent groups of patients with and without side-effects

based on age, sex or socio-economic class. Value pre-

diction uses both classication and regression to predict

the future outcome of a patient based on, for example,

their demographic or socio-economic characteristics.
However, we need to use caution as, in any data analy-

sis of continuous outcomes, the results of value predic-

tion can be inuenced by the presence of outliers in the
data.Clustering or database segmentation
Clustering usesan algorithm that segregates a database by evaluating the

dissimilarity between records. Pairs of records are com-

pared by the values of the individual elds within them,

and clustering into groups provides fast and effective

ordering in large datasets. Segmentation could be used

to group patients with similar symptoms or diagnoses to
determine whether there is a drug association. Thus,

clustering is a technique of choice if the goal is to reduce
a large sample of records to a smaller set of specic

homogeneous subgroups (clusters) without losing much
information about the whole sample. Because of the
heterogeneity between clusters, this analysis can also be
helpful in hypothesis development about the nature of

the variation between subgroups. For example, if a data-

base contained details of different cardiac pathologies

(e.g. valvular heart disease) and medication (e.g. fenu-

ramine-phentermine), clustering analysis may have seg-

regated patients according to heart disease and identied

fenuramine-phentermine as one of the main factors in

this group. We could then explore the hypothesis of an

association or causal link between cardiac valvular dis-

ease and fenuramine-phentermine.Link analysisLink analysis refers to methods that iden-tify associations or links between records or sets of data
[34]. It assesses associations by using an if x then ytype rule, by assessing patterns of behaviour or by iden-

tifying similar time sequences of events. In pharma-

covigilance, link analysis could be used to identify

associated factors such as the effect of renal impairment

on the safety prole of diuretics.Deviation detection
Deviation detection looks for out-
liers or values that deviate from the norm and can be

seen either graphically or statistically. Visualization

techniques are used to determine patterns hidden in data,
e.g. scatter plots or histograms, multidimensional
graphs for multivariate data, and time series plots. Sta-

tistics methods are then employed to measure signi-

cance of deviations once they have been detected. This

process could be used to identify patients with idiosyn-
cratic reactions or unusual symptoms, which could be
related to medication and may constitute an ADE signal.

Regression analysis and stratication can be used to

assess the inuence of age, sex and comorbidity on ADE

signal generation.Measures of disproportionality (Figure 1)
To 
ourknowledge, measures of disproportionality are the only

techniques which have been used to identify ADEs.

They have been used by the Netherlands Pharmacovig-

ilance Foundation [35], the UK Yellow Card database

[36], WHO Uppsala Monitoring Centre [37] and PEM

database [25]. Using the denition of a signal as a pro-
portional reporting ratio (see Figure 1) 
>2, c2 >4 andthree or more cases, Evans 
et al. [36] identied 487signals from the Yellow Card database. Of those, 70%

were recognized ADEs, 13% were due to the underlying

disease and 17% were investigated further. Of the latter,

28% were reviewed in detail and in three cases the

manufacturer was requested to change the product

information. Using the Netherlands Pharmacovigilance

Foundation it was possible to identify an association
between delay of withdrawal bleeding during concomi-

tant use of oral contraceptives and itraconazole, which

had previously only been suggested by case reports

[38]. The same authors also assessed the statistical

interaction between the use of diuretics and nonsteroi-

dal anti-inammatory drugs (NSAID), and showed

signicantly higher use in combination, suggesting an
effect of NSAID on diuretic use. In a retrospective study

Lindquist et al. [39] dened an ADE subsequently
reported in the medical literature as the gold standard,
and reported that the Bayesian Condence Propagation

Neural Network (BCPNN) method at the WHO Uppsala
A. M. Wilson et al.
13257:2Br J Clin PharmacolMonitoring Centre had a 46% positive predictive value
and a 84% negative predictive value. More recently,

Bate et al. [40] have demonstrated the use of the
BCPNN Data Mining approach to detect signals of spe-
cic adverse drug reactions and also adverse events as

a drug class effect. They demonstrated the association

of pericarditis with practolol but not with other 
b-block-ers, the association of captopril and other angiotensin-
converting enzymes with cough, and the association

with terfenadine and heart rate and rhythm disorders.

Puijenbroek et al. [41] discuss the advantages and dis-
advantages of the different methods of disproportional-

ity and show that although they were comparable, the

Information Component (see Figure 1) was the most

versatile but required knowledge of Bayesian statistics.
The US Food and Drug Administration (FDA) uses a
data mining algorithm called the Multi-item Gamma
Poisson Shrinker (MGPS) to interrogate their spontane-

ous reporting database, as this has the ability to look at
drugdrug interactions. The MGPS algorithm examines

the ratio of an observed ADE to the total number of

ADEs (over 56 million) in order to detect a signal using

Bayesian statistical analysis. In a retrospective study the

MPGS was shown to identify 20 out of 30 known ADEs

15 years prior to detection by standard methods [23].

The same authors also show that the number signal

scores (the adjusted ratio of observed to expected

counts) increased for rhabdomyolysis with cerivastatin

from 1998 until 2001, when this drug was voluntarily

withdrawn from the market [23].
Data interpretation and utilization
It is likely that the
KDD process will have to be repeated many times with

redenition of the goals and further analysis in light of
the results of previous mining attempts. However, once

a signal is identied then the degree of causation needs

to be determined. Clearly the strength of the signal is
important, but other criteria, suggested by Bradford-

Hill, including the consistency and specicity of the

nding, temporality, and plausibility, are required [42].

The Medicines Control Agency applies SNIP criteria

(the Strength of the signal, whether it is New, clinically

Important or whether there is potential for Preventative

measures), each criterion being independently associ-
ated with a signal generation [43]. Often causation is not
conrmed and other methods such as casecontrol stud-
ies are required to examine the ADE [44, 45]. Once a

potential ADE is identied and further investigation

substantiates the adverse effect, this information needs

to be quickly disseminated via reports to medical prac-
titioners, e.g. the Dear Doctor letters, or articles in the
medical literature [46]. In this way the medical commu-

nity can be educated and steps can be put into place to
reduce ADEs, making drug therapy less hazardous for

patients.What KDD can and cannot doIt is currently not possible to link all available datasets
together and run one of the data mining techniques to
identify all possible adverse events. Data mining pro-

cesses are not able to account for inaccurate or missingFigure 1Denitions of measures of disproportionality
 Suspected event All other events
 Suspected drug A 
B All other drugs C 
D Measure Definition Utilization Reporting Odds Ratio 
(ROR) A/C B/D  Netherlands 
Pharmacoviligance 
Foundation Yules Q ratio 
AD-BC 
AD+BC 

  Proportional reporting ratio  
(PRR) A/(A+C) C/(C+D) 
 UK Yellow Card Information component 
(IC)  log2  p(x,y) 
       p(x)p(y) 

 WHO Uppsala Monitoring 
Centre Where  p(x) = probability that drug x listed on case sheet  
p(y) = probability that ADE y listed on case sheet   
p(y,x) = probability that drug-ADE combination x  
and y listed on case sheet 
Data mining in pharmacovigilance
Br J Clin Pharmacol57:2133
data, and if a signal is not detected it is impossible todetermine whether no ADE exists or the data are insuf-

cient. Furthermore, KDD only generates a signal and,
in the context of pharmacovigilance, further studies or

investigations will be required to conrm a potential

ADE.However, by being open minded, it is possible to
search for many different ADEs at once. Also, informa-

tion in many databases is under-utilized, and therefore

KDD may be possible to generate new information from

existing data sources at minimal extra cost. KDD will
not replace traditional methods of pharmacovigilance,

but if used in conjunction may reduce the time requiredfor ADE identication [23]. It is impossible to predict

the actual circumstances in which KDD will contribute

to pharmacovigilance, but one would suspect that rare

and atypical events, clusters of symptoms and signs not

yet formulated in a diagnosis and common diagnoses
not recognized as drug-related could all be agged ear-

lier and more reliably by employing systematic data

mining techniques. Regular and repetitive analyses of

accumulating data could maximize the opportunity to
detect a signal of an ADE which is only evident after

prolonged usage. In the future it may be possible to
perform continuous data mining analysis, e.g. link anal-
ysis, on large electronic medical records to identify sig-

nals in a cost-effective manner.
ConclusionWe have described the steps in KDD, the different sta-
tistical processes available during the data mining

procedure, and discussed examples of where these

techniques have been used in pharmacovigilance. When

considering the frequency and signicance of ADEs, as

well as the inadequacy of premarketing ADE evaluation,

it is clear that cost-effective methods of quickly identi-

fying potential ADEs are required. Given the availability

of larger datasets and faster computer processing

speeds, we suggest that data mining techniques will
have a greater role in pharmacovigilance in the future.
A.H. is the recipient of a Career Investigator Award from
the Canadian Institutes for Health Research.
References1Pirmohamed M, Breckenridge AM, Kitteringham NR, Park BK. 
Adverse drug reactions. Br Med J 1998; 316: 12958.
2Patel P, Zed PJ. Drug-related visits to the emergency department: 
how big is the problem? Pharmacotherapy 2002; 22: 91523.
3Juntti-Patinen L, Neuvonen PJ. Drug-related deaths in a University 
central hospital. Eur J Clin Pharmacol 2002; 58: 47982.
4Bates DW, Cullen DJ, Laird N et al. Incidence of adverse drug 
events and potential adverse drug events. Implications for 
prevention. ADE Prevention Study Group. JAMA 1995; 274: 29

34.
5Leape LL, Cullen DJ, Clapp MD et al. Pharmacist participation on 
physician rounds and adverse drug events in the intensive care 
unit. JAMA 1999; 282: 26770.
6Bates DW, Leape LL, Cullen DJ et al. Effect of computerized 
physician order entry and a team intervention on prevention of 
serious medication errors. JAMA 1998; 280: 131116.
7Honig PK, Woosley RL, Zamani K, Conner DP, Cantilena LR Jr. 
Changes in the pharmacokinetics and electrocardiographic 

pharmacodynamics of terfenadine with concomitant 

administration of erythromycin. Clin Pharmacol Ther 1992; 52: 

231
8.8Ioannidis JP, Lau J. Completeness of safety reporting in 
randomized trials: an evaluation of 7 medical areas. JAMA 2001; 

285: 43
743.9Mukherjee D, Nissen SE, Topol EJ. Risk of cardiovascular events 
associated with selective COX-2 inhibitors. JAMA 2001; 286: 

9549.
10Bombardier C, Laine L, Reicin A et al. Comparison of upper 
gastrointestinal toxicity of rofecoxib and naproxen in patients with 
rheumatoid arthritis. VIGOR Study Group. N Engl J Med 2000; 

343: 15208.
11Edwards IR. The accelerating need for pharmacovigilance.J Roy 
Coll Physicians Lond 2000; 34: 4851.
12Friedman MA, Woodcock J, Lumpkin MM, Shuren JE, Hass AE, 
Thompson LJ. The safety of newly approved medicines: do recent 
market removals mean there is a problem? JAMA 1999; 281: 

172834.
13Chakrabarti S. Mining the web: Discovering knowledge from 
hypertext data.  Morgan Kaufmann Publishers, San Francisco, CA, 

2002.14Chowdhury GG. Template mining for information extraction from 
digital documents. Library Trends 1999; 48: 181207.
15Kwansnik BH. The role of classication in knowledge 
representation and discovery. Library Trends 1999; 48: 2247.
16Amery WK. Signal generation from spontaneous adverse event 
reports. Pharmacoepidemiol Drug Saf 1999; 8: 14750.
17Helma C, Gottmann E, Kramer S. Knowledge discovery and data 
mining in toxicology. Stat Meth Med Res 2000; 9: 32958.
18Coulter DM, Bate A, Meyboom RH, Lindquist M, Edwards IR. 
Antipsychotic drugs and heart muscle disorder in international 

pharmacovigilance: data mining study. extra. Details of methods: 

explanation of data mining methods. Br Med J 2001; 322: 1207

9.19Brown EG. Effects of coding dictionary on signal generation: a 
consideration of use of MedDRA compared with WHO-ART. Drug 

Saf 2002; 25: 44552.
20Brown EG, Wood L, Wood S. The medical dictionary for regulatory 
activities (MedDRA). Drug Saf 1999; 20: 10917.
21Strom BL. How should one perform pharmacoepidemiology 
studies? Choosing among the available alternatives. In 
A. M. Wilson et al.
13457:2Br J Clin PharmacolPharmacoepidemiology, 3rd edn, ed Strom BL. Chichester: John 
Wiley & Sons Ltd, 2000; 40113.
22Biriell C, Edwards R. Reasons for reporting adverse drug reactions 
 some thoughts based on an international review. 

Pharmacoepidemiol Drug Saf 1997; 6: 216.
23Szarfman A, Machado SG, ONeill RT. Use of screening algorithms 
and computer systems to efciently signal higher-than-expected 

combinations of drugs and events in the US FDAs spontaneous 

reports database. Drug Saf 2002; 25: 38192.
24Mann RD. Prescription-event monitoringrecent progress and 
future horizons. Br J Clin Pharmacol 1998; 46: 195201.
25Heeley E, Wilton LV, Shakir SA. Automated signal generation in 
prescription-event monitoring. Drug Saf 2002; 25: 42332.
26Rawson NS, Rawson MJ. Acute adverse event signalling scheme 
using the Saskatchewan Administrative health care utilization 
datales: results for two benzodiazepines. Can J Clin Pharmacol 

1999; 6: 15966.
27Barbone F, McMahon AD, Davey PG et al. Association of road-
trafc accidents with benzodiazepine use. Lancet 1998; 352: 

1331
6.28Honigman B, Lee J, Rothschild J et al. Using computerized data 
to identify adverse drug events in outpatients. J Am Med Inform 

Assoc 2001; 8: 25466.
29Cerrito P. Application of data mining for examining polypharmacy 
and adverse effects in cardiology patients. Cardiovasc Toxicol 

2001; 1:
 1779.30Niu MT, Erwin DE, Braun MM. Data mining in the US Vaccine 
Adverse Event Reporting System (VAERS): early detection of 

intussusception and other events after rotavirus vaccination. 

Vaccine 2001; 19: 462734.
31Bytzer P, Hallas J. Drug-induced symptoms of functional dyspepsia 
and nausea. A symmetry analysis of one million prescriptions. 

Aliment Pharmacol Ther 2000; 14: 
147984.
32Bagheri H, Michel F, Lapeyre-Mestre M et al. Detection and 
incidence of drug-induced liver injuries in hospital: a prospective 
analysis from laboratory signals. Br J Clin Pharmacol 2000; 50: 

47984.
33Mey C, Hentschel H, Hippius M, Balogh A. Documentation and 
evaluation of adverse drug reactions (ADR)  contribution from a 
poison information center. Int J Clin Pharmacol Ther 2002; 40: 

1027.
34Perner P, Petrou M. Machine learning and data mining in pattern 
recognition. Berlin: Springer Verlag, 1999.
35Egberts AC, Meyboom RH, van Puijenbroek EP. Use of measures 
of disproportionality in pharmacovigilance: three Dutch examples. 

Drug Saf 2002; 25: 4538.
36Evans SJ, Waller PC, Davis S. Use of proportional reporting ratios 
(PRRs) for signal generation from spontaneous adverse drug 

reaction reports. Pharmacoepidemiol Drug Saf 2001; 10: 

4836.37Bate A, Lindquist M, Edwards IR, Orre R. A data mining 
approach for signal detection and analysis. Drug Saf 2002; 25: 

393
7.38van Puijenbroek EP, Egberts AC, Meyboom RH, Leufkens HG. 
Signalling possible drugdrug interactions in a spontaneous 
reporting system: delay of withdrawal bleeding during 

concomitant use of oral contraceptives and itraconazole. Br J Clin 

Pharmacol 1999; 47: 68993.
39Lindquist M, Stahl M, Bate A, Edwards IR, Meyboom RH. A 
retrospective evaluation of a data mining approach to aid nding 

new adverse drug reaction signals in the WHO international 
database. Drug Saf 2000; 23: 53342.
40Bate A, Lindquist M, Orre R, Edwards IR, Meyboom RH. Data-
mining analyses of pharmacovigilance signals in relation to 

relevant comparison drugs. Eur J Clin Pharmacol 2002; 58: 483

90.41van Puijenbroek EP, Bate A, Leufkens HG, Lindquist M, Orre R, 
Egberts AC. A comparison of measures of disproportionality for 

signal detection in spontaneous reporting systems for adverse 

drug reactions. Pharmacoepidemiol Drug Saf 2002; 11: 310.
42Shakir SA, Layton D. Causal association in pharmacovigilance and 
pharmacoepidemiology: thoughts on the application of the Austin 

Bradford-Hill criteria. Drug Saf 2002; 25: 46771.
43van Puijenbroek EP, van Grootheest K, Diemont WL, Leufkens HG, 
Egberts AC. Determinants of signal selection in a spontaneous 

reporting system for adverse drug reactions. Br J Clin Pharmacol 

2001; 52
: 57986.
44Dunn N, Freemantle S, Mann R. Nicorandil and diabetes: a nested 
casecontrol study to examine a signal generated by prescription-
event monitoring. Eur J Clin Pharmacol 1999; 55: 15962.
45Kaufman DW, Rosenberg L, Mitchell AA. Signal generation and 
clarication: use of casecontrol data. Pharmacoepidemiol Drug 

Saf 2001; 10: 197203.
46Meyboom RH, Lindquist M, Egberts AC, Edwards IR. Signal 
selection and follow-up in pharmacovigilance. Drug Saf 2002; 25: 

45965.
European Journal of Adapted Physical Act
ivity, 2
(1), 
4661  European Federation of Adapted Physical Activity, 2009
  EUJAPA, Vol. 
2, No. 1
 46 A CONTEMPORARY REVIE
W OF ENGLISH LANGUAG
E LITERATURE ON 
INCLUSION OF STUDENT
S WITH DISABILITIES 
IN PHYSICAL EDUCATIO
N: A 
EUROPEAN PERSPECTIVE
  Deirdre OBrien
 EMMAPA Student; Faculty of Physical Culture at Palacky University, Olomouc, Czech Republic
 Martin
 
 Faculty of Physical Culture at Palacky University, Olomouc, Czech Republic
 Peter 
David Howe
 Loughborough University
, Peter Harrison Centre for Disability Sport, UK
   The 
state of inclusion for students with disabilities in general physical educat
ion (GPE) varies 
across European countries. In many countries inclusive physical education is still a developing 
practice. The purpose of this review of literature published in English is to highlight current debates 

surrounding the inclusion of students w
ith disabilities in physical education, in hope that there 
might be a degree of consensus of opinion surrounding the facilitation of inclusive practices within 

Europe.  This review covers an eight year period commencing in early 2000.   A total of twenty 

seven articles are highlighted in this review using the Theoretical Model for the Study of Classroom 
Teaching (Dunkin 
 Biddle, 1974) which suggests that study of teaching and learning involve four 
variables: presage (teacher), context (students), process (
interaction) and product. The ultimate aim 
of this review is to use recent publications in the field of APA to highlight a need for the 

establishment of professional guidelines for   successful implementation of good practice within 

GPE throughout Europe
.  KEYWORDS:
 physical education, integration, inclusion, mainstreaming, and disabilities
. INTRODUCTION
 Authors in the field of adapted physical 
education articulate different definitions of 

inclusion.  Lieberman and Houston
-Wilson 
(2002) described inclusio
n as providing 

services to students with disabilities in the 

typical classroom environment rather than 

removing them from it in order to receive 

special services. To others inclusion is 

viewed as a cohesive sense of community, 

acceptance of differences a
nd responsiveness in 

individual needs 
(Stainback & Stainback, 

1996).  Miller (1994)
 viewed inclusion as the 
point in the continuum of services which places 

the student with a disability in regular 

education classrooms, with appropriate support 

personnel, t
o receive an education and related 
services alongside peers. Overall the term 

inclusion is associated with providing services 

to ensure that all students regardless of their 
ability can achieve their full potential in an 

appropriate educational setting.
 It is generally believed that all students 
with disabilities should experience 

participating in regular physical education 

lessons with their friends as part of their 

growth and development 
(BAALPE, 1996)
.  Research has been conducted in the area of 

includin
g students with disabilities in 

mainstream schools, but research directly 

associated with physical education is still in 

short supply. Research conducted recently 

however suggests that inclusion in physical 

education can effectively work for the child 

with
 a disability (Goodwin & Watkinson, 
2000) and this can be achieved without 

negatively affecting peers without disabilities
 (Faison
-Hodge & Porretta, 2004;
 Obrusnkov, 
et al., 2003). It is essential for educators to 

determine how much support a child needs
 to 
receive appropriate physical education within 
OBrien
 et al.
 Review of Studies in Inclusive Physical Education
   EUJAPA, Vol. 
2, No. 1
 47 the general physical education (GPE) setting 
(Block & Krebs, 1992)
 since  within physical 
education there is believed to be a continuum 

of placement options with different levels of 

inclusion (Block, 2007).
   Inclusion of 
students with disabilities in GPE has been the 

focus of growing number of studies in the last 

20 years.  Block and Vogler (1994) reviewed 

literature with regard to inclusive school 

settings. Their initial findings were favourable 

towards in
clusion.  More recently Block and 

Obrusnkov (2007) reviewed a ten year period 

from 1995 to 2005 and 
they concurred with the 
earlier findings of Block and Vogler (1994) 

also found numerous positive outcomes of 

inclusion in GPE
.  
METHOD
 This review examine
s the literature 
regarding inclusion of students with disabilities 

in physical education from the year 2000 to 

2008.  Various databases were accessed, these 

including SPORTDiscus, Health Medline, 

ProQuest and PsycINFO. A grand total of 114 

articles were so
urced from a broad selection of 
journals.  Conscious of the inadequacy of many 

of the articles relevance to the literature 

review, the researcher re
-examined the articles 
and applied a specifically designed five criteria 

to increase the focus of the study
: (a) must be 
published in English, as this is the first 

langu
age of two of the three authors,
 (b) must 
be directly related to physical education or 

physical act
ivity with a focus on inclusion,
 (c) 
must be an original study;  (d) must be 
published between 
2000 to
 2008, and (e) must 
be published in journals, thus excluding books, 
unpublished papers, doctoral dissertations and 

master theses. Having sourced 114 articles 

originally, 27 proved suitable after 

implementing the selection criteria. The review 

that f
ollows is based upon the Theoretical 
Model for the Study of Classroom Teaching 
(Dunkin 
 Biddle, 1974) adopted within 


Dunkin 
 Biddle suggested that study of 
teaching and learning involve four key 
variables: pr
esage (teacher), context (students), 
process (interaction) and product. Presage 

variables include those influencing teaching 
behaviour of PE teachers. Context variables 

include background of students, their skills, 

and attitudes toward physical activities 
or 

previous experiences. Process variables include 

student
-student interaction, teacher
-student 
interaction, teacher behaviours and student 

behaviours. Within the sample of articles the  

prevalence of focus upon the variables 

articulated by Dunkin and Bidd
le (1974) was a 

follows (a) Presage var
iables 
- teachers (13 
articles),
 (b) Context va
riables 
- students (2 
articles),
 (c) Process variables 
- interacti
on in 
inclusive PE (9 articles),
 and (d) Product 
variables 
- effectiveness of inclusive PE (3).  
Some of
 the articles related to more than one 
key area as various authors investigated more 

than one variable within a study with particular 

overlap of issues related to process and product 

variables
.  
RESULTS
 Presage variables 
 Teachers in Inclusive 
Physical Ed
ucation
 In accordance with Theoretical Model for 
the Study of Classroom Teaching (Dunkin 
 Biddle, 1974) articles focusing on teacher 

formative experiences, teacher training 

experiences, and teacher properties were 

included in the presage variables results
. Preservice Training of Teachers
 In accordance with Sherrill (2004) it is 
important to prepare future physical education 

teachers engage with inclusive practices for 

students with disabilities in GPE settings. In 

order to prepare these teachers
-in-trainin
g we 
must be able to measure and to understand their 

attitudes towards inclusion. Hodge et al
. (2003) 
selected a purposeful sample of ten teachers
-in-training all majoring in physical education and 

enrolled in an adapted physical education 

(APE) course to 
explore the meaning of 
practicum experiences.  Over an eight week 

period as part of the APE course students 

participated in a Unified Physical Activity 

Program (UPAP).  Data were collected via self
-reflective journals.  Results from the study 

revealed that
 the experience of planning and 
incorporating inclusive practices impacted 

favourably on the teachers
-in-training overall 
OBrien
 et al.
 Review of Studies in Inclusive Physical Education
   EUJAPA, Vol. 
2, No. 1
 48 confidence and attitudes towards teaching 
children with disabilities. At the end of the 

study teachers
-in-training showed a positive 
attitude to an integrated physical activity 
environment.  Establishing a routine with the 

children, having a variety of activities and 

carefully planning each lesson with a readiness 

for flexibility were aspects evident of the 

students teaching that worked
 well. This 
allowed for meaningful interactions and 

experiences.  The UPAP program was a great 

initial starting point for these students to gain a 

hands-on experience. Hodge et al.
 (2003) 
further highlighted that many of the 

experiences and skills gained t
hrough this 
practicum can be easily transferred into a 

physical education class environment.  
 In another study Hardin and Brent (2005) 
explored how the Physical Education Teacher 

Education training curriculum has affected the 

competence and confidence of 
practising 

physical education teachers.  Five newly 

qualified teachers who taught students with 

disabilities were selected to be interviewed 

enquiring about their education, training and 

inclusion in physical education.  The 

assortment of Q
-sort cards (fil
e cards that 
contain 
personality statements)
 that teachers 
had to organise and rank in accordance with 

their self
-understanding    Results from the Q
-sort cards revealed that teachers felt that the 

experience of teaching was the best resource 

for learning 
how to teach students with 

disabilities.  Other teachers and course work 

were also seen as vital tools.   The majority of 

the teachers experience of course work was 

only based upon one class taken on adapted 

physical activity with three of the five teache
rs 
not experiencing teaching students with 

disabilities during this time. Hardin and Brent 

(2005) commented that one class of training for 

teachers in adapted physical education is not 

sufficient.  This assertion is in line wi
th the 

thoughts of Kozub et al
. (1999) who have 
suggested that teacher training has a large 

impact on how future physical education 

teachers develop their knowledge of disabilities 

and their understanding of differences in 

students.  The understanding of students with 

disabilities and 
their associated differences is a 
stepping stone to overall acceptance and 

inclusion within physical education settings
.  Attitudes of PE teachers towards inclusion
 A vital part of inclusion is the teachers 
attitudes. Attitudinal instruments are vital too
ls 
for researchers, as they provide them with a 

means of collecting their data and analyzing it.  

Various instruments have been used throughout 

the studies in this review. The Physical 

Educators Judgements about Inclusion (PEJI) 

instrument was developed b
y Hod
ge et al.
 (2002).  This instrument is designed to 

establish the attitudes of physical education 

teacher education (PETE) pre
-service teachers 
towards inclusion of students with disabilities 

in GPE.  The instrument constitu
tes three areas 

of exploratio
n (a
) Social Judgement, (b) 
Contact, and (c
) Planned Behaviour, which 
together can be analysed to establish teaching 

competence as it relates to inclusive aims.  

Eighteen PETE pre
-service and experienced 
teachers were selected for the focus group, 

which co
mprised ten females and eight males.  
All participants had taken at least one course 

on teaching students with disabilities in PE.  To 

generate validity of the new instrument, it was 

distributed to two hundred and seventy two 

PETE pre
-service teachers.  Th
ree main 
subscales were reveale
d from the study, these 

being: (a
) judgements abo
ut inclusion versus 
exclusion, (b
) judgements about acceptance of 

students with disabilities, and (c
) judgements 
about perceived training needs.  
 
 (2002) develo
ped 
instrument based on the Theory of Planned 
Beha
viour developed by Ajzen (1991)
. The 
Attitude Towards Inclusion of Students with 
Physical Disabilities in Physical Education 

(ATIPDPE), which was later revised by 


-R having 
greate
r number of items in attitudinal subscale 
following suggestions from previous studies 

using ATIPDPE.  This instrument measures 

three psychological components of Inclu
sive 

Physical Education (IPE): (a) attitudes toward 

IPE, (b) subjective norm toward IPE, a
nd (c
) perceived behavioural control in relation to 

IPE. 
 Teachers perceptions of students abilities 
(Karper & Martinek, 1985) have an impact on 
OBrien
 et al.
 Review of Studies in Inclusive Physical Education
   EUJAPA, Vol. 
2, No. 1
 49 both student learning opportunities and 
participation. Smith (2004) purposively 

selected seven experienced te
achers to 

interview.  He examined the inclusion of 

students with Special Educational Needs (SEN) 

in secondary school physical education.  The 

use of open questions provided a greater range 

of questions and the opportunity to probe 

certain topics which aros
e in the interviews.  
Results illustrated the demands teachers face 

with inclusion.  Teachers believed it was 

unsuitable to teach a full class of twenty eight 

students and also include SEN students within 

this context.  Equal opportunities for all was 

the general philosophy of all teachers, yet 
many students with SEN were not given the 

same opportunities as their peers. While team 

games were a strong and traditional part of the 

curriculum, all teachers collectively stated this 

area as the most difficult w
ithin which to 
actively engage in inclusive practices.  

Teachers highlighted individual activities as 

much more appropriate for instigation inclusive 

practices but this by there very nature make it 

difficult to institute a philosophy of inclusion.   

Smith 
(2004) outlined that often students with 
SEN are expected to fit into the curriculum, 

rather than adaptations made to ensure the 

inclusion of these students.  He further suggests 

that for inclusion to take place, the curriculum 

needs to be more flexible 
and to move away 

from the strong focus on team sports.  This 

study was one of the few to be based solely 

upon qualitative data and as such interpretation 

of result maybe opens to continued re
-interpretat
ion. In another study Morley et al.
 (2005) explored t
he perceptions of forty three 
secondary school teachers views towards 

inclusion in mainstream physical education.  

Understanding, awareness, extra planning and 

organisation were highlighted as variables the 

teachers were aware that needed to be adapted.  

Teachers perceptions of their own inadequacy 

and lack of confidence were mentioned, you 

want to help them; you want them to do their 

best, you want to include them but its knowing 

how to adapt it (teaching and instructions) to 

suit them (Morley et al.,
 2005). This suggests 
that Indoor and individual activities were seen 

as easier to include students in, while outdoor 
and team activities were viewed as more 

problematic.  The severity and type of 

disability was cause for concern as behavioural 

and emotion
al impairments were deemed the 

most challenging for successful inclusion. 

Teachers also remarked on the effect on the 

students without disabilities, I do think about 

other members of the group wondering if that 

person is holding them back (Morley et al.,
 
2005).  Morley et. al. also highlight that the 

majority of teachers commented on the 

inadequate and lack of resources and support, 

and teacher training.  Regarding training they 

commented, None at all, we are not trained or 

It wasnt catered for in my 
teacher training
 (Morley et al., 2005)
.   Teachers Concerns and Perception about 
Barriers in Inclusive PE
 For teachers to feel somewhat 
apprehensive about taking students with 

disabilities into their physical education is 

understandable.
  They would natu
rally wonder 
as to how they are going to include and 

motivate all students with and wi
thout 

disabilities.
  Lienert et
 al. (2001) interviewed 
thirty physical educators from Germany and 

United States to discover the concerns teachers 

have in regard to inclus
ion of students with and 
without disabilities in physical education.  

Purposeful sampling was used to select sixteen 

teachers from Berlin and fourteen from Texas. 

This study was directed by the Concerns
-Based 
Adoption Model (CBAM).  An interview guide 

comp
rised semi
-structured questions and a 
demographic questionnaire was used. Concerns 
were reported for four of the seven s
tages of 
the CBAM, which were: (a) personal, (b
) management
, (c) consequence, and (d
) collaboration, with management being of 

paramount 
importance.  Culturally the teacher 
in the United States
 had more personal 
concerns and worries about everyday demands 

and competency to meet those demands the 

needs of pupils with disabilities.  Some 

teachers 
in the United States 
did not try to 
teach the 
students with disabilities, instead they 

handed over responsibility to the para
-professionals. The German teachers felt it was 

very important to have the choice whether to 

teach integrated classes or not. This research 
OBrien
 et al.
 Review of Studies in Inclusive Physical Education
   EUJAPA, Vol. 
2, No. 1
 50 suggests that all teachers were unsat
isfied with 
facilities, equipment and the high ratio of 
students with disabilities in their classes.  

Collaboration in a supportive environment was 

collectively viewed as highly important by 

teachers in both countries.  Encouragingly 

overall in both countr
ies the teachers stated that 
the positive effects of inclusion were far greater 

than the negative effects.
 Similarly Hodge et al. (2004) examined the 
beliefs and behaviours of nine secondary 

school physical education teachers in relation 

to including stude
nts with disabilities.  This 
study again uses purposeful sampling to select 

the seven male and two female experienced 

teachers.  Results from the interviews and 

questionnaires outlined that teachers expressed 

both positive and negative beliefs about 

inclus
ion.  On the positive side the teachers 
who contributed to the study felt that inclusion 

was good conceptually but often there were 

many barriers to overcome to achieve 

successful inclusion. The inability to give the 

time and individual attention to the 
students 

especially those with severe disabilities due to 

lack of support was a recurring theme.  The 

majority of teachers commented that the 

availability of support impacted upon the 

efficacy of their teaching.  Many teachers did 

not know how to adequatel
y adapt their 
teaching environments to be safe and inclusive.  

The pre
-service training of these teachers 
indicated that only five out of the nine had an 

adapted physical education module included in 

their undergraduate studies. However nine 
participants i
n a study is a relatively small 
number to ensure that a genuine and varied 

perspective of PE teachers was obtained.
 Meegan and MacPhail (2006) questioned 
general expectations of physical education 

teachers to be fully committed to teaching 

students with di
sabilities, suggesting 
thatinstitutions of higher education are failing 

to prepare teachers
-in-training with the skills 
required for the inclusion of all students. In 

another study that examined the barriers of 

GPE teachers when including students with 

vis
ual impairments, Lieberman et al. (2002) 
found that the most common barrier identified 

was a deficit in professional preparation.  An 
in-service workshop on physical education for 
students with visual impairments was attended 

by one hundred and forty eight
 teachers, who 
had visually impaired students in their physical 

education classes.  A questionnaire distributed 

prior to the workshop was used to determine 

the perceived barriers facing teachers.  Results 

disclosed that 66% of teachers found lack of 

profes
sional preparation as the main barrier.  
Other barriers identified were lack of 

equipment (63%) and programming or 

curriculum (57%). Many teachers felt 

unprepared to incorporate students with visual 

disabilities into the class.  Lieberman and 

colleagues su
ggested that the curriculum of the 
teacher training programmes needs to be 

altered to meet the needs of physical education 

teachers.  Teacher training is a clear 

problematic area with regard to inclusion, but it 

is a problem that can be rectified.
 Fejgin e
t al. (2005) used a questionnaire to 
examine the relationship between inclusion and 

burnout in physical education.  The data were 

collected from three hundred and sixty three 

elementary school physical education teachers 

across six districts in Israel.  Bu
rnout was 
found to be related to the number of students 

with SEN in a class, the support available, and 

work place conditions.  A strong correlation 

was discovered between the structural and 

social dimensions of a school in relation to 

causing burnout.  Th
ese teachers also viewed 

inclusion as problematic because of the 

additional time requirements, the discipline of 

students, evaluation and classroom 
management depending on the severity of the 
students impairments. Behaviour, learning 

problems and lack of 
support was another area 

linked to burnout. Simil
arly to other studies 
Fejgin et
 al. (2005) highlighted that teachers do 
not have any pre
-service training on how to 
include students with SEN in physical 

education.  This is a recurring trend seen 

throughout
 many countries.
 The studies in this section used a 
combination of interviews and questionnaires 

to retrieve their data.  One study used an 

interview, two studies used questionnaires and 

one study used a combination of both.  The 

outcomes varied throughout
 these studies.  
OBrien
 et al.
 Review of Studies in Inclusive Physical Education
   EUJAPA, Vol. 
2, No. 1
 51 Overall, the main concerns and barriers 
identified were professional preparation, 

management and support
.   Support Personnel in Inclusive Physical 
Education
  Equal participation between all students 
in PE is strengthened by a combination 
of 

teacher and APE consultant, teacher and 

teacher assistant or teacher and peer tutor 

support systems (Murata & Jansma, 1997).  

Two main sources of support personnel, para
-educators and APE teachers were identified in 

the articles consulted for this revie
w, were 
considered vital to inclusive practices. Davis
 et al. (2007) distributed questionnaires to 

determine what actual were the responsibilities 

and training needs of paraeducators within 

physical education.  Seventy six paraeducators 

responded to the qu
estionnaire (99% female).  

Responses in this study revealed 61% believed 

that they were adequately trained for physical 

education, but surprisingly only 16% had 

received training in adapted physical education. 

The majority of the paraeducators interviewed 

had simply completed one
-day training course.  
38% participated in physical education with a 

student.  The level of participation varied 

greatly from the majority escorting the students 

and giving prompt cues to directly working 

with the students during PE
.  Assisting in 
assessment and sharing individual educational 

plan (IEP) suggestions was carried out by 28%.  

Paraeducators felt they needed to know the IEP 

goals of each child to help develop and 

reinforce those goals.  Encouragingly 90% 
were willing to b
e trained in physical 
education, but requesting incentives to do so. 
Authors highlighted five responsibilities that 

paraeducators can improve in physical 

education with students
. These responsibilities 
were: (a
) assis
tance with social interaction, (b
) tran
sfer from on
e activity to the next, (c) 
safety, (d
) interaction with students, and (e
) cooperative learning among students and 

reinforcing instructions for the teacher.  
 Probably the best and most effective form 
of support to a physical education teacher 
is 

that of the adapted physical education teacher.  

They have undergone specialised training and 

have a true understanding of the inclusion 
process.  A study by Lytle and Collier (2002) 

investigated APE specialists perceptions of 

consultation.  Six partic
ipants were involved in 
the study, four female and two male, with age 

ranging from thirty five to forty six years and 

case loads of students ranging from twenty four 

to one hundred and ten.  Data was collected 

through interview, field observations, 

researc
her notes and focus group interactions.  

Results indicated that the skills, attitudes and 

knowledge of the APE specialist combined 

with the educational environment were 

influential factors in the types of services 

provided.  The use of consultations and th
eir 

implementation were often influenced by the 

social, intellectual and physical environment.  

All participants commented that no formal 

training in consultation was provided as part of 

their training.  In another study Lytle and 

Hutchinson (2004) explore
d the experiences 
and roles of APE teachers primarily through 

the use of observations and interviews.  

Experienced teachers in APE were used in the 

study, four being female and two male.  When 

the data were analysed various roles of the 

APE teacher were pr
esented: (a) ad
vocator, (b) 
educator, (c) courier, (d) resource coordinator,
 and (e) supporter/helper. There were some 

negative reactions to the supporter/helper role, 

as the situation of territorial issues with the 

GPE teacher often arose.  Overall it was
 highlighted that the various roles in the 

consultation process is a huge part of the APE 

teachers daily life.  Specific training in 

consultation was not part of the participants 
APE training.  Lytle and Hutchinson suggested 
that more training in areas s
uch as adult 
interactions and effective communication in the 

consultation process is required.
 
 al. (2008) 
studied the nature of 
work and roles of public school adapted 

physical educators in selected school districts 

in the United States with th
e aim of adding to 
the information base to enable the 

improvement of service delivery and 

professional preparation. Participants of the 

study were 6 females and 2 males with 

experience teaching (range of 2
23 years) in 
the field of APE. Data collection inc
luded 
individual in depths interviews, demographic 
OBrien
 et al.
 Review of Studies in Inclusive Physical Education
   EUJAPA, Vol. 
2, No. 1
 52 data sheets and interview notes. Results 
showed the differences in the nature of work 

among APE specialists. Participants had high 

teaching loads (44
90 students) and served 
wide range of schools (1
20), w
hich created 
quite different teaching profiles. Most teachers 

were involved in APE consulting. Results also 

indicated the needs to incorporate issues of 

consulting into teacher preparation and change 

the university studies more relevant to real life 

teach
ing
.  Context Variables 
 Students in Inclusive 

Physical Education
 Dunkin and Biddle (1974) highlighted in 
their Theoretical Model for the Study of 

Classroom Teaching the importance of 

studying students in learning process. Under 

context variables they fo
cused on learner 
properties (students with and without 

disabilities), and school, community and 

classroom contexts
. Students without disabilities
 The perception and attitudes of students 
without disabilities can have a substantial 

impact on the success of 
inclusion in a GPE 

class.  From the previous studies inclusion of 

students with and without disabilities can be 

successfully implemented
. In relation to this 
Verderber et
 al. (2003) used the theory of 
planned behaviour to investigate the intentions 

of midd
le school students to engage with 

students with severe disabilities in GPE.  350 

grade sixth to eight students completed the 

Verderber Inventory of Students Intention to 

Participate in Inclusive Physical Education 
(VISIPIPE), with its validity proved at t
he 
commencement of the study.  Results indicated 
that students believed they should work and 

play with students with severe disabilities, but 

these beliefs were primarily motivated by 

abiding with the beliefs of parents and teachers.  

In comparison student
s did not believe that 
friends had the same beliefs.  Overall the study 

showed that teachers and parents can have an 

influence on middle grade students beliefs and 

attitudes towards others. 
 Other study, using the theory of planned 
behavior
 Kodish et al. 
(2006) examined the 
determinants of physical activity in an 
inclusive setting.  Kodish and colleagues used 

four classes containing one hundred and 

fourteen students aged ten to thirteen. 
Class 

one (C1) and class three (C3) were physical 

education classes t
hat each had four students 
with autism included in them (
n = 63), while 
class two (C2) and class four (C4) were GPE 

classes without students with identified 

disabilities included (
n = 51). 
None of the 
students with autism has received any direct 

support wi
thin physical education.  The 
physical education teacher used the Dynamic 

Physical Education Curriculum (DPE) which is 

positive towards inclusion. Questionnaires and 

electronic pedometers were used for 

assessment. The pedometer measured the steps 

taken and
 activity time over a two week period. 
Results showed that student intentions to be 

physically active lead in actual behaviour. 

Results also indicated that the subjective norm 

and perceived behavioural control were critical 

predictors of students intenti
ons to be 
physically active.   
 The main message that is outlined in the 
studies above is that successful inclusion in 

physical education can take place effectively.  

The two studies including students with severe 

disabilities and autism, used questionnair
es to 
establish their results. The third study used a 

pre and post
-test design to establish their 
results on the inclusion of a student with 

muscle dystrophy.  All the studies revealed that 

inclusion can be implemented without any 

negative impact on any of
 the students
.   Process Variables 
 Interaction in Inclusive 
Physical Education
 In accordance with Theoretical Model for 
the Study of Classroom Teaching (Dunkin 
 Biddle, 1974) articles focusing on teacher
 classroom behavior and student classroom 

behavior
 were included in the process variables 
results. Studies describing the nature of 

interactions in inclusive physical education 

were selected for this section
.  Experiences of Students with Disabilities with 
Inclusion
 The purpose of a quality physical 
educa
tion programme is to direct and provide 

students with the knowledge and skills to be 
OBrien
 et al.
 Review of Studies in Inclusive Physical Education
   EUJAPA, Vol. 
2, No. 1
 53 physically active throughout their life (Block, 
2007).  Due to greater inclusion in schools 

physical education, teachers will engage 

students with and without disabiliti
es in the 
same class (Verderber et
 al.
, 2003).  It is 
important to question, however, whether these 

students want to be included in physical 

education and whether they experience the 

same level of physical education as their peers. 
 Goodwin and Watkinson (20
00) used a 
maximum variation purposeful sampling 

design when they looked at the experiences of 

nine elementary students with disabilities in 

inclusive physical education.  These students 

were wheelchair users in physical education 

(10-12 years old), data w
as collected through 
interviews, field notes and drawings.  Goodwin 

and Watkinson (2000) discussed the students 

experiences within the conceptual framework 

of ecological perception and affordance theory.  

Themes of sense of belonging, skilful 

participatio
n and sharing in the benefits were 
associated with good days.  In contrast, 

themes of social isolation, questioned 

competence and restricted participation were 

associated with bad days.  Results showed 

that students with disabilities preferred 

particip
ation in inclusive PE rather in 
segregated setting, or in special program. This 

study gave insight into the contributing factors 

of positive and negative experiences of 

students with disabilities. 
 Another study by Hutzler et al. (2002) 
explored the person
al experiences of children 

aged nine to fifteen with physical disabilities in 
physical education in Israel.  The purpose of 
the research was to identify supporting and 

limiting mechanisms with regard to the 

students inclusion and empowerment. 

Purposeful s
ampling was implemented with ten 
students (8 boys, 2 girls) most of whom were 

impaired by cerebral palsy.  Interviews were 

conducted in a semi
-structured manner with an 
eleven point rating scale.  Results illustrated 

that 60% of the students had been tease
d and 
ridiculed by other children, through imitating 

their walk, expressing pity or opening their 

brace.  Forty percents expressed that their peers 

had been supportive in physical education and 

twenty percents of students referred to having 
extracurricular
 interactions with other children 
with a disability, but they wouldnt like to be 

seen with them(page  number?). In regard to 

failures, over half of the comments from the 

students related to experiencing failure in 

physical activity. 
 But what about the 
other side of the coin? 
It is vital to explore the affect of inclusion on 

both students with and without disabilities.  

Faison
-Hodge and Porretta (2004) compared 
the physical activity levels of students with and 

without disabilities during physical educati
on 
and recess.  The school was purposively 

selected as it included students with mental 

retardation (MR) within a GPE class.  

Participants in this study were 46 fourth and 

fifth grade students (8 
 11 years) with eight 
students with mild intellectual disab
ilities. The 
students fitness levels were tested by
 the 
Fitnessgram Progressive Aerobic 

Cardiovascular Endurance Run (PACER).  

Students wore a heart
-rate monitor and were 
videotaped during physical education and 

recess.  Results indicated that students ha
d higher levels of moderate to vigorous physical 

activity (MVPA) during recess than physical 

education.  Students with low cardio
-
respiratory fitness and students with MR had 

similar levels in both physical education and 

recess.  Faison
-Hodge and Porretta 
(2004) 
suggest that these results may be due to 

students undergoing a fitness testing module in 

physical education. A lot of the time would 

have been spent on testing, writing results and 

assisting other students, while recess is free 
time and open choice 
of activities for the 
students with no instruction time.  Overall, the 

study highlights that students with MR can be 

included in physical education.  Due to the 

school previously including students with MR 

in their PE classes, these classes would not 

typic
ally represent the GPE.  The findings may 
not be transferable to all other schools for t
his 
reason and
 results are dependent in part on the 
severity of MR which can impact on the form 

of inclusion.
 Another study with positive corresponding 
results towards 
inclusion emerged from 

utilizing the Newcomb volleyball lead up game 

and an adapted version using a balloon.  
OBrien
 et al.
 Review of Studies in Inclusive Physical Education
   EUJAPA, Vol. 
2, No. 1
 54 Kalyvas and Reid (2003) investigated the effect 
of sport adaptations on students with and 

without disabilities using a quasi
-experimental 
factoria
l design.  The factors being studied 

were participation and enjoyment of thirty five 

students aged seven to twelve, fifteen of which 

had a physical disability.   These students were 

split into three different classes for the 

purposes of the study.  Both th
e adapted and 

non-adapted games were played for fifteen 
minutes each for three classes.  A combination 

of systematic observations, individual 

interviews and questionnaires were used to 

collect the data.  Throughout the three classes 

students with disabilit
ies preferred the adapted 

game, discovering it to be more enjoyable and 

they felt that their peers were more cooperative 

and helpful during the game.  Statements of 

students without disabilities varied with age, 

the two younger classes enjoyed both games, 

finding the adapted game easier to play but still 

fun.  The older students without disabilities 

considered the use of a balloon frustrating as it 

was harder to win points and the pace of the 

game was slower.  All students did realise that 

the adapted game 
helped their peers with 

disabilities and did not have any objections to 

playing the adapted version.  Overall during the 

adapted game all students were skilfully 

successful and had greater activity levels, 

indicating that the adapted game did not hinder 

their performance.  Kalyvas and Reid (2003) 
believe that students with disabilities can be 

included in GPE programs once appropriate 

adaptations are made which do not compromise 
the experience for students without disabilities.  
If over a longer period of ti
me whilst using 
adapted games, this approach could have an 

effect on the attitudes, interest and participation 

of students with and without disabilities.
 The studies showed that experiences of 
students with disabilities greatly differ, some 

having good exp
eriences and others having bad 

ones.  Three of the studies focused on inclusion 

of students with physical disabilities while the 

fourth studied inclusion of students with 

intellectual disabilities.
 Peer tutoring in Inclusive Physical Education
 Peer tutor h
as been highlighted as an 
effective support tool that can be used to assist 
both the teacher and the student with 

disabilities. Fenrick and Peterson (1984) found 

that peer tutoring increased instructional time 

and helped to develop positive attitudes.  The
 successful training and implementation of peer 

tutors has provided encouraging results to
-date.
 Lieberman et
 al. (2000) explored the effect 
of peer tutors on the activity levels of deaf 

students in GPE.  Through the use of a 

purposeful sampling design, ei
ght deaf students 
were gender matched with eight typically 

developing peer students. Peer tutors were 

trained for four to five thirty minute sessions in 

sign language and basic teaching strategies.  

An assessment of the tutors competency was 

implemented t
hrough a theory and practical 

test.  In total of thirty two classes were 

observed over a five month period.  The 

System for Observing Fitness Instruction Time 

(SOFIT) was used to collect data.  A single 

subject delayed multiple baseline design across 

parti
cipants was employed. Results showed 
that there was an increase in moderate to 

vigorous activity levels (MVPA) for deaf 

students. Interestingly there was also an 

increase in the peer tutors MVPA.  All 

students increased their levels of MVPA by at 

least 19
%.  Lieberman and colleagues (2000) 
indicated their was a great deal of value added 

to  the peer tutors from studying teaching 

strategies and engaging in the process of  

feedback that ultimately helped them and 

motivate their disabled peers.  
 Klavina and 
Block (2008) studied the 
effect of nine trained peer tutors on the 
physical, instructional and social interaction 
behaviours of three students with severe and 

multiple disabilities (SMD) and peers without 

disabilities.  The study observed forty six GPE 

cla
sses.  Each class was forty five minutes in 

duration, two to three times a week and 

containing twenty five to thirty students.  Each 

class was videotaped with each SMD students 

wearing a microphone.  Three instructional 

support conditions for SMD were used
 throughout the study: teacher
-directed, peer
-directed and voluntary peer support.  The peer 

tutors underwent training, they used the Tip to 

Teach, Assist and Practice manual (TIP
-TAP 
steps).  After training, each peer tutor was 
OBrien
 et al.
 Review of Studies in Inclusive Physical Education
   EUJAPA, Vol. 
2, No. 1
 55 assessed through three tria
ls; a score of 90% 
had to be attained. The results showed that 
during the teacher
directed instructional 
condition, interaction behaviour between SMD 

students and other peers was low across all 

participants. In contrast, the interaction 

between adult suppo
rt personnel and SMD 

students was high.  During the peer
-mediated 
instructional condition, interaction behaviours 

with peer tutors had an immediate increase.  

Differing from the previous result, interaction 

behaviours with adult support personnel 

decreased
 for all students during the peer
-mediated instructional condition.  It was 

observed by teachers that students with SMD 

enjoyed being assisted by their classmates. 

Klavina and Block (2008)  have outlined that 

social interaction behaviours showed low 

result
s throughout the study, which could 
indicate students did not engage in 

conversations or in nonverbal interactions not 

related to GPE class.  During the voluntary 

peer support segment, the mean scores of 

interaction behaviours with other peers, not 

designa
ted as peer tutors increased for all 
students with SMD.  Inadvertently the 

teachers interaction behaviour decreased 

during voluntary peer support.  Overall positive 

results were denoted from this peer tutoring 

study from teachers, peer tutors and students
 with SMD.
 Most recently Klavina (2008) studied 
the 
effe
ct of peer
-mediated and teacher
-directed
 instructions on the activity engagement time of 

students with severe and multiple disabilities 
(SMD). She studied inclusive GPE sessions 
under two kinds of ins
tructional support 
conditions for three students with SMD: (a) 

teacher
-directed, and (b) peer
-mediated. 
Instructional behaviour data showed that 

during peer
-mediated support conditions the 
instructions provided by tutors were more 

frequent than instruction
s provided by teachers 

during teacher
-directed conditions. Physical 
behaviour data indicated that peer
-mediated 
conditions resulted in similar levels of physical 

behaviour for all students with SMD when 

compared to teachers directed conditions. Also, 

for a
ll students with SMD the activity 
engagement time data was higher in conditions 

where peer tutors were involved.
 Peer tutoring is an increasingly popular and 
highly successful strategy of support within 

physical education.  With appropriate training 

this r
esource could be developed to its 
maximum potential with all students 

benefiting
.   Social Interaction and Active Learning Time in 
Physical Education (ALT
-PE)
 Initially the topics of social interaction and 
ALT
-PE were highlighted as two separate 
areas, but
 due to a lack of articles 
corresponding to the research criteria, these 

have been combined.  The social interaction of 

students is a vital part of physical education.  

The sense of belonging within a group and 

creating friendships are valuable skills gain
ed and developed through social interaction 

(Moffett et al., 2006).  Therefore it is important 

to know whether students with and without 

disabilities, socially interact in physical 

education.
 To establish this Grenier (2006) 
investigated an inclusive physi
cal education 
class with sixteen students over a period of six 

months. One of the students had severe 

cerebral palsy and a visual impairment and the 

study used a social constructionist perspective.  

The data was sourced from interviews, 

observations, docum
ent review and journals.  
For the majority of the time, adaptations were 

made to include all students in the class.  When 

adaptations could not be created, the student 

with a disability did different activities but with 
similar goal outcomes to the rest of
 the class.  
Students were often paired for activities and a 
focus on both skill development and social 

interaction was promoted in the class.  Overall, 

the teaching was focused on the social 

interactions of students while learning and 

developing their ski
lls.  This led to greater 
acceptance and understanding of each other in 

the class.  The results from this study were 

highly positive but could this have primarily 

been due to the small class size.
 Place and Hodge (2001) studied the social 
inclusion of thre
e girls with physical 
disabilities and nineteen students without 

disabilities in GPE during a six week softball 
OBrien
 et al.
 Review of Studies in Inclusive Physical Education
   EUJAPA, Vol. 
2, No. 1
 56 unit.  Data was collected through observations, 
videotaping of classes and interviews.  

Analysis of data was conducted through the use 

of Academ
ic Learning Time for Physical 
Education (ALT
-PE) and the Analysis of 
Inclusion Practices in Physical Education Form 

S (AIPE
-S).  The students with disabilities did 
not receive any assistance in physical education 

but did in other subjects.  Results indicat
ed that 
students with and without disabilities rarely 

engaged in social interaction.  The students 

with physical disabilities tended to stay and 

work together in class, often at a distance from 

the other students.  Results also suggest the 

teacher did not 
emphasise any social 

interaction during the classes.  Students with 

disabilities often felt neglected and 

awkwardness was observed between students 

though there was communication observed 

between the two groups of student did talk.    

There was no demonstr
ation, praise or 
feedback provided by students with and 

without disabilities, but these interactions did 

take place between the students with 

disabilities themselves.  Overall during class 

time, the students with disabilities spent more 

time on
-task compar
ed to their peers.  It is 
evident from this study that social interaction 

needs to be encouraged and emphasised by 

GPE teachers. 
 Within the two contrasting studies, the 
teachers focus and goals played a pivotal role 

in the students opportunities for soc
ial 
interactions.  One of the studies explored the 

inclusion of a student with cerebral palsy and a 
visual impairment.  The second study looked at 
the inclusion of three students with physical 

disabilities. With only two studies in this 

section it highligh
ts the need for more research 
in this area
. Product Variables 
 Effectiveness of 
Inclusive Physical Education
 Various studies have outlined that students 
with disabilities can be included in GPE 

without any negative effects on the other 

students learning 
experience. In accordance 

with Dunkin and Biddle (1974) model that 

suggests studying immediate students growth 

and long
-term effects on learning in inclusive 
physical education, we have selected articles 
studying the effect of IPE on learning of 

students 
with and without disabilities. 
 Obrusnkov et al. (2003) investigated the 
effect of inclusion. They used a pre and post 

test evaluative case study design with a 

purposively selected sample. The aim was to 

evaluate the effect of including a student with 

muscular dystrophy who used a wheelchair into 
a GPE class of twenty one fourth grade 

students without disabilities where the student 

with a disability did not receive any direct 

support throughout the two weeks volleyball 

unit.  The non
-inclusive class compr
ised of 
eighteen fifth grade students.  Results from the 

skill and knowledge tests showed that both 

classes improved on all measures of skills, and 

there was no significant difference between the 

gains of the two classes.  The results of 

attitudinal questi
onnaire CAIPE
-R (Block, 
1995) results revealed that both groups had 

positive attitudes, but students in the inclusive 

class had slightly more accepting attitudes 

towards students with disabilities.  Overall 

there was no significant difference found 

between
 the inclusive and the non
-inclusive 
volleyball class.  The results of the study are 

however based upon a very short time frame 

for true attitude changes to take place.  A 

follow
-up study to see if the attitudes had been 
maintained or changed would provide
 more 
depth to the study.  
 In another study, Block et
 al. (2001) 
conducted a twelve week study to determine 

the effects of partner training by students 

without disabilities to students with severe 
multiple disabilities.  Twenty six students with 
severe mu
ltiple disabilities aged five to twenty 
one attending a Special Education school were 

partnered with twenty five partners from fifth 

and sixth grade students from a local school as 

part of the Special Olympics Motor Activities 

Training Program (MATP).  Pre
-testing of 
motor skills and interviews with parents was 

taken before the study began.  Partners having 

undertaken four hours of training were 

assigned to a particular student.  During the 

twelve weeks the teaching of the motor skills 

was solely the respon
sibility of the partner.  
Results showed significant improvement in 

motor skills and adaptive behaviours in 
OBrien
 et al.
 Review of Studies in Inclusive Physical Education
   EUJAPA, Vol. 
2, No. 1
 57 students after working with their partners.  
Observations were conducted by staff members 

and indicated that while partners and students 

were workin
g together, partners were quite 
talkative and students made more effort than 

usual.  Overall the study outlined that the 

training of students with severe multiple 

disabilities can be successfully carried out by 

fifth and sixth grade students once these 

stu
dents are adequately trained.  
 Similarly Ward and Ayvazo (2006) 
assessed the effects of class wide peer tutoring 

(CWPT) in teaching catching skills.  The 

school selected in this study specialized in the 

inclusion of students with autism.  Two 

students wit
hout disabilities and two students 
with autism in a kindergarten class of sixteen 

students were selected. Authors focused at the 

number of catches participants made during 

each session.  These results were used to 

identify the level of engagement and the w
ork 
completed by the students.  The second 

measure focused on the number of correct 

catching skills highlighting which students 

with disabilities were able to perform skills that 

their peers were performing.  Training of peer 

tutors was implemented in a si
ngle thirty 

minute session prior to the first intervention. 

The results indicated that the scores of 

performance and correct performance of the 

autistic students increased during peer tutoring 

session in comparison to their results during 

whole class instr
uction.  Overall the results 
were positive, but one might question whether 

it was primarily due to the individual assistance 
the students with autism received rather than 
successful inclusion process of these students
.  
DISCUSSION
 Currently at undergraduat
e level of 
Primary and PE teacher training, there is 

minimum specific training for APE (Morley et 

al., 2005).  Hands
-on practicum experiences in 
schools, knowledge of the various disabilities, 

instructional and curricular modifications are 

examples of key 
material that needs to be 

incorporated into teacher training. Greater 

flexibility is needed in the curriculum of the 

higher education institutions (Lieberman et al., 

2002, Smith, 2004).  Immediate intervention 
and re
-structuring of training in these 
instit
utions needs to be introduced 

(Douthwaite, 1990).  An example of one such 

program is the project European Inclusive 

Physical Education Training (www.eipet.eu). 

This focused on competencies of GPE teachers 

in relation to IPE and provides guidelines for 

higher education programme (curriculum) 
development.  
 Peer tutoring seems to be well developed 
and highly successful element of support in 

physical education (Block et al., 2001, Klavina 

and Block, 2008).  With correct training of the 

peer tutor students, i
t is evident that students 
with different types and severity of disabilities 

can be included in GPE (Block et al., 2001; 

Lieberman et al., 2000; Klavina, 2008).  Other 

programmes available to help build awareness, 

understanding and cooperation between 

stud
ents are the Paralympic School Day, 

Awareness Days and Special Olympics Unified 

Games.  Special needs assistants are 

increasingly being employed in schools in 

Ireland.  With increased training in IPE, this 

support could be maximised in assisting the 

studen
ts and the teacher (Davis et al., 2007).  
 APE specialists are a fundamental support 
and resource for including students with 

disabilities in GPE (Lytle & Hutchinson, 2004; 



Developing study programs at univer
sities and 
teacher training institutions specialising in 

Adapted Physical Education/Activity would be 

greatly beneficial.  Having study programs in 

APE would encourage more people to work in 
this area and raise the competence level of 
European professional
s. Various studies 
revealed in the review that teachers found 

including all students in outdoor lessons more 

difficult due to issues of accessibility (Morley 

et al, 2005). The facilities and equipment need 

to be adapted to ensure participation of all 

stude
nts (Fejgin et al., 2005). Research that 
mirrors that of Davis
 et 
al. (2007) is needed in 
Europe, to establish the current status and 

training needs of SEN assistants.  Peer tutoring 

studies could follow Klavina and Block (2008) 

and focusing on different p
eer tutoring 

interventions within the GPE environment.  
 OBrien
 et al.
 Review of Studies in Inclusive Physical Education
   EUJAPA, Vol. 
2, No. 1
 58 Research looking at the attitudes of GPE 
teachers, Primary teachers and students with 
and without disabilities is needed to identify 

the needs of European educators and students.  

Research investigat
ing the adequacy of the 
Primary and Physical Education teachers 

preparation and their willingness to develop an 

inclusive environment is critical if there is to be 

a positive future for inclusive physical 

education across the diverse and changing 

society 
that is contemporary Europe
.  Perspective Paragraph
 Inclusion in physical education can 
effectively work for the child with a disability 

(Goodwin & Watkinson, 2000) and it can work 

without negatively affecting peers without 

disabilities
 (Faison
-Hodge & Por
retta, 2004;
 Obrusnkov et al
., 2003). The success of 
inclusion is greatly increased when various 

factors such as support, personnel, training and 

positive attitudes exist.  Studies showed 

successful and positive inclusive practice could 

be achieved in GP
E even when sometimes all 
of the aforementioned factors were not readily 

available (Obrusnkov et al., 2003).  Europe is 

in the process of making a positive move 

towards greater inclusion of students with both 

mild to severe disabilities.  Legislation is 
the 

vital tool to success, as it creates the blueprints 

for schools and communities to follow. If 

governments and professional organisations in 

Europe will support inclusive PE ultimately the 

experience of GPE of students with disabilities 

is likely to imp
rove
.  REFERENCES
 Ajzen, I. (1991). The theory of planned 
behavior. 
Organizational Behavior and 
Human Decision Processes
, 50, 179-211. BAALPE. (1996). 
Physical Education for 
Pupils with Special Educational Needs in 
Mainstream Education.
 West Midlands: The 
British Association of Advisors and 

Lecturers in Physical Education.
 Block, M. (1995). Development and 
validation of the childrens attitudes toward 

integrated physical education
-revised 
(CAIPE
-R) inventory. 
Adapted Physical 
Activity Quarterly, 12, 
60-77. Block, M.E, (2007). 
A Teacher's Guide to 
Including Students with Disabilities in 

General Physical Education. 
(3rd ed).
 Baltimore: Paul H. Brookes Publishing Co.
 Block, M.E., Conatser, P., Montgomery, R., 
Flynn, L., Munson, D., & Dease, R. (2001). 

Effects o
f Middle School
-Aged Partners on 
the Motor and Affective Behaviors of 

Students with Severe Disabilities. 
Palaestra, 
17 (4), 34
-39. Block, M.E, & Krebs, P. (1992). An 
Alternative to the Continuum of the Least 

Restrictive 
Adapted Physical 
Activity 
Quarterly
 , 9, 97-113. Block, M.E, & Obrusnkov, I. (2007). 
Inclusion in Physical Education: A Review 

of the Literature From 1995
-2005. Adapted 
Physical Activity Quarterly
 , 24, 103-124. Block, M.E, & Vogler, E.W. (1994). Inclusion 
in regular physical education: Th
e research 
base.  
Journal of Physical Education, 

Recreation and Dance, 65(1)
, 40-44. Davis, R.W., Kotecki, J.E., Harvey, M.W., & 
Oliver, A. (2007). Responsibilities and 

Training Needs of Paraeducators in Physical 

Education. 
Adapted Physical Activity 
Quarte
rly, 24,
 70-83. Douthwaite, R. (1990). Meeting Special 
Needs in Mainstream Schools: A Case 

Study.  
British Journal of Physical 

Education, 21 
(4), 393
-396.
 Dunkin, J. 
 Biddle, B. (1974). 
The Study of 
Teaching.
 New York: Holt, Rinehart and 
Winston.  
 Faison
-Hodge, J., & Porretta, D.L. (2004). 
Physical Activity Levels of Students With 

Mental Retardation and Students Without 

Disabilities. 
Adapted Physical Activity 
Quarterly, 21, 
139-152. Fejgin, N., Talmor, R., & Erlich, I. (2005). 
Inclusion and burnout in phy
sical education. 
European Physical Education Review, Vol II 

(I)
, 29-50. Fenrick, N.J., & Peterson, T.K. (1984). 
Developing Positive Changes in Attitudes 

Towards Moderately/Severly Handicapped 

Students Through a Peer Tutoring Program. 

Education and Training
 of the Mentally 
Retarded, 19
, 83-90. OBrien
 et al.
 Review of Studies in Inclusive Physical Education
   EUJAPA, Vol. 
2, No. 1
 59 Goodwin, D.L., & Watkinson, E.J. (2000).  
Inclusive Physical Education From the 
Perspective of Students With Physical 

Disabilities. 
Adapted Physical Activity 
Quarterly, 17
, 144-160. Grenier, M. (2006). A Social Constru
ctionist 
Perspective of Teaching and Learning in 

Inclusive Physical Education. 
Adapted 

Physical Activity Quarterly, 23
, 245-260. Hardin & Brent. (2005). Physical Education 
Teachers' Reflections on Preparation for 

Inclusion. 
Physical Educator, 62 (1)
. Hodge
, S.R., Ammah, J.O.A., Casebolt, K., 
Lamaster, K., & O'Sullivan, M. (2004). 

High School General Physical Education 

Teachers' Behaviors and Beliefs Associated 

with Inclusion. 
Sport, Education and 
Society, 9
(3), 395
-419. Hodge, S.R, Murata, N.M., & Kozub, F.
M. 
(2002). Physical Educators' Judgements 

About Inclusion: A New Instrument for 

Preservice Teachers. 
Adapted Physical 
Education Quarterly, 19
, 435-452. Hodge, S.R., Tannehill, D., & Kluge, M.A. 
(2003). Exploring the Meaning of Practicum 

Experiences for PET
E Students. 
Adapted 
Physical Activity Quarterly, 20
, 381-399. Hutzler, Y., Fliess, O., Chacham, A., & Van 
den Auweele, Y. (2002). Perspectives of 

Children With Physical Disabilities on 

Inclusion and Empowerment: Supporting 

and Limiting Factors. 
Adapted Phy
sical 
Activity Quarterly, 19
, 300
-317. Kalyvas, V., & Reid, G. (2003). Sport 
Adaptation, Participation, and Enjoyment of 
Students With and Without Physical 
Disabilities. 
Adapted Physical Activity 
Quarterly, 20,
 182-199. Klavina, A. (2008). Using peer
-media
ted 
instructions for students with severe and 

multiple disabilities in inclusive physical 

education: A multiple case study. 
European 
Journal of Adapted Physical Activity, 1(2), 

719. Klavina, A. & Block, M.E. (2008).  The 
Effect of Peer Tutoring on Interac
tion 

Behaviors in Inclusive Physical Education. 

Adapted Physical Activity Quarterly, 25
, 
132-158. Kodish, S., Kulinna, P.H., Martin, J., 
Pangrazi, R. & Darst, P. (2006). 

Determinants of Physical Activity in an 

Inclusive Setting. 
Adapted Physical Activity 

Quarterly, 23, 
390-409. Kozub, F. M., Sherblom, P. R., & Perry, T. L. 
(1999). Inclusion Paradigms and 

Perspectives: A Stepping Stone to 

Accepting Learner Diversity In Physical 

Education. 
QUEST,
 51, 346
-354. 
Attitudes Toward Inclusion of Students with 

Physical Disabilities in Physical Education 

in the Revised "ATIPDPE
-R" 
Instrument/Scale for Prospective Czech 

Educators. 
Acta Universitatis Palackianae 
Olomucensis, 37
(1), 13
-18. 
about inclusion of children with disabilities 

into general physical education. 

Proceedings of European Congress of 

Adapted Physical Activities.
 (Available 
Online at www.eufapa.eu)
 
, D. 
 Sherrill, 
C. (2008). The nature of work and roles of  
      public school adapted physical educators in 
the United States. 
European Journal of  
      Adapted Physical Activity, 1(2),
 45-55. 
(2002). Componen
ts/Indicators of Attitudes 

Toward Inclusion of Students with Physical 

Disabilities in PE in the ATIPDPE 

Instrument/Scale for Prospective Czech 

Physical Educators. 
Acta Universitatis 
Palackianae Olomucensis, 32 (2),
 35-39.  Lieberman, L.J., Dunn, J.M., van 
der Mars, 
H., & McCubbin, J. (2000). Peer Tutors' 

Effects on Activity Levels of Deaf Students 

in Inclusive Elementary Physical Education. 

Adapted Physical Activity Quarterly, 17,
 20-39. Lieberman, L. J., & Houston
-Wilson, C. 
(2002). 
Strategies for Inclusio
n, A 
Handbook for Physcial Educators.
 U.S.A: 
Human Kinetics.
 Lieberman, L.J., Houston
-Wilson, C., & 
Kozub, F.M. (2002). Perceived Barriers to 

Including Students With Visual Impairments 

in General Physical Education. 
Adapted 

Physical Activity Quarterly, 19
, 364-377. OBrien
 et al.
 Review of Studies in Inclusive Physical Education
   EUJAPA, Vol. 
2, No. 1
 60 Lienert, C., Sherrill, C., & Myers, B. (2001). 
Physical Educator's Concerns About 
Integrating Children With Disabilities: A 

Cross
-Cultural Comparison. 
Adapted 
Physical Activity Quarterly, 18,
 1-17. Lytle, R.K., Collier, D. (2002). The 
Consultatio
n Process: Adapted Physical 
Education Specialists' Perceptions. 
Adapted 

Physical Activity Quarterly, 19,
 261-279. Lytle, R.K., & Hutchinson, G.E. (2004). 
Adapted Physical Educators: The Multiple 

Roles of Consultants. 
Adapted Physical 

Activity Quarterly, 21
, 34-49. Meegan, S., & MacPhail, A. (2006). Irish 
physical educators' attitude toward teaching 

students with special educational needs. 

European Physical Education Review
 , 12
 (1)
, 75-97. Miller, S. (1994). Inclusion of children with 
disabilities: can we m
eet the challenge? 

Physical Educator
 , 51 (1).
 Moffett, A.C., Alexander, M.G.F., & 
Dummer, G.M. (2006). Teaching Social 

Skills and  Assertiveness to Students with 

Disabilities. 
Teaching Elementary Physical 
Education,
 43-47. Morley, D., Bailey, R., Tan, J.,
 & Cooke, B. 
(2005). Inclusive Physical Education: 

teacher's views of including pupils with 

Special Educational Needs and/or 

disabilities in Physical Education. 
European 
Education Review
, Vol II(I), 84
-107. Murata, N.M., & Jansma, P. (1997). Influence 
of s
upport personnel on students with and 
without disabilties in general physical 
education. 
Clinical Kinesiology, 51 (2)
, 37-46. Obrusnkov, I., Vlkov, H., & Block, M.E. 
(2003). Impact of Inclusion in General 
Physical Education on Students Without 

Disabili
ties. 
Adapted Physical Activity 
Quarterly, 20, 
230-245. Place, K., & Hodge, S.R. (2001). Social 
Inclusion of Students With Physical 

Disabilities in General Physical Education: 

A Behavioral Analysis. 
Adapted  Physical 

Activity Quarterly, 18,
 389
-404. Sherri
ll, C. (2004).
 Adapted physical activity, 
recreation and sport: Crossdisciplinary and 

lifespan
 (6th ed.). Boston, MA: McGraw
-Hill Higher Education.
Smith, A. (2004). 
The inclusion of pupils with special 

educational needs in secondary school 

physical educati
on. Physical Education and 
Sport Pedagogy, 9 (1)
, 37
-53. Stainback, W., & Stainback, S. (1996).
 Inclusion: A Guide for Educators.
 Baltimore: Brookes Publishing Co.
   Verderber, J.M.S., Rizzo, T.L., & Sherrill, C. 
(2003). Assessing Students Intention to 

Par
ticipate in Inclusive Physical Education. 
Adapted Physical Activity Quarterly, 20,
 26-45. Ward, P., & Ayvazo, S. (2006).  Classwide 
Peer Tutoring in Physical Education: 

Assessing Its Effects With Kindergartners 

With Autism. 
Adapted Physical Activity 
Quarte
rly, 23,
 233-244.  
 Corresponding authors e
-mail address: 
martin.kudlacek@upol.cz
 
 
 The study has been supported by the research 

grant from Ministry of Education, Youth and 

Sports of the Czech Republic (No. M
SMT 
6198959221) Physical Activity and Inactivity 

of the Inhabitants of the Czech Republic in the 

Context of Behavioral Changes 
 OBrien
 et al.
 Review of Studies in Inclusive Physical Education
   EUJAPA, Vol. 
2, No. 1
 61 EIN LITERATURBERBLI
CK ZUR INKLUSION VON
 SCHLERN MIT 
BEHINDERUNGEN IN DER
 SCHULISCHEN BEWEGUNG
SERZIEHUNG AUS 
EUROPISCHER PERSPEK
TIVE
 (Resmee)
  Der Status der Inklusion von Schler/innen mit Behinderung im allgemeinen Unterricht fr 
Bewegung und Sport va
riiert stark ber die einzelnen Europischen Lnder, und in vielen Lndern 
steckt die inklusive Bewegungserziehung noch in den Kinderschuhen. Die Absicht dieser Studie ist 
es, einen berblick ber die in Englisch publizierte Literatur, die auf die Inklusio
n von 

Schler/innen mit und ohne Behinderung in der Bewegungserziehung fokussiert ist, zu geben 
 mit dem weiterfhrenden Anliegen, Empfehlungen zur Erleichterung der Inklusion in Europa zu 

entwickeln. Der berblick reicht ber einen Zeitraum von acht Jahr
en, beginnend mit 2000. Die 
Artikel hatten bestimmten Auswahlkriterien zu entsprechen. 27 Artikel entsprachen. Die 

vorgestellte Recherche basiert auf dem theoretischen Modell Study of Classroom Teaching (Dunkin 
 Biddle, 1974), das empfiehlt, dass Untersuc
hungen ber Lehren und Lernen vier Kategorien an 
Variablen einschlieen sollen: Personale Vorbedingung (Lehrer), Kontext (Schler), Prozess 
(Interaktion) und Produkt. Jede Grundkategorie von Variablen hat nachfolgende Untersektionen. 

Ein wichtiger Teil des
 berblicks sind Empfehlungen und Anleitungen an Praktiker in Adapted 
Physical Education, Bewegungserzieher und Schulverwalter ber ffentliche Grundstze und 

erfolgreiche Praxis der inklusiven Bewegungserziehung in Europa
.  SCHLSSELWRTER: 
Bewegungserzie
hung, Bewegung, Integration, Inklusion, Mainstream, 
Behinderung
.  
 REVUE DE LITTERATURE SUR LINCLUSION DES ELEVES EN SITUATION DE 
HANDICAP EN EPS SELON UNE PERSPECTIVE  EUROPENNE
 (Rsum)
  L'tat de l'intgration des lves handicaps en Education Physiq
ue et Sportive 
ordinaire
 varie 
selon les pays europens et dans de nombreux pays, l'ducation physique inclusive est encore aux 

premires tapes. Le but de cette tude est de mener une revue de la littrature publie en anglais 

portant sur l'intgration de
s lves handicaps ou non, en ducation physique, avec l'intention 
dlaborer des recommandations pour faciliter l'intgration en l'Europe. L'tude s'tend sur une 

priode de huit ans commenant au dbut de lanne 2000. Les articles devaient rpondre  d
es 

critres de slection pour tre inclus dans l'tude. Vingt
-sept articles ont rpondu aux critres. Notre 
revue de littrature sest appuye sur le modle thorique sur l'tude de l'enseignement en classe 

(Dunkin et Biddle, 1974) qui suggre que l'tude 
de  l'enseignement et de l'apprentissage concerne 
quatre catgories de variables: presage (enseignant), le contexte (les lves), le processus 

(interaction) et le produit. Chaque domaine cl de variables comprend des sous
-sections. Une partie 
importante de
 cette tude concerne des recommandations pour orienter les professionnels de 
l'ducation physique, les professeurs d'ducation physique et des administrateurs scolaires sur les 

politiques publiques, sur les bonnes pratiques lies  l'intgration en ducat
ion physique en Europe
.  MOTS CLEFS
 : Education physique, activit physique, intgration, inclusion, dficiences
. Reviewed Papers 
inroads  SIGCSE Bulletin - 138 - 
Volume 41, Number 2  2009 June 
   Reading a Computer Science Research Paper 
  Philip W. L. Fong 
Department of Computer Science 
University of Calgary 
Calgary, Alberta, Canada T2N 1N4 
pwlfong@ucalgary.ca
  
 Abstract:  This tutorial article highlights some points that a gr
aduate or senior undergraduate student should bear in 
mind when reading a computer science research paper.  Specifically, the reading process is divided into three tasks: 
comprehension, evaluation and synthesis. 
 The genre of paper review is then introduced as a vehicle for critical reading 
of research papers.  Lastly, guidelines on 
how to be initiated into the trade of 
conference and/or journal paper review 
are given.
  Designed to be used in a graduate course setting, this
 tutorial comes with a suggested marking scheme for 
grading paper reviews with a summary-critique-synthesis structure.
  Categories and Subject Descriptors
:  K.3.2 [Computing Milieux]: Computer a
nd Information Science Education --- 
Computer science education.  
General Terms
:  Documentation 
Keywords:  
Graduate Education, Reading Research Papers, Paper Review
   1.  COMPREHENSION 
The first lesson to reading research papers is learning to 
understand what a paper says.  A common pitfall for a 
beginner is to focus solely on the technicalities.  Yes, 
technical contents are very important, but they are in no 
way the only focus of a careful reading.  In general, you 

should ask yourself the following four questions when you 
are reading a research paper. 
 1. What is the research problem the paper attempts 
to address?  What is the 
motivation
 of the research 
work?  Is there a 
crisis in the research 
eld that the 
paper attempts to resolve?  Is the research work 
attempting to overcome the 
weaknesses
 of existing 
approaches?  Is an existing 
research paradigm
 challenged?  In short, what is the 
niche
 of the paper? 
2. What are the claimed contributions of the paper?
  What is new in this paper?  A new 
question
 is asked?  
A new 
understanding
 of the research problem?  A new 
methodology
 for solving problems?  A new 
algorithm
?  A new breed of software 
tools
 or systems?  A new 
experimental method
?  A new 
proof technique
?  A new 
formalism
 or 
notation
?  A new 
evidence
 to substantiate 
or disprove a previously published claim?  A new 

research area
?  In short, what is 
innovative
 about this 
paper? 
3. How do the authors substantiate their claims? 
 What is the 
methodology
 adopted to substantiate the 
claims?  What is the 
argument
 of the paper?  What are 
the major 
theorems
?  What 
experiments
 are conducted?  
Data analyses
?  Simulations
?  Benchmarks
?  User 
studies
?  Case studies
? Examples
?  In short, what 
makes the claims 
scientic (as opposed to being mere 
opinions
1)? 
4. What are the conclusions? 
 What have we 
learned
 from the paper?  Shall the 
standard practice
 of the 
eld be changed as a result of the new 
ndings?  Is the 
result 
generalizable
?  Can the result be applied to 
other 
areas
 of the 
eld?  What are the 
open problems
?  In 
short, what are the 
lessons
 one can learn from the 
paper? 
 
Every well-written research paper contains an 
abstract, which is a summary of the paper. 
 The role of an abstract is 
to outline the answers to the above questions.  Look 

therefore, rst to the abstract for answers.  The paper 
should be an elaboration of the abstract. 
Another way of looking at paper reading is that every 
good paper tells a 
story.  Consequently, when you read a 
paper, ask yourself, What is the plot? The four questions 

listed above make up an archetypical plot structure for 
every research paper. 
 2.  EVALUATION 
An integral component of scholarship is to be critical of 
scientic claims.  Ambitious claims are usually easy to 
make but difficult to substantiate.  Solid scholarship 
involves careful validation of scienti
c claims.  Reading 
research papers is therefore an
 exercise of critical thinking. 
                                                 1 Alternatively, what makes it 
a research paper rather than 
science fiction? Reviewed Papers 
inroads  SIGCSE Bulletin - 139 - 
Volume 41, Number 2  2009 June 
o Is the research problem signi
cant?  Is the work scratching 
minor itches
?  Are the authors solving 
articial 
problems
 (aka 
straw man
)?  Does the work enable 
practical applications
, deepen 
understanding
, or explore 
new design space
? o Are the contributions signicant?  Is the paper 
worth 
reading
?  Are the authors simply 
repeating the state of 
the art?  Are there real 
surprises
?  Are the authors aware 
of the relation of their work to 
existing literature
1?  Is the 
paper addressing a well-known 
open problem
? o Are the claims valid?  Have the authors been 
cutting 
corners
 (intentionally or unintentionally)?  Has the right 
theorem been proven?  Errors in proofs?  Problematic 
experimental setup?  Confounding factors?  Unrealistic, 

articial benchmarks?  Comparing apples and oranges?  
Methodological misunderstanding?  Do the numbers add 
up?  Are generalizations valid?  Are the claims modest 

enough2?  When you evaluate a research work, two caveats are worth 

noting: 
o Consistently evaluating research works in a negative way 
gives a young researcher a false sense of being critical.  

Learn to be fair:  attend to both the strengths and 

weaknesses of the work. If you are reading a classical 
paper that has been published for a while, make sure you 
are reading the paper in the right historical context:  What 
seems to be obvious now might have been ground-

breaking then. o A young researcher may want to focus on point 3 (
Are 
the claims valid?
). Evaluating the signi
cance of the 
research problem and the contributions of the paper 
usually requires a comprehensive understanding of the 

research 
eld as a whole. Yet, do not let the above 
comment hinder you from disagreeing with the paper 
authors in matters of signi
cance. 
 3.  SYNTHESIS Creativity does not arise from 
the void.  Interacting with 
the scholarly community through reading research papers is 
one of the most effective ways for generating novel 
research agendas.  When you read a research paper, you 

should see it as an opportunity for you to come up with new 
research projects.  The following is a list of questions you 
can ask to help in this direction.  (Of course, this list is not 

supposed to be exhaustive.) 
o What is the crux of the research problem? 
                                                 1 Be very sceptical of
 work that is so 
novel that it bears no relation 
to any existing work, builds upon no existing paradigm, and yet addresses 
a research problem so significant that 
it promises to transform the world.  
Such are the signs that the author mi
ght not be aware of existing literature 
on the topic.  In such a case, the authors could very well be repeating 
works that have already 
been done decades ago. 2 It is very tempting for an inexpe
rienced researcher to make overly 
general conclusion from limited evid
ence.  A high quality scientific claim 
is always 
modest --- claiming only what can be concluded from the 
evidence, making explicit the limitati
on of the evidence, and carefully 
delimiting the scope of the claim. 
o What are some alternative way to substantiate the claim 
of the authors? 
o Is there an alternative way to substantiate the claim of the 

authors? 
o What is a good argument against the case made by the 

authors? 
o Can the research results be strengthened? 
o Can the research results be applied to another context? 
o What are the open problems raised by this work? 
 Bottom line:
  If you were to do the research, how would 
you do differently? 
 4.  PAPER REVIEW A paper review is a short essay (5 pages, single-space, 1-
inch margins, 12 point font) reporting what you have 
learned from reading a research paper.  Writing reviews for 
the papers you have read is a great way to sharpen your 
paper reading skills.  Such a review is typically structured 
in three sections  
summary
, critique
 and 
synthesis. 
1. Summary. 
 Give a brief summary of the work 
in your 
own words
.  This section demonstrates your 
understanding of the paper, and as such it should answer 
the four questions outlined in Section 1. The summary 

section should be structured as follows: (1) motivation, 
(2) contribution, (3) methodology and/or argument, and 
(4) conclusion. It is imperative that you use your own 

words to summarize the paper.  Failing to adhere to this 

guideline not only constitutes plagiarism, but also 

demonstrates that you probably do not quite understand 

the work.  You can be sure that you understand something 

only when you are capable of explaining it in your own 

words. 2. Critique.  Pick two to three points
3 you want to argue 
with the authors
4.  Use the questions outlined in Section 2 
to help you come up with meaningful critiques.  Do not 
repeat the Limitations
 section of the paper. Doing so 
means that you 
agree with the authors!  Pick points of 
disagreement
, and launch an intellectual debate with the 
authors.  Carefully articulate and substantiate your case.  
Do not just say, I dont like this point.  Instead, give 
technical reasons to substantiate your critiques.  Be 
specic in your choice of words.  Avoid generic 
adjectives such as bad, poor, lame, stupid, etc, 

and their synonyms and antonyms.  You can go a lot 

further by replacing such vague words with more speci
c                                                  3 This restriction on the number of cr
itiques is intentional.  Firstly, the 
restriction forces you to pinpoint the 
major weaknesses of the paper, rather 
than to spend efforts debating issues 
of peripheral importance.  Secondly, 
such a restriction allows you to en
joy the mental room necessary for 
developing a substantial case against the authors. 
4 Notice that the Critique section pr
esents only negative evaluations of 
the paper.  Have we forgotten about 
being fair to a research work?  No, 
positive evaluations are omitted for a good reason.  Experience tells us 
that students thend to give positive evaluation in the following form: I 
agree with the authors.  They did this and that, and they did a good job.  

The end result is usually a repetition of the authors' claims.  I find that 

focusing on critiques offers a more su
bstantial learning experience to the students, forcing them to th
ink rather than to parrot. Reviewed Papers 
inroads  SIGCSE Bulletin - 140 - 
Volume 41, Number 2  2009 June 
ones:  inelegant, inefficient, memory-intensive, 
ill-dened, etc. 3. Synthesis.
  Propose one to two ways in which the 
research work can be further developed.  Do not repeat 

the Future Work
 section of the paper.  Be original.  
Consult the list of questions in Section 3 if you run out of 
ideas.   I use this format when I ask students in my graduate classes 
to review a paper. Consult the Appendix for a sample 
outline, a marking scheme, and page length suggestions.  
 5. AN ALTERNATIVE PAPER REVIEW FORMAT 
The format of paper review outlined in the previous section 
is the one I adopt for my graduate classes.  It works for me, 

but it is de
nitely not the only way to structure a paper 
review.  I outline here an alternative format I learned from 

a friend of mine.  
1. What is the purpose of the work? 
2. How do the authors achieve this purpose?  Why is this 
particular approach adopted? 
3. Do you think the purpose has been achieved? 
4. What insights have you gained from reading this work? 
 Notice the parallel between this alternative structure and 
the summary-critique-synthesis 
structure in the previous 
section. 
   6. READING RESEARCH PAPERS LIKE A PRO 
When a research paper is submitted to a conference or a 
journal, it will undergo a 
peer review
 process, in which the 
paper is subject to the intense scrutiny of peer researchers.  
The referees
 who review the submitted paper will read the 
paper in more or less the same way as we outlined in Sections 
1 and 2, and then they will write up a 
referee report
 in a style 
similar to the paper review discussed in Section 4, except for 

the synthesis section.  Based on the referee reports, the 

program chair of a conference or the editor of a journal will 

then make the decision of whether to accept the paper.  It is 

therefore instructional to understand how a referee go about 

reviewing a paper, and learn to read research papers like a 

professional.  A very good introduction to the subject can be 

found in an article by Smith [1].  The paper is slanted towards 
experimental computer science.  For a perspective focusing on 

theoretical computer science, consult the article by Parberry 

[2].  After reading these papers, I highly recommend graduate 

students to 
nd opportunities to practice professional paper 
reviewing.  Your thesis supervisor will likely be involved in 

the program committees of conferences, or asked to review 

papers for conferences.  Approach your supervisors, and 

volunteer to help out with paper reviews.  By actually writing 

up a professional review report, and discussing your review 

with your supervisor, you will gain tremendous insight into the 

paper publishing process, as well as the implicit value system 

of the academic world.  This kind of training is hard to acquire 

through other means. 
 BIBLIOGRAPHY 
[1] Alan Jay Smith, The 
task of the referee, 
IEEE Computer
, 23(4):65-71, April 1990. 
[2] Ian Parberry, A guide for new referees in theoretical computer science, 
Information and Computation
, 112(1):96-116, July 1995. 
   APPENDIX: MARKING SCHEME AND OUTLINE OF A PAPER REVIEW 
1. SUMMARY (40%: 2.5 pages) 
a. Motivation (8%) 
b. Contribution (8%) 
c. Methodology (16%) 
d. Conclusion (8%) 
2. CRITIQUE (30%: 1.5 pages) 
a. 1st Critique (15%) 
b. 2nd Critique (15%) 
c. Optional: 3
rd Idea 
 If a 3rd critique is given, then each critique is worth 10%.  Students ar
e encouraged to focus their efforts in two rather than three c
ritiques. 3. SYNTHESIS (30%: 1 page) 

a. 1st Idea (30%) b. Optional: 2
nd Idea 
 If a 2nd idea is presented, then each idea is worth 15%.  Students ar
e encouraged to focus their efforts on one rather than two ideas. 
 Operating Systems B. RANDELL, Editor The Nucleus of a Multiprogramming System PIER BRINCIt HANSEN A /S Regnecentralen, Copenhagen, Denmark This paper describes the philosophy and structure of a multi- programming system that can be extended with a hierarchy of operating systems to suit diverse requirements of program scheduling and resource allocation. The system nucleus sim- ulates an environment in which program execution and input/ output are handled uniformly as parallel, cooperating proc- esses. A fundamental set of primitives allows the dynamic creation and control of a hierarchy of processes as well as the communication among them. KEY WORDS AND PHRASES: multiprogramming, operating systems, paralle I processes, process concept, process communication, message buffering, process hierarchy, process creation, process removal CR CATEGORIES: 4.30, 4.31, 4.32, 4.41 l, Introduction The multiprogramming system developed by Regnecen- tralen for the RC 4000 computer is a general tool for the design of operating systems. It allows the dynamic creation of a hierarchy of processes in which diverse strategies of program scheduling and resource allocation can be imple- mented. For the designer of advanced information systems, a. vital requirement of any operating system is that it allow him to change the mode of operation it controls; otherwise. his freedom of design can be seriously limited. Unfortu- nately, this is precisely what present operating systems do, not allow. Most of them are based exclusively on a single, mode of operation, such as batch processing, priority scheduling, real-time scheduling, or conversational access. When the need arises, the user often finds it hopeless to, modify an operating system that has made rigid assump- tions in its basic design about a specific mode of operation. The alternative--to replace the original operating system with a new one--is in most computers a serious, if not im- possible, matter because the rest of the software is inti- mately bound to the conventions required by the origina~ system. This unfortunate situation indicates that the mairr. problem in the design of a multiprogiramming system is not. to define functions that satisfy specJ.fie operating needs, but~ rather to supply a system lmeleus that can be extended: with new operating systems i~ an orderly manner. This is: the primary objective of the RC 4000 system. In the following, the philosophy and structure of the RC 4000 multiprogramming system is explained. The dis- cussion does not include details of implementation; size and performance are presented, however, to give an idea of the feasibility of this approach. The functional specifica- tions of the multiprogramming system are described in detail in a report [1] available from Regnecentralen. 2. System Nucleus Our basic attitude during the designing was to make no assumptions about the particular strategy needed to optimize a given type of installation, but to concentrate on the fundamental aspects of the control of an environment consisting of parallel, cooperating processes. Our first task was to assign a precise meaning to the process concept, i.e. to introduce an unambiguous ter- minology defining what a process is and how it is imple- mented on the actual computer. The next step was to select primitives for the synchro- nization and transfer of information among parallel processes. Our final decisions concerned the rules for the dynamic creation, control, and removal of processes. The purpose of the system nucleus is to implement these fundamental concepts: simulation of processes; communi- cation among processes; creation, control, and removal of processes. 3. Processes We distinguish between internal and external processes, roughly corresponding to program execution and input/ output. More precisely, an internal process is the execution of one or more interruptable programs in a given storage area. An internal process is identified by a unique process name. Thus other processes need not be aware of the actual loca- tion of an internal process in the store, but can refer to it by name. A sharp distinction is made between the concepts pro- gram and internal process. A program is a collection of instructions describing a computational process, whereas an internal process is the execution of these instructions in a given storage area. In connection with input/output, the system distin- guishes between peripheral devices, documents, and ex- ternal processes. A peripheral device is an item of hardware connected to the data channel and identified by a device number. A document is a collection of data stored on a physical medium, such as a deck of punched cards, a printer form, a reel of magnetic tape, or a file on the backing store. An external process is the input/output of a given docu- ment identified by a unique process name. This concept 238 Comxnuuieatton~ ~f the~ACM Volume 13 / Number 4 / April, 1970 implies that internal processes can refer to documents by name without knowing the actual devices on which they are mounted. Multiprogramming and communication between inter- nal and external processes are coordinated by the system nucleus--an interrupt response program with complete control of input/output, storage protection, and the inter- rupt system. We do not regard the system nucleus as an independent process, but rather as a software extension of the hardware structure, which makes the computer more attractive for mu]tiprogramming. Its function is to imple- ment our process concept and primitives that processes can invoke to create and control other processes and communi- cate with them. So far we have described the multiprogramming system as a set of independent, parallel processes identified by names. The emphasis has been on a clear understanding of relationships among resources (store and peripherals), data (programs and documents), and processes (internal and external). 4. Process Communication In a system of parallel, cooperating processes, mecha- nisms must be provided for the synchronization of two processes during a transfer of information. Dijkstra has demonstrated that indivisible lock and unlock operations operating on binary semaphores are sufficient primitives from a logical point of view [3]. We have been forced to conclude, however, that the semaphore concept alone does not fulfill our requirements of safety and efficiency in a dynamic environment in which some processes may turn out to be black sheep and break the rules of the game. Instead we have introduced message buffering within the system nucleus as the basic means of process communica- tion. The system nucleus administers a common pool of message buffers and a message queue for each process. The following primitives are available for the communi- tion between internal processes: send message (receiver, message, buffer), wait message (sender, message, buffer), send answer (result, answer, buffer), wait answer (result, answer, buffer). Send message copies a message into the first available buffer within the pool and delivers it in the queue of a named receiver. The receiver is activated if it is waiting for a message. The sender continues after being informed of the identity of the message buffer. Wait message delays the requesting process until a mes- sage arrives in its queue. When the process is allowed to proceed, it is supplied with the name of the sender, the contents of the message, and the identity of the message buffer. The buffer is removed from the queue and made ready to transmit an answer. Send answer copies an answer into a buffer in which a message has been received and delivers it in the queue of the original sender. The sender of the message is activated if it is waiting for the answer. The answering process con- tinues immediately. Wait answer delays the requesting process until an answer arrives in a given buffer. On arrival, the answer is copied into the process and the buffer is returned to the pool. Th6 result specifies whether the answer is a response from another process or a dummy answer generated by the system nucleus in response to a message addressed to a nonexisting process. The procedure wait message forces a process to serve its queue on a first-come, first-served basis. The system, how- ever, also includes two primitives that enable a process to wait for the arrival of the next message or answer and serve its queue in any order. This communication scheme has the following advan- tages. The multiprogramming system is dynamic in the sense that processes can appear and disappear at any time. Therefore a process does not in general have a complete knowledge of the existence of other processes. This is reflected in the procedure wait message, which makes it possible for a process to be unaware of the existence of other processes until it receives messages from them. On the other hand, once a communication has been established between two processes (i.e. by means of a message) they need a common identification of it in order to agree on when it is terminated (i.e. by means of an answer). Thus we can properly regard the selection of a buffer as the creation of an identification of a conversation. A happy consequence of this is that it enables two processes to exchange more than one message at a time. We must be prepared for the occurrence of erroneous or malicious processes in the system (e.g. undebugged pro- grams). This is tolerable only if the system nucleus ensures that no process can interfere with a conversation between two other processes. This is done by storing the identity of the sender and receiver in each buffer and check- ing it whenever a process attempts to send or wait for an answer in a given buffer. Efficiency is obtained by the queueing of buffers, which enables a sending process to continue immediately after delivery of a message or an answer, regardless of whether or not the receiver is ready to process it. To make the system dynamic, it is vital that a process can be removed at any time, even if it is engaged in one or more conversations. In this case, the system nucleus leaves all messages from the removed process undisturbed in the queues of other processes. When these processes answer them, the system nucleus returns the buffers to the com- mon pool. The reverse situation is also possible: during the removal of a process, the system nucleus finds unanswered messages sent to the process. These are returned as dummy answers to the senders. The main drawback of message buffering is that it intro- duces yet another resource problem, since the common pool contains a finite number of buffers. If a process were Volume 13 / Number 4 / April, 1970 Communications of the ACM 239 allowed to empty the pool by sending messages to igno- rant processes, which do not respond with answers, further communication within the system would be blocked. Con- sequently a limit is set to the number of messages a process can send simultaneously. By doing this, and by allowing a process to transmit an answer in a received buffer, we have placed the entire risk of a conversation on the process that opens it. 5. External Processes Originally the communication primitives were designed for the exchange of messages between internal processes. Later we also decided to use send message and wait answer for communication between internal and external processes. For each kind of external process, the system nucleus contains a piece of code that interprets a message from an internal process and initiates input/output using a storage area specified in the message. When input/output is termi- nated by an interrupt, the nucleus generates an answer to the internal process with information about actual block size and possible error conditions. This is essentially the implementation of the external process concept. We consider it to be an important aspect of the system that internal and external processes are handled uniformly as independent, self-contained processes. The difference between them is merely a matter of processing capability. A consequence of this is that any external process can be replaced by an internal process of the same name if more complex criteria of access and response become desirable. External processes are created on request from internal processes. Creation is simply the assignment of a name to a particular peripheral device. To guarantee internal proc- esses exclusive access to sequential documents, primitives are available for the reservation and release of external processes. Typewriter consoles are the only external processes that can send messages to internal processes. The operator opens a conversation by pushing an interrupt key and typing the name of the internal receiver followed by a line of text. A file on the backing store can be used as an external process by copying a description of the file from a catalog on the backing store into the system nucleus; following this, internal processes can initiate input/output by send- ing messages to the file process. Real-time synchronization of internal processes is ob- tained by sending messages to a clock process. After the elapse of a time interval specified in the message, the clock returns an answer to the sending process. In general, external processes can be used to obtain synchronization between internal processes and any signal from the external world. For example, an internal process may send a message to a watchdog process and receive an answer when a magnetic tape is mounted on a station. In response, the internal process can give the station a tem- porary name, identify the tape by reading its label, and rename the station accordingly. 6. Internal Processes A final set of primitives in the system nucleus allows the creation, control, and removal of internal processes. Internal processes are created on request from other internal processes. Creation involves the assignment of a name to a contiguous storage area selected by the parent process. The storage area must be within the parent's own area. After creation, the parent process can load a program into the child process and start it. The child process now shares computing time with other active processes includ- ing the parent process. On request from a parent process, the system nucleus waits for the completion of all input/output initiated by a child process and stops it. In the stopped state, the process can still receive messages and answers in its queue. These can be served when the process is restarted. Finally, a parent process can remove a child process in order to assign its storage area to other processes. According to our philosophy, processes should have complete freedom to choose their own strategy of program scheduling. The system nucleus only supplies the essential primitives for initiation and control of processes. Conse- quently, the concepts of program loading and swapping are not part of the nucleus. Time-sharing of a common storage area among child processes on a swapping basis is possible, however, because the system does not check whether inter- nal processes overlap each other as long as they remain within the storage areas of their parents. Swapping from process A to process B can be implemented in a parent process as follows: stop(A) ; output(A); input(B) ; start(B). 7. Process Hierarchy The idea of the system nucleus has been described as the simulation of an environment in which program execution and input/output are handled uniformly as parallel, co- operating processes. A fundamental set of primitives allows the dynamic creation and control of processes as well as communication among them. For a given installation we still need, as part of the sys- tem, programs that control strategies of operator com- munication, program scheduling, and resource allocation; but it is essential for the orderly growth of the system that these operating systems be implemented as other programs. Since the difference between operating systems and pro- duction programs is one of jurisdiction only, this problem is solved by arranging the internal processes in a hierarchy in which parent processes have complete control over child processes. After initial loading, the internal store contains the sys- tem nucleus and a basic operating system, S, which can create parallel processes, A, B, C, etc., on request from consoles. The processes can in turn create other processes, D, E, F, etc. Thus while S acts as a primitive operating system for A, B, and C, these in turn act as operating sys- tems for their children, D, E, and F. This is illustrated by Figure 1, which shows a family tree of processes on the left 240 Communications of the ACM Volume 13 / Number 4 / April, 1970 and the corresponding storage allocation on the right. This family tree of processes can be extended to any level, sub- ject only to a limitation of the total number of processes. In this multiprogramming system, all privileged func- tions are implemented in the system nucleus, which has no built-in strategy. Strategies can be introduced at the var- ious higher levels, where each process has the power to control the scheduling and resource allocation of its children. The only rules enforced by the nucleus are the following: a process can only allocate a subset of its own resources (including storage and message buffers) to its children; a process can only start, stop, and remove its own children (including their descendants). After removal of a process, its resources are returned to the parent process. Fro. 1 SYSTEM NUCLEUS A D E S B F G H Initially all system resources are owned by the basic operating system S. For details of process control and re- source allocation, the reader should consult the manual of the system [1]. We emphasize that the only function of the family tree is to define the rules of process control and resource alloca- tion. Computing time is shared by round-robin scheduling among active processes regardless of their position in the hierarchy, and each process can communicate with all other processes. Regarding the future development of operating systems, the most important characteristics of the system can now be seen as the following. 1. New operating systems can be implemented as other programs without modification of the system nucleus. In this connection, we should mention that the ALGOL and FORTRAN languages for the RC 4000 contain facilities for calling the nucleus and initiating parallel processes. Thus it is possible to write operating systems I in high-level lan- guages. 2. Operating systems can be replaced dynamically, thus enabling an installation to switch among various modes of operation; several operating systems can, in fact, be active simultaneously. 3. Standard programs and user programs can be executed under different operating systems without modi- fication, provided there is common agreement on the possi- ble communication between parents and children. 8. Implementation The RC 4000 is a 24-bit, binary computer with typical instruction execution times of 4 microseconds [2]. It per- mits practically unlimited expansion of the internal store and standardized connection of all kinds of peripherals. Multiprogramming is facilitated by program interruption, storage protection, and privileged instructions. The present implementation of the system makes multi- programming feasible with a minimum store of 16K-32K words backed by a fast drum or disk. The system nucleus includes external processes for a real-time clock, type- writers, paper tape input/output, line printer, magnetic tape, and files on the backing store. The size of the nucleus and the basic operating system is as follows: words primitives 2400 code for external processes 1150 process descriptions and buffers 1250 system nucleus 4800 basic operating system 1400 6200 The communication primitives are executed in the un- interruptable mode within the system nucleus. The execu- tion times of these set a limit to the system's response to real-time events: msec send message 0.6 wait answer 0.4 wait message 0.4 send answer 0.6 An analysis shows that the 2 milliseconds required by a complete conversation (the sum of the four primitives) are used as follows: percent validity checking 25 process activation 45 message buffering 30 This distribution is so even that one cannot hope to in- crease the speed of the system by introducing additional, ad hoc machine instructions. The only realistic solution is to make the hardware faster. The primitives for creation, start, stop, and removal of processes are implemented in an anonymous internal process within the system nucleus to avoid intolerably long periods in the uninterruptable mode. Typical execution times for these are: msec create process 3 start process 26 stop process 4 remove process 30 (Continued on page 250) Volume 13 / Number 4 / April, 1970 Communications of the ACM 241 The analysis presented here suggests that spatial domains are the primitive element of this particular graphic language. In this light, the common assumption that line segments are the primitives of many graphic languages may require revision. RECEIVED JUNE, 1969; REVISED OCTOBER, 1969 REFERENCES 1. GRoss, MAURICE, AND NIVAT, MAURICE. A command language for visualization and articulated movements. In Computer and Information Sciences II, Julius T. Tou (Ed), Academic Press, New York, 1967. 2. NILSSON, NILS J. A mobile automaton: An application of artificial intelligence techniques. Proc. Int. Joint Conf. Artificial Intelligence, May 1969, Washington, D. C. 3. EASTMAN, CHARLES M. Explorations of the cognitive proc- esses of design, Dep. of Comput. Sci., Carnegie-Mellon U., Feb. 1968, ARPA Rep. DDC No. AD671158, Clearinghouse, Springfield, VA 22151. 4. EASTMAN, CHARLES M. Cognitive processes and ill-defined problems: A case study from design, Proc. Int. Joint Conf. Artificial Intelligence, May 1969, Washington, D. C. 5. HOWDEN, W. E. The sofa problem. Comput. 3". 11, 3 (Nov. 19687, 299-301. 6. SUTHERLAND, I. E. Sketchpad: a man-machine graphical communication system. Prec. AFIPS 1963 Spring Joint Comput. Conf., Vol. 23, Spartan Books, New York, pp. 329- 346. 7. GRAY, J. C. Compound data structure for computer aided design: a survey, Proc. ACM 22nd Nat. Conf. 1967, Thomp- son Book Co., Washington, D. C., pp. 355--365. 8. THOMAS, E. M. GRASP--~ graphic service program. Proc. ACM 22nd Nat. Conf., 1967, MDI Publications, Wayne, Pa., pp. 395-402. 9. ARMOUR, GORDON C., AND BUFFA, Elwoov. A heuristic algorithm and simulation approach to relative location of facilities. Man. Sci. (Jan. 1963), 244-309. 10. LEE, R. B. AND MOORE, J. M. CORELAP--computerized relationship layout planning, J. Indust. Eng., 18, 3 (Mar. 1967) 195-200. 11. SIMPSON, M. G., ET AL. The planning of multi-storybuildings: a systems analysis and simulation approach. Proc. European Meeting on Statistics, Econometrics and Management Science, Amsterdam, Sept. 1968. 12. BARKEN, ROBERT. A set of algorithms for automatically laying out hybrid integrated circuits. Internal working doc., Bell Telephone Lab., Holmdel, N. J., Aug. 1968. 13. NILSSON, N. J., AND RAPHAEL, B. Preliminary design of an intelligent robot. In Computer and Information Sciences II, Julius T. Tou (Ed.), Academic Press, New York, 1967. 14. ROSEN, C. A., AND NILSSON, N. J. Application of intelligent automata to reconnaisance. SRI Project 5953, Third Interim Report, Rome Air Develop. Center, Rome, N. Y., Dec. 1967. 15. FAIR, G. R., FLOWERDEW, ET AL. Note on the computer as an aid to the architect. Comput. J. 9, 1 (June 1966). 16. GRISWOLD, R., POAGE, J., AND POLONSKY, I. The SNOBOIA programming language. Bell Telephone Lab., Holmdel, N. J., Aug., 1968. 17. McCARTHY, JOHN, ET AL. LISP1.5 Programmer's Manual. MIT Press, Cambridge, Mass., 1965. 18. MORAN, THOMAS. Structuring three-dimensional space for computer manipulation. Dep. Comput. Sci. working paper, Carnegie-Mellon U., Pittsburgh, Pa., June, 1968. 19. MORAN, THOMAS. A model of a multi-lingual designer. In Emerging Methods in Environmental Design and Planning, G. Moore (Ed.), MIT Press, Cambridge, Mass. (in press). 20. WYLIE, C. ROMNEY, ET AL. Halftone perspective drawings by computer. Teeh. Rep. 4-2, Comput. Sci. Dep., U. of Utah, Salt Lake City, Utah, Feb. 1968. T Hansen--cont'd from page 241 The excessive times for the start and removal of an internal process are due to the peculiar storage protection system of the RC 4000, which requires the setting of a protection key in every storage word of a process. 9. Conclusion Ideas similar to those described here have been sug- gested by others [4-6]. We have presented our system because we feel that, taken as a whole, it represents a sys- tematic and practical approach to the design of replaceable operating systems. As an inspiration to other designers, it is perhaps most important that it illustrates a sequence of design steps leading to a general system nucleus, namely, the definition of the process concept, the communication scheme, and the dynamic creation and structuring of processes. We realize, of course, that a final evaluation of the sys- tem can only be made after it has been used to design a number of operating systems. 250 Communications of the ACM Acknowledgments. The design philosophy was de- veloped by J~rn 5ensen, S~ren Lauesen, and the author. Leif Svalgaard participated in the implementation and testing of the final product. Regarding fundamentals, we have benefited greatly from Dijkstra's analysis of cooperating sequential processes. RECEIVED JULY, 1969; REVISED JANUARY, 1970 REFERENCES 1. RC $000 Soflware: Multiprogramming System. P. Brinch Hansen (Ed.). A/S Regnecentralen, Copenhagen, 1969. 2. RC 4000 Computer: Reference Manual. P. Brinch Hansen (Ed.). A/S Regnecentralen, Copenhagen, 1969. 3. DIJKSTRA, E. W. Cooperating Sequential Processes. Math. Dep., Technological U., Eindhoven, Sept. 1965. 4. I'IARRISON, M. C., AND SCHWARTZ, J. W. SHARER, a time sharing system for the CDC 6600. Comm. ACM 10, (Oct. 1967), 659. 5. I'IUXTABLE, D. H. R., AND WARWICK, M. T. Dynamic super- visors--their design and construction. Proc. ACM Syrup. on Operating System Principles, Gatlinburg, Tenn., Oct. 1--4, 1967. 6. WICHMANN, B. A. A modular operating system. Proc. IFIP Cong. 1968, North Holland Pub. Co., Amsterdam, p. C48. Volume 13 / Number 4 / Apri|,r1970 The Structure of the "THE"-Multiprogramming Edsger W. Dijkstra Technological University, Eindhoven, The Netherlands System A multiprogramming system is described in which all ac- tivities are divided over a number of sequential processes. These sequential processes are placed at various hierarchical levels, in each of which one or more independent abstractions have been implemented. The hierarchical structure proved to be vital for the verification of the logical soundness of the design and the correctness of its implementation. KEY WORDS AND PHRASES: operating system, multlprogrammlng system, system hierarchy, system structure, real-tlme debugging, program verification, synchronizing primitives, cooperating sequential processes, system levels, input-output bufferingt mulfiprogramming, processor sharing, mulfiprocessing' CR CATEGORIES: 4.30, 4.32 Introduction In response to a call explicitly asldng for papers "on timely research and development efforts," I present a progress report on the multiprogramming effort at the Department of Mathematics at the Technological Uni- versity in Eindhoven. Having very limited resources (viz. a group of six peo- ple of, on the average, haif-time availability) and wishing to contribute to the art of system design--including all the stages of conception, construction, and verification, we were faced with the problem of how to get the necessary experience. To solve this problem we adopted the follow- ing three guiding principles: (1) Select a project as advanced as you can conceive, as ambitious as you can justify, in the hope that routine work earl be kept to a minimum; hold out against all pres- sure to incorporate such system expansions that would only result into a purely quantitative increase of the total amount of work to be done. (2) Select a machine with sound basic characteristics (e.g. an interrupt system to fall in love with is certainly an inspiring feature); from then on try to keep the spe- cific properties of the configuration for which you are pre- paring the system out of your considerations as long as possible. (3) Be aware of the fact that experience does by no means automatically lead to wisdom and understanding; in other words, make a conscious effort to learn as much as possible fl'om your previous experiences. Presented at an ACM Symposium on Operating System Principles, Gatlinburg, Tennessee, October 1-4, 1967. Volume 11 / Number 5 / May, 1968 Accordingly, I shall try to go beyond just reporting what we have done and how, and I shall try to formulate as well what we have learned. I should like to end the introduction with two short remarks on working conditions, which I make for the sake of completeness. I shall not stress these points any further. One remark is that production speed is severely slowed down if one works with half-time people who have other obligations as well. This is at least a factor of four; prob- ably it is worse. The people themselves lose time and energy in switching over; the group as a whole loses de- cision speed as discussions, when needed, have often to be postponed until all people concerned are available. The other remark is that the members of the group (mostly mathematicians) have previously enjoyed as good students a university training of five to eight years and are of Master's or Ph.D. level. I mention this explicitly because at least it1 my country the intellectual level needed for system design is in general grossly underestimated. I am convinced more than ever that this type of work is very difficult, and that every effort to do it with other than the best people is doomed to either failure or moderate success at enormous expense. The Tool and the Goal The system has been designed for a Dutch machine, the EL X8 (N.V. Electrologica, Rijswijk (ZH)). Charac- teristics of our configuration are: (1) core memory cycle time 2.5usec, 27 bits; at present 32K; (2) drum of 512K words, 1024 words per track, rev. time 40msec; (3) an indirect addressing mechanism very well suited for stack implementation; (4) a sound system for commanding peripherals and controlling of interrupts; (5) a potentially great number of low capacity chan- nels; ten of them are used (3 paper tape readers at 1000char/see; 3 paper tape punches at 150char/ sec; 2 teleprinters; a plotter; a line printer); (6) absence of a number of not unusual, awkward features. The primary goal of the system is to process smoothly a continuous flow of user programs as a service to the Uni- versity. A multiprograrmning system has been chosen with the following objectives in mind: (1) a reduction of turn-around time for programs of short duration, (2) economic use of peripheral devices, (3) automatic control Communications of the ACM 341 of backing store to be combined with economic use of the central processor, and (4) the economic feasibility to use the machine for those applications for which only the flexi- bility of a general purpose computer is needed, but (as a rule) not the capacity nor the processing power. The system is not intended as a multiaeeess system. There is no common data base via which independent users can communicate with each other: they only share the configuration and a procedure library (that includes a translator for ALGOL 60 extended with complex numbers). The system does not eater for user programs written in machine language. Compared with larger efforts one can state that quanti- tatively spealdng the goals have been set as modest as the equipment and our other resources. Qualitatively speak- ing, I am afraid, we became more and more immodest as the work progressed. A Progress Report We have made some minor mistakes of the usual type (such as paying too much attention to eliminating what was not the real bottleneck) and two major ones. Our first major mistake was that for too long a time we confined our attention to % perfect installation"; by the time we considered howto make the best of it, one of the peripherals broke down, we were faced with nasty prob- lems. Taking care of the "pathology" took more energy than we had expected, and some of our troubles were a direct consequence of our earlier ingenuity, i.e. the com- plexity of the situation into which the system could have maneuvered itself. Had we paid attention to the pathology at an earlier stage of the design, our management rules would certainly have been less refined. The second major mistake has been that we conceived and programmed the major part of the system without giving more than scanty thought to the problem of de- bugging it. I must decline all credit for the fact that this mistake had no serious consequences--on the contrary! one might argue as an afterthought. As captain of the crew I had had extensive experience (dating back to 1958) in making basic software dealing with real-time interrupts, and I knew by bitter experience that as a result of the irreproducibility of the interrupt moments a program error could present itself misleadingly like an occasional machine malfunctioning. As a result I was terribly afraid. Having fears regarding the possibility of debugging, we decided to be as careful as possible and, prevention being better than cure, to try to prevent nasty bugs from entering the construction. This decision, inspired by fear, is at the bottom of what I regard as the group's main contribution to the art of system design. We have found that it is possible to design a refined multiprogramming system in such a way that its logical soundness can be proved a priori and its implemen- tation can admit exhaustive testing. The only errors that 342 Communications of the ACM showed up during testing were trivial coding errors (occurring with a density of one error per 500 instructions), each of them located within 10 minutes (classical) inspec- tion by the machine and each of them correspondingly easy to remedy. At the time this was written the testing had not yet been completed, but the resulting system is guaranteed to be flawless. When the system is delivered we shall not live in the perpetual fear that a system derail- ment may still occur in an unlikely situation, such as might result from an unhappy "coincidence" of two or more critical occurrences, for we shall have proved the eon'eetness of the system with a rigor and explicitness that is unusual for the great majority of mathematical proofs. A Survey of the System gtructure Storage Allocation. In the classical yon Neumann machine, information is identified by the address of the memory location containing the information. When we started to think about the automatic control of secondary storage we were familiar with a system (viz. GmR ALGOL) in which all information was identified by its drum address (as in the classical yon Neumann machine) and in which the function of the core memory was nothing more than to make the information "page-wise" accessible. We have followed another approach and, as it turned out, to great advantage. In our terminology we made a strict distinction between memory units (we called them "pages" and had "core pages" and "drum pages") and corresponding information units (for lack of a better word we called them "segments"), a segment just fitting in a page. For segments we created a completely independent identification mechanism in which the number of possible segment identifiers is much larger than the total number of pages in primary and secondary store. The segment iden- tifier gives fast access to a so-called "segment variable" in core whose value denotes whether the segment is still empty or not, and if not empty, in which page (or pages) it can be found. As a consequence of this approach, if a segment of in- formation, residing in a core page, has to be dumped onto the drum in order to make the core page available for other use, there is no need to return the segment to the same drum page from which it originally came. In fact, this freedom is exploited: among the free drum pages the one with minimum latency time is selected. A next consequence is the total absence of a drum allo- cation problem: there is not the slightest reason why, say, a program should occupy consecutive drum pages. In a multiprogramming environment this is very convenient. Processor Allocation. We have given full recognition to the fact that in a single sequential process (such as ca~ be performed by a sequential automaton) only the time succession of the various states has a logical meaning, but not the actual speed with which the sequential process is Volume 11 / Number 5 / May, 1968 performed. Therefore we have arranged the whole system as a society of sequential processes, progressing with un- defined speed ratios. To each user program accepted by the system corresponds a sequential process, to each input peripheral corresponds a sequential process (buffering input streams in synchronism with the execution of the input commands), to each output peripheral corresponds a sequential process (unbuffering output streams in syn- chronism with the execution of the output commands); furthermore, we have the "segment controller" associated with the drum and the "message interpreter" associated with the console keyboard. This enabled us to design the whole system in terms of these abstract "sequential processes." Their harmonious cooperation is regulated by means of explicit mutuM synchronization statements. On the one hand, this ex- plicit mutual synchronization is necessary, as we do not make any assumption about speed ratios; on the other hand, this mutual synchronization is possible because "delaying the progress of a process temporarily" can never be harmful to the interior logic of the process delayed. The fundamental consequence of this approaeh--viz, the ex- plicit mutual synchronization--is that the harmonious cooperation of a set of such sequential processes can be established by discrete reasoning; as a further consequence the whole harmonious society of cooperating sequential processes is independent of the actual number of processors available to carry out these processes, provided the proces- sors available can switch from process to process. System Hierarchy. The total system admits a strict hierarchical structure. At level 0 we find the responsibility for processor allo- cation to one of the processes whose dynamic progress is logically permissible (i.e. in view of the explicit mutual synchronization). At this level the interrupt of the real- time clock is processed and introduced to prevent any process to monopolize processing power. At this level a priority rule is incorporated to achieve quick response of the system where this is needed. Our first abstraction has been achieved; above level 0 the number of processors actually shared is no longer relevant. At higher levels we find the activity of the different sequential processes, the actual processor that had lost its identity having disap- peared from the picture. At level 1 we have the so-called "segment controller," a sequential process synchronized with respect to the drum interrupt and the sequential processes on higher levels. At level 1 we find the responsibility to cater to the book- keeping resulting from the automatic backing store. At this level our next abstraction has been achieved; at all higher levels identification of information takes place in terms of segments, the actual storage pages that had lost their identity having disappeared from the picture. At level 2 we find the "message interpreter" taking care of the allocation of the console keyboard via which con- versations between the operator and any of the higher level processes can be carried out. The message interpreter works in close synchronism with the operator. When the operator presses a key, a character is sent to the machine together with an interrupt signal to announce the next keyboard character, whereas the actual printing is done through an output command generated by the machine under control of the message interpreter. (As far as the hardware is concerned the console teleprinter is regarded as two independent peripherals: an input keyboard and an output printer.) If one of the processes opens a conversa- tion, it identifies itself in the opening sentence of the con- versation for the benefit of the operator. If, however, the operator opens a conversation, he must identify the process he is addressing, in the opening sentence of the conversation, i.e. this opening sentence must be inter- preted before it is known to which of the processes the conversation is addressed! Here lies the logical reason for the introduction of a separate sequential process for the console teleprinter, a reason that is reflected in its name, "message interpreter." Above level 2 it is as if each process had its private con- versational console. The fact that they share the same physical console is translated into a resource restriction of the form "only one conversation at a time," a restriction that is satisfied via mutual synchronization. At this level the next abstraction has been implemented; at higher levels the actual console teleprinter loses its identity. (If the message interpreter had not been on a higher level than the segment controller, then the only way to imple- ment it would have been to make a permanent reservation in core for it; as the conversational vocabulary might be- come large (as soon as our operators wish to be addressed in fancy messages), this would result in too heavy a per- manent demand upon core storage. Therefore, the vo- cabulary in which the messages are expressed is stored on segments, i.e. as information units that can reside on the drum as well. For this reason the message interpreter is one level higher than the segment controller.) At level 3 we find the sequential processes associated with buffering of input streams and unbuffering of out- put streams. At this level the next abstraction is effeeted, viz. the abstraction of the actual peripherals used that are allocated at this level to the "logical communication units" in terms of which are worked in the still higher levels. The sequential processes associated with the peripherals are of a level above the message interpreter, because they must be able to converse with the operator (e.g. in the case of detected malfunctioning). The limited number of periph- erals again acts as a resource restriction for the processes at higher levels to be satisfied by mutual synchronizatioI~ between them. At level 4 we find the independent:user programs and at level 5 the operator (not implemented by us). The system structure has been described at length in order to make the next section intelligible. Volume 11 / Number 5 / May, 1968 Communications of the ACM 343 Design Experience  The conception stage took a long time. During that period of time the concepts have been born in terms of which we sketched the system in the previous section. Furthermore, we learned the art of reasoning by which we could deduce from our requirements the way in which the processes should influence each other by their inutual synchronization so that these requirements would be met. (The requirements being that no information can be used before it has been produced, that no peripheral can be set to two tasks simultaneously, etc.). Finally we learned the art of reasoning by which we could prove that the society composed of processes thus mutually synchronized by each other would indeed in its time behavior satisfy all requirements. The construction stage has been rather traditional, perhaps even old-fashioned, that is, plain machine code. Reprogramming on account of a change of specifications has been rare, a circumstance that must have contributed greatly to the feasibility of the "steam method." That the first two stages took more time than planned was some- what compensated by a delay in the delivery of the machine. In the verification stage we had the machine, during short shots, completely at our disposal; these were shots during which we worked with a virgin machine without any software aids for debugging. Starting at level 0 the system was tested, each time adding (a portion of) the next level only after the previous level had been thoroughly tested. Each test shot itself contained, on top of the (par- tial) system to be tested, a number of testing processes with a double function. First, they had to force the system into all different relevant states; second, they had to verify that the system continued to react according to specifica- tion. I shall not deny that the construction of these testing programs has been a major intellectual effort: to convince oneself that one has not overlooked "a relevant state" and to convince oneself that the testing programs generate them all is no simple matter. The encouraging thing is that (as far as we know!) it could be done. This fact was one of the happy consequences of the hierarchical structure. Testing level 0 (the real-time clock and processor allo- cation) implied a number of testing sequential processes on top of it, inspecting together that under all circum- stances processor time was divided among them accord- ing to the rules. This being established, sequential processes as such were implemented. Testing the segment controller at level 1 meant that all "relevant states" could be formulated in terms of se- quential processes making (in various combinations) demands on core pages, situations that could be provoked by explicit synchronization among the testing programs. At this stage the existence of the real-time clock--al- though interrupting all the time--was so immaterial that one of the testers indeed forgot its existence! By that time we had implemented the correct reaction upon the (mutually unsynchronized) interrupts from the reaI-time clock and the drum. If we ihad not introduced the separate levels 0 and 1, and if we had not created a ternfinology (viz. that of the rather abstract sequential processes) in which the existence of the clock interrupt could be discarded, but had instead tried in a nonhierar- ehieal construction, to make the central processor react directly upon any weird time succession of these two interrupts, the number of "relevant states" would have exploded to sueh a height that exhaustive testing would have been an illusion. (Apart from that it is doubtful whether we would have had the means to generate them all, drum and clock speed being outside our control.) For the sake of completeness I must mention a further happy consequence. As Stated before, above level 1, core and drum pages have lost their identity, and buffering of input and output streams (at level 3) therefore occurs in terms of segments. While testing at level 2 or 3 the drum channel hardware broke down for some time, but t~sting proceeded by restricting the number of segments to the number that could be held in core. If building up the line printer output streams had been implemented as "dump- ing onto the drum" and the actual printing as "printing from the drum," this advantage would have been denied to us. Conclusion As far as program verification is concerned I present nothing essentially new. In testing a general purpose object (be it a piece of hardware, a program, a machine, or a system), one cannot subject it to all possible cases: for a computer this would imply that one feeds it with all possible programs! Therefore one must test it with a set of relevant test cases. What is, or is not, relevant cannot be decided as long as one regards the mechanism as a black box; in other words, the decision has to be based upon the internal structure of the mechanism to be tested. It seems to be the designer's responsibility to construct his mecha- nism in such a way--i.e, so effectively structured--that at each stage of the testing procedure the number of rele- vant test cases will be so small that he can try them all and that what is being tested will be so perspicuous that he will not have overlooked any situation. I have presented a survey of our system because I think it a nice example of the form that such a structure might take. In my experience, I am sorry to say, industrial software makers tend to react to the system with mixed feelings. On the one hand, they are inclined to think that we have done a kind of model job; on the other hand, they express doubts whether the techniques used are applicable outside the sheltered atmosphere of a University and express the opinion that we were successful only because of the modest scope of the whole project. It is not my intention to under- estimate the organizing ability needed to handle a much bigger job, with a lot more people, but I should like to ven- 344 Communications of the ACM Volume 11 / Number 5 / May, 1968 ture the opinion that the larger the project, the more essen- tial the structuring! A hierarchy of five logical levels might then very well turn out to be of modest depth, especially when one designs the system more consciously than we have done, with the aim that the software can be smoothly adapted to (perhaps drastic) configuration ex- pansions. Acknowle@ments. I express my indebtedness to my five collaborators, C. Bron, A. N. Habermann, F. J. A. Hendriks, C. Ligtmans, and P. A. Voorhoeve. They have contributed to all stages of the design, and together we learned the art of reasoning needed. The construction and verification was entirely their effort; if my dreams have come true, it is due to their faith, their talents, and their persistent loyalty to the whole project. Finally I should like to thank the members of the pro- gram committee, who asked for more information on the synchronizing primitives and some justification of my claim to be able to prove logical soundness a priori. In answer to this request an appendix has been added, which I hope will give the desired information and justification. APPENDIX Synchronizing Primitives Explicit mutual synchronization of parallel sequential processes is implemented via so-called "semaphores." They are special purpose integer variables allocated in the universe in which the processes are embedded; they are initialized (with the value 0 or 1) before the parallel proc- esses themselves are started. After this initialization the parallel processes will access the semaphores only via two very specific operations, the so-called synchronizing primi- tives. For historical reasons they are called the P-opera- tion and the V-operation. A process, "Q" say, that performs the operation "P (sem)" decreases the value of the semaphore called "sem" by 1. If the resulting value of the semaphore concerned is nonnegative, process Q can continue with the execution of its next statement; if, however, the resulting value is negative, process Q is stopped and booked on a waiting list associated with the semaphore concerned. Until fur- ther notice (i.e. a V-operation on this very same sema- phore), dynamic progress of process Q is not logically permissible and no processor will be allocated to it (see above "System Hierarchy," at level 0). A process, "R" say, that performs the operation "V (sem)" increases the value of the semaphore called "sem" by 1. If the resulting value of the semaphore concerned is positive, the V-operation in question has no further effect; if, however, the resulting value of the semaphore concerned is nonpositive, one of the processes booked on its waiting list is removed from this waiting list, i.e. its dynamic progress is again logically permissible and in due time a processor will be allocated to it (again, see above "System Hierarchy," at level 0). COROLLARY 1. If a semaphore value is nonpositive its absolute value equals the number of processes booked on its waiting list. COROLLARY 2. The P-operation represents the potential delay, the complementary V-operation represents the re- moval of a barrier. Note 1. P- and V-operations are "indivisible actions"; Volume 11 / Number 5 / May, 1968 i.e. if they occur "simultaneously" in parallel processes they are noninterfcring in the sense that they can be re- garded as being performed one after the other. Note 2. If the semaphore value resulting from a V- operation is negative, its waiting list originally contained more than one process. It is undefined--i.e, logically im- material-which of the waiting processes is then removed from the waiting list. Note 3. A consequence of the mechanisms described above is that a process whose dynamic progress is permis- sible can only loose this status by actually progressing, i.e. by performance of a P-operation on a semaphore with a value that is initially nonpositive. During system conception it transpired that we used the semaphores in two completely different ways. The difference is so marked that, looking back, one wonders whether it was really fair to present the two ways as uses of the very same primitives. On the one hand, we have the semaphores used for mutual exclusion, on the other hand, the private semaphores. Mutual Exclusion In the following program we indicate two parallel, cyclic processes (between the brackets "parbegin" and "par- end") that come into action after the surrounding uni- verse has been introduced and inigiahzed. begin semaphore mutex; mutex := 1; parbegin begin L1 : P (mutex ) ; critical section 1; remainder of cycle 1; go to L1 end; begin L2: P mutex); critical section 2; V (mutex); remainder of cycle 2; go to L2 end pareud end V (mutex) ; As a result of the P- and V-operations on "mutex" the actions, marked as "critical sections" exclude e~ch other mutually in time; the scheme given allows straight- forward extension to more than two parallel processes, Communications of the ACM 345 the maxinnun value of mutex equals l, the minimum value equals -(n - 1) if we have n parallel processes. Critical sections are used always, ~md only for the pur- pose of unambiguous inspection and modification of the state variables (allocated in the surrounding universe) that describe the current state of the system (as far as needed for the regulation of the ham~onious cooperation between the various processes). Private Semaphores Each sequential process has associated with it a num- ber of private semaphores and no other process will ever perform a P-operation on them. The universe initializes them with the value equal to 0, their maximum value equals 1, and their minhnum value equals -1. Whenever a process reaches a stage where the pemfis- sion for dynamic progress depends on current values of state variables, it follows the pattern: P(mutex) ; "inspection and modification of state variables including a conditional V(private semaphore)"; V (mutex) ; P(private semaphore). If the inspection learns that the process in question should eontinne, it performs the operation "V (private semaphore) "--the semaphore value then changes from 0 to 1--other~4se, this V-operation is skipped, leaving to the other processes the obligation to perform this V- operation at a suitable moment. The absence or presence of this obligation is reflected in the finM values of the state variables upon leaving the critical section. Whenever a process reaches a stage where as a result of its progress possibly one (or more) blocked processes should now get permission to continue, it follows the pat- tern: P (mutex) ; "modification and inspection of state variables includ- ing zero or more V-operations on private semaphores of other processes"; V(mutex). By the introduction of suitable state variables and appropriate programming of the critical sections any strategy assigning peripherals, buffer areas, etc. can be implemented. The amount of coding and reasoning can be greatly reduced by the observation that in the two complemen- tary critical sections sketched above the same inspection can be performed by the introduction of the notion of "an tlnstabIe si/;uation," such as a free resider and a process needing a. reader. Whenever ~m unstable situation emerges it is removed (including one or more g-operations on private semaphores) in the very same critical section in which i{; has been created. Proving the ttarmonious Cooperation The sequential processes in the system east all be re~ garded as cyclic processes in which a certain neutral point can be marked, the so-called "homing position," in which all processes are when the system is at rest,. When a cyclic process leaves its homing position "it accepts a task"; when the task has been performed and not earlier, the process returns to its homing position. Each eyelie process has a specific task processing power (e.g. the execution of a user program or unbuffering a portion of printer output, etc.). The harmonious cooperation is mainly proved in roughly three stages. (1) It is proved that although a process performing a task may in so doing generate a finite number of tasks for other processes, a single initial task cannot give rise to an infinite number of task generations. The proof is simple as processes can only generate tasks for processes at lower levels of the hierarchy so that circularity is excluded. (If a process needing a segment from the drum has gener- ated a task for the segment controller, special precautions have been taken to ensure that the segment asked for remains in core at least until the requesting process has effectively accessed the segment concerned. Without this precaution finite tasks could be forced ~o generate an infinite number of tasks for the segment controller, and the system could get stuck in an unproductive page flutter.) (2) It is proved that it is impossible that all processes have returned to their honfing position while somewhere in the system there is still pending a generated but unae- eepted task. (This is proved via instability of the situation just described.) (3) It is proved that after the acceptance of an initial task all processes eventually will be (again) in their hom- ing position. Each process blocked in the course of task execution relies on the other processes for removal of the barrier. Essentially, the proof in question is a demon- stration of the absence of "circular waits": process P waiting for process Q waiting for process R waiting for process P. (Our usual term for the circular wait is "the Deadly Embrace.") In a more general society than our system this proof turned out to be a proof by induction (on the level of hierarchy, starting at the lowest level), as A. N. Habermann has shown in his doctoral thesis. i l,i N 346 Communications of the ACM Volume 11 / Nu~tber 5 / May, 1968 The UNIX Time- Sharing System Dennis M. Ritchie and Ken Thompson Bell Laboratories UNIX is a general-purpose, multi-user, interactive operating system for the Digital Equipment Corporation PDP-11/40 and 11/45 computers. It offers a number of features seldom found even in larger operating systems, including: (1) a hierarchical file system incorporating demountable volumes; (2) compatible file, device, and inter-process I/O; (3) the ability to initiate asynchronous processes; (4) system command language selectable on a per-user basis; and (5) over 100 subsystems including a dozen languages. This paper discusses the nature and implementation of the file system and of the user command interface. Key Words and Phrases: time-sharing, operating system, file system, command language, PDP-11 CR Categories: 4.30, 4.32 1. Introduction There have been three versions of UNIX. The earliest version (circa 196%70) ran on the Digital Equipment Corporation PDP-7 and -9 computers. The second ver- sion ran on the unprotected PDP-11/20 computer. This paper describes only the PDP-I 1/40 and /45 [1] system since it is more modern and many of the differences between it and older UNIX systems result from redesign of features found to be deficient or lacking. Since PDP-11 UNIX became operational in February 1971, about 40 installations have been put into service; they are generally smaller than the system described here. Most of them are engaged in applications such as the preparation and formatting of patent applications and other textual material, the collection and processing of trouble data from various switching machines within the Bell System, and recording and checking telephone service orders. Our own installation is used mainly for research in operating systems, languages, com- puter networks, and other topics in computer science, and also for document preparation. Perhaps the most important achievement of UNIX is to demonstrate that a powerful operating system for interactive use need not be expensive either in equipment or in human effort: UNIX can run on hardware costing as little as $40,000, and less than two man- years were spent on the main system software. Yet UNIX contains a number of features seldom offered even in much larger systems. It is hoped, however, the users of UNIX will find that the most important characteristics of the system are its simplicity, elegance, and ease of use. Besides the system proper, the major programs available under UNIX are: assembler, text editor based on QED [2], linking loader, symbolic debugger, compiler for a language resembling BCPL [3] with types and structures (C), interpreter for a dialect of BASIC, text formatting program, Fortran compiler, Snobol inter- preter, top-down compiler-compiler (TMC) [4], bot- tom-up compiler-compiler (YACC), form letter generator, macro processor (M6) [5], and permuted index program. There is also a host of maintenance, utility, recrea- tion, and novelty programs. All of these programs were written locally. It is worth noting that the system is totally self-supporting. All UNIX software is maintained under UNIX; likewise, UNIX documents are generated and formatted by the UNIX editor and text formatting program. Copyright  1974, Association for Computing Machinery, Inc. General permission to republish, but not for profit, all or part of this material is granted provided that ACM's copyright notice is given and that reference is made to the publication, to its date of issue, and to the fact that reprinting privileges were granted by permission of the Association for Computing Machinery. This is a revised version of a paper presented at the Fourth ACM Symposium on Operating Systems Principles, IBM Thomas J. Watson Research Center, Yorktown Heights, New York, October 15-17, 1973. Authors' address: Bell Laboratories, Murray Hill, NJ 07974. 365 2. Hardware and Software Environment The PDP-11/45 on which our UNIX installation is implemented is a 16-bit word (8-bit byte) computer with 144K bytes of core memory; UNIX occupies 42K bytes. This system, however, includes a very large number of device drivers and enjoys a generous allotment of space for I/O buffers and system tables; a minimal system Communications July 1974 of Volume 17 the ACM Number 7 capable of running the software mentioned above can require as little as 50K bytes of core altogether. The vDv-11 has a 1M byte fixed-head disk, used for file system storage and swapping, four moving-head disk drives which each provide 2.5M bytes on removable disk cartridges, and a single moving-head disk drive which uses removable 40M byte disk packs. There are also a high-speed paper tape reader-punch, nine-track magnetic tape, and DEctape (a variety of magnetic tape facility in which individual records may be ad- dressed and rewritten). Besides the console typewriter, there are 14 variable-speed communications interfaces attached to 100-series datasets and a 201 dataset in- terface used primarily for spooling printout to a com- munal line printer. There are also several one-of-a-kind devices including a Picturephone  interface, a voice response unit, a voice synthesizer, a phototypesetter, a digital switching network, and a satellite PDP-11/20 which generates vectors, curves, and characters on a Tektronix 611 storage-tube display. The greater part of UNIX software is written in the above-mentioned C language [6]. Early versions of the operating system were written in assembly language, but during the summer of 1973, it was rewritten in C. The size of the new system is about one third greater than the old. Since the new system is not only much easier to understand and to modify but also includes many functional improvements, including multipro- gramming and the ability to share reentrant code among several user programs, we considered this in- crease in size quite acceptable. 3. The File System The most important role of UNIX is to provide a file system. From the point of view of the user, there are three kinds of files: ordinary disk files, directories, and special files. 3.1 Ordinary Files A file contains whatever information the user places on it, for example symbolic or binary (object) programs. No particular structuring is expected by the system. Files of text consist simply of a string of characters, with lines demarcated by the new-line character. Binary programs are sequences of words as they will appear in core memory when the program starts executing. A few user programs manipulate files with more structure: the assembler generates and the loader expects an object file in a particular format. However, the structure of files is controlled by the programs which use them, not by the system. 3.2 Directories Directories provide the mapping between the names of files and the files themselves, and thus induce a structure on the file system as a whole. Each user has a directory of his own files; he may also create subdirec- tories to contain groups of files conveniently treated together. A directory behaves exactly like an ordinary file except that it cannot be written on by unprivileged programs, so that the system controls the contents of directories. However, anyone with appropriate per- mission may read a directory just like any other file. The system maintains several directories for its own use. One of these is the root directory. All files in the system can be found by tracing a path through a chain of directories until the desired file is reached. The starting point for such searches is often the root. Another system directory contains all the programs provided for general use; that is, all the commands. As will be seen, however, it is by no means necessary that a program reside in this directory for it to be executed. Files are named by sequences of 14 or fewer characters. When the name of a file is specified to the system, it may be in the form of a path name, which is a sequence of directory names separated by slashes "/" and ending in a file name. If the sequence begins with a slash, the search begins in the root directory. The name /alpha/beta/gamma causes the system to search the root for directory alpha, then to search alpha for beta, finally to find gamma in beta. Gamma may be an ordinary file, a directory, or a special file. As a limiting case, the name "/" refers to the root itself. A path name not starting with "/" causes the sys- tem to begin the search in the user's current directory. Thus, the name alpha/beta specifies the file named beta in subdirectory alpha of the current directory. The simplest kind of name, for example alpha, refers to a file which itself is found in the current directory. As another limiting case, the null file name refers to the current directory. The same nondirectory file may appear in several directories under possibly different names. This feature is called/inking; a directory entry for a file is sometimes called a link. UNIX differs from other systems in which linking is permitted in that all links to a file have equal status. That is, a file does not exist within a particular directory; the directory entry for a file consists merely of its name and a pointer to the information actually describing the file. Thus a file exists independently of any directory entry, although in practice a file is made to disappear along with the last link to it. Each directory always has at least two entries. The name .... in each directory refers to the directory itself. Thus a program may read the current directory under the name "." without knowing its complete path name. The name ".." by convention refers to the parent of the directory in which it appears, that is, to the directory in which it was created. The directory structure is constrained to have the form of a rooted tree. Except for the special entries " " and "..", each directory must appear as an entry in exactly one other, which is its parent. The reason for this is to simplify the writing of programs which 366 Communications July 1974 of Volume 17 the ACM Number 7 visit subtrees of the directory structure, and more im- portant, to avoid the separation of portions of the hierarchy. If arbitrary links to directories were per- mitted, it would be quite difficult to detect when the last connection from the root to a directory was severed. keeping which would otherwise be required to assure removal of the links when the removable volume is finally dismounted. In particular, in the root directories of all file systems, removable or not, the name ".." refers to the directory itself instead of to its parent. 3.3 Special Files Special files constitute the most unusual feature of the UNIX file system. Each I/O device supported by UNIX is associated with at least one such file. Special files are read and written just like ordinary disk files, but requests to read or write result in activation of the associated device. An entry for each special file resides in directory /dev, although a link may be made to one of these files just like an ordinary file. Thus, for example, to punch paper tape, one may write on the file/dev/ppt. Special files exist for each communication line, each disk, each tape drive, and for physical core memory. Of course, the active disks and the core special file are protected from indiscriminate access. There is a threefold advantage in treating I/O devices this way: file and device I/o are as similar as possible; file and device names have the same syntax and mean- ing, so that a program expecting a file name as a param- eter can be passed a device name; finally, special files are subject to the same protection mechanism as regular files. 3.4 Removable File Systems Although the root of the file system is always stored on the same device, it is not necessary that the entire file system hierarchy reside on this device. There is a mount system request which has two arguments: the name of an existing ordinary file, and the name of a direct-access special file whose associated storage vol- ume (e.g. disk pack) should have the structure of an independent file system containing its own directory hierarchy. The effect of mount is to cause references to the heretofore ordinary file to refer instead to the root directory of the file system on the removable volume. In effect, mount replaces a leaf of the hierarchy tree (the ordinary file) by a whole new subtree (the hierarchy stored on the removable volume). After the mount, there is virtually no distinction between files on the removable volume and those in the permanent file system. In our installation, for example, the root directory resides on the fixed-head disk, and the large disk drive, which contains user's files, is mounted by the system initialization program; the four smaller disk drives are available to users for mounting their own disk packs. A mountable file system is generated by writing on its corresponding special file. A utility pro- gram is available to create an empty file system, or one may simply copy an existing file system. There is only one exception to the rule of identical treatment of files on different devices: no link may exist between one file system hierarchy and another. This restriction is enforced so as to avoid the elaborate book- 3.5 Protection Although the access control scheme in UNIX is quite simple, it has some unusual features. Each user of the system is assigned a unique user identification number. When a file is created, it is marked with the user ID of its owner. Also given for new files is a set of seven protection bits. Six of these specify independently read, write, and execute permission for the owner of the file and for all other users. If the seventh bit is on, the system will temporarily change the user identification of the current user to that of the creator of the file whenever the file is executed as a program. This change in user ID is effective only during the execution of the program which calls for it. The set-user-ID feature provides for privileged pro- grams which may use files inaccessible to other users. For example, a program may keep an accounting file which should neither be read nor changed except by the program itself. If the set-user-identification bit is on for the program, it may access the file although this access might be forbidden to other programs invoked by the given program's user. Since the actual user ID of the invoker of any program is always available, set- user-Io programs may take any measures desired to satisfy themselves as to their invoker's credentials. This mechanism is used to allow users to execute the care- fully written commands which call privileged system entries. For example, there is a system entry invokable only by the "super-user" (below) which creates an empty directory. As indicated above, directories are expected to have entries for "." and "..". The com- mand which creates a directory is owned by the super- user and has the set-user-ID bit set. After it checks its invoker's authorization to create the specified directory, it creates it and makes the entries for " " and "..". Since anyone may set the set-user-ID bit on one of his own files, this mechanism is generally available with- out administrative intervention. For example, this pro- tection scheme easily solves the MOO accounting prob- lem posed in [7]. The system recognizes one particular user ID (that of the "super-user") as exempt from the usual constraints on file access; thus (for example) programs may be written to dump and reload the file system without un- wanted interference from the protection system. 3.6 I/o Calls The system calls to do I/o are designed to eliminate the differences between the various devices and styles of access. There is no distinction between "random" and "sequential" I/O, nor is any logical record size imposed by the system. The size of an ordinary file is determined 367 Communications July 1974 of Volume 17 the ACM Number 7 by the highest byte written on it; no predetermination of the size of a file is necessary or possible. To illustrate the essentials of I/O in UNIX, some of the basic calls are summarized below in an anonymous language which will indicate the required parameters without getting into the complexities of machine language programming. Each call to the system may potentially result in an error return, which for sim- plicity is not represented in the calling sequence. To read or write a file assumed to exist already, it must be opened by the following call: filep = open (name, flag) Name indicates the name of the file. An arbitrary path name may be given. The flag argument indicates whether the file is to be read, written, or "updated," that is read and written simultaneously. The returned value filep is called a file descriptor. It is a small integer used to identify the file in sub- sequent calls to read, write, or otherwise manipulate it. To create a new file or completely rewrite an old one, there is a create system call which creates the given file if it does not exist, or truncates it to zero length if it does exist. Create also opens the new file for writing and, like open, returns a file descriptor. There are no user-visible locks in the file system, nor is there any restriction on the number of users who may have a file open for reading or writing. Although it is possible for the contents of a file to become scrambled when two users write on it simultaneously, in practice, difficulties do not arise. We take the view that locks are neither necessary nor sufficient, in our environment, to prevent interference between users of the same file. They are unnecessary because we are not faced with large, single-file data bases maintained by independent processes. They are insufficient because locks in the ordinary sense, whereby one user is pre- vented from writing on a file which another user is reading, cannot prevent confusion when, for example, both users are editing a file with an editor which makes a copy of the file being edited. It should be said that the system has sufficient internal interlocks to maintain the logical consistency of the file system when two users engage simultaneously in such inconvenient activities as writing on the same file, creating files in the same directory, or deleting each other's open files. Except as indicated below, reading and writing are sequential. This means that if a particular byte in the file was the last byte written (or read), the next I/o call implicitly refers to the first following byte. For each open file there is a pointer, maintained by the system, which indicates the next byte to be read or written. If n bytes are read or written, the pointer advances by n bytes. Once a file is open, the following calls may be used: n = read(filep, buffer, count) n = write(filep, buffer, count) Up to count bytes are transmitted between the file specified by filep and the byte array specified by buffer. The returned value n is the number of bytes actually transmitted. In the write case, n is the same as count except under exceptional conditions like I/O errors or end of physical medium on special files; in a read, however, n may without error be less than count. If the read pointer is so near the end of the file that reading count characters would cause reading beyond the end, only sufficient bytes are transmitted to reach the end of the file; also, typewriter-like devices never return more than one line of input. When a read call returns with n equal to zero, it indicates the end of the file. For disk files this occurs when the read pointer becomes equal to the current size of the file. It is possible to generate an end-of-file from a typewriter by use of an escape sequence which depends on the device used. Bytes written on a file affect only those implied by the position of the write pointer and the count; no other part of the file is changed. If the last byte lies beyond the end of the file, the file is grown as needed. To do random (direct access) I/O, it is only necessary to move the read or write pointer to the appropriate location in the file. location = seek(filep, base, offset) The pointer associated with filep is moved to a position offset bytes from the beginning of the file, from the current position of the pointer, or from the end of the file, depending on base. Offset may be negative. For some devices (e.g. paper tape and typewriters) seek calls are ignored. The actual offset from the beginning of the file to which the pointer was moved is returned in location. 3.6.1 Other I/O Calls. There are several additional system entries having to do with I/o and with the file system which will not be discussed. For example: close a file, get the status of a file, change the protec- tion mode or the owner of a file, create a directory, make a link to an existing file, delete a file. 4. Implementation of the File System As mentioned in 3.2 above, a directory entry con- tains only a name for the associated file and a pointer to the file itself. This pointer is an integer called the i-number (for index number) of the file. When the file is accessed, its i-number is used as an index into a system table (the i-list) stored in a known part of the device on which the directory resides. The entry thereby found (the file's i-node) contains the description of the file as follows. 1. Its owner. 2. Its protection bits. 3. The physical disk or tape addresses for the file contents. 4. Its size. 368 Communications July 1974 of Volume 17 the ACM Number 7 5. Time of last modification. 6. The number of links to the file, that is, the number of times it appears in a directory. 7. A bit indicating whether the file is a directory. 8. A bit indicating whether the file is a special file. 9. A bit indicating whether the file is "large" or "small." The purpose of an open or create system call is to tam the path name given by the user into an i-number by searching the explicitly or implicitly named directories. Once a file is open, its device, i-number, and read/write pointer are stored in a system table indexed by the file descriptor returned by the open or create. Thus the file descriptor supplied during a subsequent call to read or write the file may be easily related to the in- formation necessary to access the file. When a new file is created, an i-node is allocated for it and a directory entry is made which contains the name of the file and the i-node number. Making a link to an existing file involves creating a directory entry with the new name, copying the i-number from the original file entry, and incrementing the link-count field of the i-node. Removing (deleting) a file is done by decrementing the link-count of the i-node specified by its directory entry and erasing the directory entry. If the link-count drops to 0, any disk blocks in the file are freed and the i-node is deallocate& The space on all fixed or removable disks which contain a file system is divided into a number of 512- byte blocks logically addressed from 0 up to a limit which depends on the device. There is space in the i-node of each file for eight device addresses. A small (nonspecial) file fits into eight or fewer blocks; in this case the addresses of the blocks themselves are stored. For large (nonspecial) files, each of the eight device addresses may point to an indirect block of 256 addresses of blocks constituting the file itself. Thus files may be as large as 8.256-512, or 1,048,576 (22o ) bytes. The foregoing discussion applies to ordinary files. When an I/O request is made to a file whose i-node indicates that it is special, the last seven device address words are immaterial, and the first is interpreted as a pair of bytes which constitute an internal device name. These bytes specify respectively a device type and sub- device number. The device type indicates which system routine will deal with I/o on that device; the subdevice number selects, for example, a disk drive attached to a particular controller or one of several similar type- writer interfaces. In this environment, the implementation of the mount system call (3.4) is quite straightforward. Mount maintains a system table whose argument is the i-num- ber and device name of the ordinary file specified during the mount, and whose corresponding value is the device name of the indicated special file. This table is searched for each (i-number, device)-pair which turns up while a path name is being scanned during an open or create; if a match is found, the i-number is replaced by 1 (which is the i-number of the root directory on all file systems), and the device name is replaced by the table value. To the user, both reading and writing of files appear to be synchronous and unbuffered. That is, immediately after return from a read call the data are available, and conversely after a write the user's workspace may be reused. In fact the system maintains a rather complicated buffering mechanism which reduces greatly the number of I/O operations required to access a file. Suppose a write call is made specifying transmission of a single byte. UNiX will search its buffers to see whether the affected disk block currently resides in core memory; if not, it will be read in from the device. Then the affected byte is replaced in the buffer, and an entry is made in a list of blocks to be written. The return from the write call may then take place, although the actual I/O may not be completed until a later time. Con- versely, if a single byte is read, the system deter- mines whether the secondary storage block in which the byte is located is already in one of the system's buffers; if so, the byte can be returned immediately. If not, the block is read into a buffer and the byte picked out. A program which reads or writes files in units of 512 bytes has an advantage over a program which reads or writes a single byte at a time, but the gain is not immense; it comes mainly from the avoidance of system overhead. A program which is used rarely or which does no great volume of I/O may quite reasonably read and write in units as small as it wishes. The notion of the i-list is an unusual feature of UNIX. In practice, this method of organizing the file system has proved quite reliable and easy to deal with. To the system itself, one of its strengths is the fact that each file has a short, unambiguous name which is related in a simple way to the protection, addressing, and other information needed to access the file. It also permits a quite simple and rapid algorithm for checking the consistency of a file system, for example verifica- tion that the portions of each device containing useful information and those free to be allocated are disjoint and together exhaust the space on the device. This algorithm is independent of the directory hierarchy, since it need only scan the linearly-organized i-list. At the same time the notion of the i-list induces certain peculiarities not found in other file system organiza- tions. For example, there is the question of who is to be charged for the space a file occupies, since all directory entries for a file have equal status. Charging the owner of a file is unfair, in general, since one user may create a file, another may link to it, and the first user may delete the file. The first user is still the owner of the file, but it should be charged to the second user. The simplest reasonably fair algorithm seems to be to spread the charges equally among users who have links to a file. The current version of UNIX avoids the issue by not charging any fees at all. 369 Communications July 1974 of Volume 17 the ACM Number 7 4.1 Efficiency of the File System To provide an indication of the overall efficiency of UNIX and of the file system in particular, timings were made of the assembly of a 7621-1ine program. The assembly was run alone on the machine; the total clock time was 35.9 sec, for a rate of 212 lines per sec. The time was divided as follows: 63.5 percent assembler execution time, 16.5 percent system overhead, 20.0 percent disk wait time. We will not attempt any interpre- tation of these figures nor any comparison with other systems, but merely note that we are generally satisfied with the overall performance of the system. 5. Processes and Images An image is a computer execution environment. It includes a core image, general register values, status of open files, current directory, and the like. An image is the current state of a pseudo computer. A process is the execution of an image. While the processor is executing on behalf of a process, the image must reside in core; during the execution of other pro- cesses it remains in core unless the appearance of an active, higher-priority process forces it to be swapped out to the fixed-head disk. The user-core part of an image is divided into three logical segments. The program text segment begins at location 0 in the virtual address space. During execution, this segment is write-protected and a single copy of it is shared among all processes executing the same pro- gram. At the first 8K byte boundary above the program text segment in the virtual address space begins a non- shared, writable data segment, the size of which may be extended by a system call. Starting at the highest address in the virtual address space is a stack segment, which automatically grows downward as the hardware's stack pointer fluctuates. 5.1 Processes Except while UNIX is bootstrapping itself into opera- tion, a new process can come into existence only by use of the fork system call: processid = fork(label) When fork is executed by a process, it splits into two independently executing processes. The two processes have independent copies of the original core image, and share any open files. The new processes differ only in that one is considered the parent process: in the parent, control returns directly from the fork, while in the child, control is passed to location label. The processid returned by the fork call is the identification of the other process. Because the return points in the parent and child process are not the same, each image existing after a fork may determine whether it is the parent or child process. 370 5.2 Pipes Processes may communicate with related processes using the same system read and write calls that are used for file system I/O. The call filep = pipe( ) returns a file descriptor filep and creates an interprocess channel called a pipe. This channel, like other open files, is passed from parent to child process in the image by the fork call. A read using a pipe file descriptor waits until another process writes using the file descriptor for the same pipe. At this point, data are passed between the images of the two processes. Neither process need know that a pipe, rather than an ordinary file, is in- volved. Although interprocess communication via pipes is a quite valuable tool (see 6.2), it is not a completely general mechanism since the pipe must be set up by a common ancestor of the processes involved. 5.3 Execution of Programs Another major system primitive is invoked by execute(file, args, argo, ..., arg,+) which requests the system to read in and execute the program named by file, passing it string arguments argl, arg..,, ..., arg,+. Ordinarily, argl should be the same string as file, so that the program may determine the name by. which it was invoked. All the code and data in the process using execute is replaced from thefile, but open files, current directory, and interprocess relation- ships are unaltered. Only if the call fails, for example because file could not be found or because its execute- permission bit was not set, does a return take place from the execute primitive; it resembles a "jump" machine instruction rather than a subroutine call. 5.4 Process Synchronization Another process control system call processid = wait( ) causes its caller to suspend execution until one of its children has completed execution. Then wait returns the processid of the terminated process. An error return is taken if the calling process has no descendants. Certain status from the child process is also available. Wait may also present status from a grandchild or more distant ancestor; see 5.5. 5.5 Termination Lastly, exit (status) terminates a process, destroys its image, closes its open files, and generally obliterates it. When the parent is notified through the wait primitive, the indicated status is available to the parent; if the parent has already terminated, the status is available to the grandparent, Communications July 1974 of Volume 17 the ACM Number 7 and so on. Processes may also terminate as a result of various illegal actions or user-generated signals (7 below). 6. The Shell For most users, communication with UNIX is carried on with the aid of a program called the Shell. The Shell is a command line interpreter: it reads lines typed by the user and interprets them as requests to execute other programs. In simplest form, a command line consists of the command name followed by arguments to the command, all separated by spaces: command argl arg~   - argn The Shell splits up the command name and the argu- ments into separate strings. Then a file with name command is sought; command may be a path name in- cluding the "/" character to specify any file in the sys- tem. If command is found, it is brought into core and executed. The arguments collected by the Shell are accessible to the command. When the command is finished, the Shell resumes its own execution, and in- dicates its readiness to accept another command by typing a prompt character. If file command cannot be found, the Shell prefixes the string /bin/ to command and attempts again to find the file. Directory/bin contains all the commands intended to be generally used. 6.1 Standard I/O The discussion of I/O in 3 above seems to imply that every file used by a program must be opened or created by the program in order to get a file descriptor for the file. Programs executed by the Shell, however, start off with two open files which have file descriptors 0 and 1. As such a program begins execution, file 1 is open for writing, and is best understood as the standard output file. Except under circumstances indicated be- low, this file is the user's typewriter. Thus programs which wish to write informative or diagnostic informa- tion ordinarily use file descriptor 1. Conversely, file 0 starts off open for reading, and programs which wish to read messages typed by the user usually read this file. The Shell is able to change the standard assignments of these file descriptors from the user's typewriter printer and keyboard. If one of the arguments to a command is prefixed by ")", file descriptor 1 will, for the duration of the command, refer to the file named after the ")". For example, Is ordinarily lists, on the typewriter, the names of the files in the current directory. The command 1 s )there creates a file called there and places the listing there. Thus the argument ")there" means, "place output on there." On the other hand, ed ordinarily enters the editor, which takes requests from the user via his typewriter. The command ed (script interprets script as a file of editor commands; thus "(script" means, "take input from script." Although the file name following "(" or ")" appears to be an argument to the command, in fact it is in- terpreted completely by the Shell and is not passed to the command at all. Thus no special coding to handle I/O redirection is needed within each command; the command need merely use the standard file descriptors 0 and l where appropriate. 6.2 Filters An extension of the standard l/o notion is used to direct output from one command to the input of another. A sequence of commands separated by vertical bars causes the Shell to execute all the commands simul- taneously and to arrange that the standard output of each command be delivered to the standard input of the next command in the sequence. Thus in the com- mand line ls lpr-2[opr ls lists the names of the files in the current directory; its output is passed to pr, which paginates its input with dated headings. The argument "--2" means double column. Likewise the output from pr is input to opr. This command spools its input onto a file for off-line printing. This process could have been carried out more clumsily by ls )temp 1 pr -- 2 (temp 1 )temp2 opr (temp2 followed by removal of the temporary files. In the absence of the ability to redirect output and input, a still clumsier method would have been to require the/s command to accept user requests to paginate its out- put, to print in multi-column format, and to arrange that its output be delivered off-line. Actually it would be surprising, and in fact unwise for efficiency reasons, to expect authors of commands such as Is to provide such a wide variety of output options. A program such as pr which copies its standard input to its standard output (with processing) is called a .filter. Some filters which we have found useful per- form character transliteration, sorting of the input, and encryption and decryption. 6.3 Command Separators: Multitasking Another feature provided by the Shell is relatively straightforward. Commands need not be on different 371 Communications July 1974 of Volume 17 the ACM Number 7 lines; instead they may be separated by semicolons. ls; ed will first list the contents of the current directory, then enter the editor. A related feature is more interesting. If a command is followed by "&", the Shell will not wait for the command to finish before prompting again; instead, it is ready immediately to accept a new command. For example, as source )output & causes source to be assembled, with diagnostic output going to output; no matter how long the assembly takes, the Shell returns immediately. When the Shell does not wait for the completion of a command, the identification of the process running that command is printed. This identification may be used to wait for the completion of the command or to terminate it. The "&" may be used several times in a line: as source )output & ls )files & does both the assembly and the listing in the back- ground. In the examples above using "&", an output file other than the typewriter was provided; if this had not been done, the outputs of the various commands would have been intermingled. The Shell also allows parentheses in the above operations. For example (date; ls) )x & prints the current date and time followed by a list of the current directory onto the file x. The Shell also returns immediately for another request. 6.4 The Shell as a Command: Command files The Shell is itself a command, and may be called recursively. Suppose file tryout contains the lines as source mv a. out testprog testprog The my command causes the file a.out to be renamed testprog, a.out is the (binary) output of the assembler, ready to be executed. Thus if the three lines above were typed on the console, source would be assembled, the resulting program named testprog, and testprog ex- ecuted. When the lines are in tryout, the command sh (tryout would cause the Shell sh to execute the commands sequentially. The Shell has further capabilities, including the ability to substitute parameters and to construct argu- ment lists from a specified subset of the file names in a directory. It is also possible to execute commands conditionally on character string comparisons or on existence of given files and to perform transfers of control within filed command sequences. 6.5 Implementation of the Shell The outline of the operation of the Shell can now be understood. Most of the time, the Shell is waiting for the user to type a command. When the new-line character ending the line is typed, the Shell's read call returns. The Shell analyzes the command line, putting the arguments in a form appropriate for execute. Then fork is called. The child process, whose code of course is still that of the Shell, attempts to perform an execute with the appropriate arguments. If successful, this will bring in and start execution of the program whose name was given. Meanwhile, the other process resulting from the fork, which is the parent process, waits for the child process to die. When this happens, the Shell knows the command is finished, so it types its prompt and reads the typewriter to obtain another command. Given this framework, the implementation of back- ground processes is trivial; whenever a command line contains "&", the Shell merely refrains from waiting for the process which it created to execute the command. Happily, all of this mechanism meshes very nicely with the notion of standard input and output files. When a process is created by the fork primitive, it inherits not only the core image of its parent but also all the files currently open in its parent, including those with file descriptors 0 and 1. The Shell, of course, uses these files to read command lines and to write its prompts and diagnostics, and in the ordinary case its children--the command programs--inherit them auto- matically. When an argument with "(" or ")" is given however, the offspring process, just before it performs execute, makes the standard I/O file descriptor 0 or 1 respectively refer to the named file. This is easy because, by agreement, the smallest unused file descriptor is assigned when a new file is opened (or created); it is only necessary to close file 0 (or 1) and open the named file. Because the process in which the command pro- gram runs simply terminates when it is through, the association between a file specified after "(" or ")" and file descriptor 0 or 1 is ended automatically when the process dies. Therefore the Shell need not know the actual names of the files which are its own standard input and output since it need never reopen them. Filters are straightforward extensions of standard I/o redirection with pipes used instead of files. In ordinary circumstances, the main loop of the Shell never terminates. (The main loop includes that branch of the return from fork belonging to the parent process; that is, the branch which does a wait, then reads another command line.) The one thing which causes the Shell to terminate is discovering an end-of-file condition on its input file. Thus, when the Shell is executed as a command with a given input file, as in sh (comfile the commands in comfile will be executed until the end of comfile is reached; then the instance of the 372 Communications July 1974 of Volume 17 the ACM Number 7 Shell invoked by sh will terminate. Since this Shell process is the child of another instance of the Shell, the wait executed in the latter will return, and another command may be processed. 6.6 Initialization The instances of the Shell to which users type com- mands are themselves children of another process. The last step in the initialization of UNIX is the creation of a single process and the invocation (via execute) of a program called init. The role of init is to create one. process for each typewriter channel which may be dialed up by a user. The various subinstances of init open the appropriate typewriters for input and output. Since when init was invoked there were no files open, in each process the typewriter keyboard will receive file de- scriptor 0 and the printer file descriptor 1. Each process types out a message requesting that the user log in and waits, reading the typewriter, for a reply. At the outset, no one is logged in, so each process simply hangs. Finally someone types his name or other identification. The appropriate instance of init wakes up, receives the log-in line, and reads a password file. If the user name is found, and if he is able to supply the correct pass- word, init changes to the user's default current directory, sets the process's user ID to that of the person logging in, and performs an execute of the Shell. At this point the Shell is ready to receive commands and the logging-in protocol is complete. Meanwhile, the mainstream path of init (the parent of all the subinstances of itself which will later become Shells) does a wait. If one of the child processes term- inates, either because a Shell found an end of file or because a user typed an incorrect name or password, this path of init simply recreates the defunct process, which in turn reopens the appropriate input and output files and types another login message. Thus a user may log out simply by typing the end-of-file sequence in place of a command to the Shell. 6.7 Other Programs as Shell The Shell as described above is designed to allow users full access to the facilities of the system since it will invoke the execution of any program with appro- priate protection mode. Sometimes, however, a dif- ferent interface to the system is desirable, and this feature is easily arranged. Recall that after a user has successfully logged in by supplying his name and password, init ordinarily invokes the Shell to interpret command lines. The user's entry in the password file may contain the name of a program to be invoked after login instead of the Shell. This program is free to interpret the user's messages in any way it wishes. For example, the password file entries for users of a secretarial editing system specify that the editor ed is to be used instead of the Shell. Thus when editing system users log in, they are inside the editor and can 373 begin work immediately; also, they can be prevented from invoking UN;X programs not intended for their use. In practice, it has proved desirable to allow a temporary escape from the editor to execute the format- ting program and other utilities. Several of the games (e.g. chess, blackjack, 3D tic- tac-toe) available on UNIX illustrate a much more severely restricted environment. For each of these an entry exists in the password file specifying that the appro- priate game-playing program is to be invoked instead of the Shell. People who log in as a player of one of the games find themselves limited to the game and unable to investigate the presumably more interesting offerings of UNJX as a whole. 7. Traps The PDP-1 l hardware detects a number of program faults, such as references to nonexistent memory, unimplemented instructions, and odd addresses used where an even address is required. Such faults cause the processor to trap to a system routine. When an illegal action is caught, unless other arrangements have been made, the system terminates the process and writes the user's image on file core in the current directory. A debugger can be used to determine the state of the program at the time of the fault. Programs which are looping, which produce un- wanted output, or about which' the user has second thoughts may be halted by the use of the interrupt signal, which is generated by typing the "delete" character. Unless special action has been taken, this signal simply causes the program to cease execution without producing a core image file. There is also a quit signal which is used to force a core image to be produced. Thus programs which loop unexpectedly may be halted and the core image ex- amined without prearrangement. The hardware-generated faults and the interrupt and quit signals can, by request, be either ignored or caught by the process. For example, the Shell ignores quits to" prevent a quit from logging the user out. The editor catches interrupts and returns to its command level. This is useful for stopping long printouts without losing work in progress (the editor manipulates a copy of the file it is editing). In systems without floating point hardware, unimplemented instructions are caught, and floating point instructions are interpreted. 8. Perspective Perhaps paradoxically, the success of UNIX is largely due to the fact that it was not designed to meet any predefined objectives. The first version was written when one of us (Thompson), dissatisfied with the available computer facilities, discovered a little-used Communications July 1974 of Volume 17 the ACM Number 7 PDP-7 and set out to create a more hospitable environ- ment. This essentially personal effort was sufficiently successful to gain the interest of the remaining author and others, and later to justify the acquisition of the POP-11/20, specifically to support a text editing and formatting system. When in turn the 11/20 was out- grown, UNIX had proved useful enough to persuade management to invest in the PDP-11/45. Our goals throughout the effort, when articulated at all, have always concerned themselves with building a comfort- able relationship with the machine and with exploring ideas and inventions in operating systems. We have not been faced with the need to satisfy someone else's requirements, and for this freedom we are grateful. Three considerations which influenced the design of UNIX are visible in retrospect. First, since we are programmers, we naturally designed the system to make it easy to write, test, and run programs. The most important expression of our desire for programming convenience was that the system was arranged for interactive use, even though the original version only supported one user. We be- believe that a properly-designed interactive system is much more productive and satisfying to use than a "batch" system. Moreover such a system is rather easily adaptable to noninteractive use, while the con- verse is not true. Second, there have always been fairly severe size constraints on the system and its software. Given the partially antagonistic desires for reasonable efficiency and expressive power, the size constraint has encouraged not only economy but a certain elegance of design. This may be a thinly disguised version of the "salva- tion through suffering" philosophy, but in our case it worked. Third, nearly from the start, the system was able to, and did, maintain itself. This fact is more important than it might seem. If designers of a system are forced to use that system, they quickly become aware of its functional and superficial deficiencies and are strongly motivated to correct them before it is too late. Since all source programs were always available and easily modified on-line, we were willing to revise and rewrite the system and its software when new ideas were invented, discovered, or suggested by others. The aspects of UNIX discussed in this paper exhibit clearly at least the first two of these design considera- tions. The interface to the file system, for example, is extremely convenient from a programming standpoint. The lowest possible interface level is designed to eliminate distinctions between the various devices and files and between direct and sequential access. No large "access method" routines are required to insulate the programmer from the system calls; in fact, all user programs either call the system directly or use a small library program, only tens of instructions long, which buffers a number of characters and reads or writes them all at once. Another important aspect of programming con- venience is that there are no "control blocks" with a complicated structure partially maintained by and de- pended on by the file system or other system calls. Generally speaking, the contents of a program's address space are the property of the program, and we have tried to avoid placing restrictions on the data structures within that address space. Given the requirement that all programs should be usable with any file or device as input or output, it is also desirable from a space-efficiency standpoint to push device-dependent considerations into the operating sys- tem itself. The only alternatives seem to be to load routines for dealing with each device with all programs, which is expensive in space, or to depend on some means of dynamically linking to the routine appropriate to each device when it is actually needed, which is expen- sive either in overhead or in hardware. Likewise, the process control scheme and command interface have proved both convenient and efficient. Since the Shell operates as an ordinary, swappable user program, it consumes no wired-down space in the system proper, and it may be made as powerful as desired at little cost. In particular, given the framework . in which the Shell executes as a process which spawns other processes to perform commands, the notions of I/O redirection, background processes, command flies, and user-selectable system interfaces all become essen- tially trivial to implement. 8.1 Influences The success of'UNIX lies not so much in new inven- tions but rather in the full exploitation of a carefully selected set of fertile ideas, and especially in showing that they can be keys to the implementation of a small yet powerful operating system. The fork operation, essentially as we implemented it, was present in the Berkeley time-sharing system [8]. On a number of points we were influenced by Multics, which suggested the particular form of the I/o system calls [9] and both the name of the Shell and its general func- tions. The notion that the Shell should create a process for each command was also suggested to us by the early design of Multics, although in that system it was later dropped for efficiency reasons. A similar scheme is used by TENEX [10]. 9. Statistics The following statistics from UNIX are presented to show the scale of the system and to show how a system of this scale is used. Those of our users not involved in document preparation tend to use the system for pro- gram development, especially language work. There are few important "applications" programs. 374 Communications July 1974 of Volume 17 the ACM Number 7 9.1 Overall 72 user population 14 maximum simultaneous users 300 directories 4400 files 34000 512-byte secondary storage blocks used 9.2 Per day (24-hour day, 7-day week basis) There is a "background" process that runs at the lowest possible priority; it is used to soak up any idle cPu time. It has been used to produce a million-digit approximation to the constant e - 2, and is now generating composite pseudoprimes (base 2). 1800 commands 4.3 CPU hours (aside from background) 70 connect hours 30 different users 75 logins 9.3 Command CPU Usage (cut off at 1%) 15.7% Ccompiler 1.7% Fortran compiler 15.2% users' programs 1.6% remove file 11.7% editor 1.6% tape archive 5.8% Shell (used as a corn- 1.6% file system consistency mand, including com- check mand times) 1.4% library maintainer 5.3% chess 1.3% concatenate/printfiles 3.3% list directory 1.3% paginate and print file 3.1% document formatter 1.1% print disk usage 1.6% backup dumper 1.0% copy file 1.8% assembler 9.4 Command Accesses (cut off at 1%) 15.3% editor 1.6% debugger 9.6% list directory 1.6% Shell (used as a command) 6.3% remove file 1.5% print disk availability 6.3% C compiler 1.4% list processes executing 6.0% concatenate/printfile 1.4% assembler 6.0% users' programs 1.4% print arguments 3.3% list people logged on 1.2% copy file system 1.1% paginate and print file 3.2% rename/move file 1.1% print current date/time 3.1% file status 1.1% file system consistency 1.8% library maintainer check 1.8% document formatter 1.0% tape archive 1.6% execute another com- mand conditionally 9.5 Reliability Our statistics on reliability are much more subjective than the others. The following results are true to the best of our combined recollections. The time span is over one year with a very early vintage 11/45. There has been one loss of a file system (one disk out of five) caused by software inability to cope with a hardware problem causing repeated power fail traps. Files on that disk were backed up three days. A "crash" is an unscheduled system reboot or halt. There is about one crash every other day; about two-thirds of them are caused by hardware-related dif- 375 ficulties such as power dips and inexplicable processor interrupts to random locations. The remainder are software failures. The longest uninterrupted up time was about two weeks. Service calls average one every three weeks, but are heavily clustered. Total up time has been about 98 percent of our 24-hour, 365-day schedule. Acknowledgments. We are grateful to R.H. Canaday, L.L. Cherry, and L.E. McMahon for their contribu- tions to uNIX. We are particularly appreciative of the inventiveness, thoughtful criticism, and constant sup- port of R. Morris, M.D. McIlroy, and J.F. Ossanna. References 1. Digital Equipment Corporation. PDP-I1/40 Processor Handbook, 1972, and PDP-I 1/45 Processor Handbook, 1971. 2. Deutsch, L.P., and Lampson, B.W. An online editor. Comm. ACM 10, 12 (Dec. 1967), 793-799, 803. 3. Richards, M. BCPL: A tool for compiler writing and system programming. Proc. AFIPS 1969 SJCC, Vol. 34, AFIPS Press, Montvale, N.J., pp. 557-566. 4. McClure, R.M. TMG--A syntax directed compiler. Proc. ACM 20th Nat. Conf., ACM, 1965, New York, pp. 262-274. 5. Hall, A.D. The M6 macroprocessor. Computing Science Tech. Rep.#2, Bell Telephone Laboratories, 1969. 6. Ritchie, D.M. C reference manual. Unpublished memorandum, Bell Telephone Laboratories, 1973. 7. Aleph-null. Computer Recreations. So[?ware Practice and Experience 1, 2 (Apr.-June 1971), 201-204. 8. Deutsch, L.P., and Lampson, B.W. SDS 930 time-sharing system preliminary reference manual. Doc. 30.10.10, Project G ENI E, U of California at Berkeley, Apr. 1965. 9. Feiertag, R.J., and Organick, E.I. The Multics input-output system. Proc. Third Syrup. on Oper. Syst. Princ., Oct. 18-20, 1971, ACM, New York, pp. 35-41. 10. Bobrow, D.G., Burchfiel, J.D., Murphy, D.L., and Tomlinson, R.S. TENEX, a paged time sharing system tbr the PDP-10. Comm. ACM15, 3 (Mar. 1972), 135-143. Communications July 1974 of Volume 17 the ACM Number 7 Keywords: Social networking sites (SNS); Social media; Academics;
Health; Awareness; Privacy; Security policies
Introduction
In this era of technology, Internet plays a dynamic role in 
almost every eld of life. rough internet, users can communicate 
with each other using dierent platforms. Students use this facility 

for communication, preparing dierent projects, assignment and 

presentations. ere are dierent platforms for dierent purposes. 

For formal communication users usually use Gmail, Yahoo mail or 

Hotmail. For casual talks or sharing dierent kind of stu users prefer 

Facebook, twitter or Instagram. For contacting someone urgently users 

avail the facility of WhatsApp. For online streaming users use YouTube. 

And for online games, users go to miniclip or blizzard entertainment.
Social media is a concept based on a platform for the people around 
the world to discuss their issues and opinions and exchange information. 

And social networking sites (SNS) are basically those tools that allows 

the people to exchange information, ideas, images, videos and much 

more through a specic network. Few examples of SNS are Facebook, 

Twitter, YouTube, Blizzard Entertainment, WhatsApp etc. SNS provide 

its users a facility to create their own prole with their own list of users. 

With this list of users, the person can connect at that specic platform 

and that platform oers dierent features like chatting, blogging, audio 

and video calling, mobile connectivity and video and photo sharing.
Facebook provides its users to make their own prole. Besides this, 
the users can create a page or group, which can be used for marketing 

purpose or educational purpose etc. Facebook also provides dierent 

facilities like; blocking, following, posting, commenting, liking, 

sharing, messaging and audio/video calls. Using twitter, a user can 

make an account and then can avail the facility of tweeting, retweeting, 

messaging and adding other users as followers.
Instagram users post pictures to let their followers know what is 
happening around them. e followers can like comment and share 

the posts. You Tube is a platform where users make channels and then 

upload videos. e viewers watch the videos and can report, dislike, 

like, comment and share the video on other social platforms. e 

users can search any video by its name as well. Gamers who use online 

gaming platforms avail the facility of multiplayers gaming mode and 

besides that they can chat with each other. In this paper we will review 

the past literature available to understand the role of social media in 

students life and then we will nd out the positive and negative impacts 

of social networking platforms on students physical and psychological 

health and also how it aects the students academics in Pakistan.
Literature Review
According to OKeee GS and Clarke-Pearson K [1], social media 
impacted more in negative side. It includes accessing inappropriate 

contents without understanding the privacy policies. ey also said that 

aer the introduction of social media, there is more online harassment 

and cyberbullying.
Ahn J [2] while doing research presented a theory known as 
Signaling eory. According to her theory, an individual while trying 

to make himself/herself popular adds so many unknown users as his/

her friend, and this is how an individual compromises their own trust 

and privacy.
Rideout V [3] said that the youth is spending more time on social 
media just for entertainment purpose. She told that an American child 

on average spends 7-8 hours a day, just for entertainment and usually 

multi-task between dierent social networks they use.
Pardo A [4] has made an eort to explain social medias importance 
in students life. He said that such platforms allow students to 

communicate with each other for sharing information and teachers as 

well. Pardo states that such kind of interactions are necessary for people 

to learn new things.
Cain J and Chretien K [5] introduced a new term interprofessional 
education. It is dened as when dierent individuals come together 

on a single platform to share knowledge which leads to improved and 

better understanding as they are learning from each others experiences.
El-Badawy TA and Hashem Y [6] in their research concluded that 
social media has no relationship with students academic performance. 

And there is no impact (positive or negative) on students academic 

routine.
According to Tariq W and Mehboob M [7], social media is ruining 
the future and carrier of students. It distracts students from their 
Hassan Khalid*Department of Computer Science, Abdul Wali Khan University, Mardan, Pakistan
AbstractIn the present world of technology, Internet plays a vital role in students life. Students use the internet facility 
for dierent purposes i.e., communication, preparation, acquiring knowledge, entertainment etc. In this research, 
our task is to nd the impact of social networks on students physical and psychological health. We also nd that 

how social networks impact the academics of students. Lastly, we discuss how youngsters got compromised by their 

personal and private data. In the end, we gured out that the excessive use of social networks eect the students 

physical and psychological health negatively. Social Networking Platforms have a positive impact on their academics. 

A number of students have been harassed and their personal data has been misused due to lack of knowledge about 

security and privacy policies. erefore, we suggested that social media should be taught as a subject or at a workshop/

seminar for the awareness of cybercrimes and the policies (security and privacy both).
*Corresponding author:
 Hassan Khalid, Department of Computer Science,

Abdul Wali Khan University, Mardan, Pakistan, Tel: +923355823363; E-mail:

malikhasankhalid@gmail.comReceived June 14, 2017
; Accepted August 01, 2017
; Published August 07, 2017Citation: Khalid H (2017) 
The Effects of Social Networks on Pakistani Students. 
J Inform Tech Softw Eng 7: 203. doi: 10.4172/2165-7866.1000203

Copyright:  2017 
Khalid H. 
This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted 
use, distribution, and reproduction in any medium, provided the original author and 

source are credited.Journal ofInformation Technology & Software EngineeringJournal of Information Technology &Software EngineeringISSN: 2165-7866Page 2 of 6studies. Further on, they analyzed the data collected from a survey that 
there is a very large number users using social media with multiple IDs 

(real and fake one). Lastly they said that in Pakistan there are no such 

laws for social networking sites to be followed. 
Kalia G [8] in her research paper concluded that the students should 
be engaged on a social platform with each other to learn better and 

more. Teachers should help kids to understand what social media is? 
and its fair use. Finally she said that social media is an entertainment 
world but it can also be used for educative and professional purposes.
Shahjahan ATM and Chisty KU [9] came to the conclusion that 
the overall eect of social media on teenagers is negative. Because of 
addiction to social platforms, students usually get low grades. Parents 
are not aware of their childrens social accounts and their policies, which 

aect them later on. e teenager lack condence and try to avoid face-
to-face communications.
Siddiqui S and Singh T [10] discussed both the positive and negative 
aspects of social media. Students can make a group of their class on any 

social platform where they can discuss and express themselves. ey 
can share useful information by using that specic platform. ey can 
explore the opportunities for their future. But it also distract students 

from their academics. Teenagers are not completely aware of the terms 

and policies and that social platform. And they misuse it by creating 

fake accounts for fun purposes and later on get addicted to it.
Khurana N [11] discovered that in India 66% of the youth uses 
social platforms for at least 2 hours a day. He also discovered that a 

very large number of youngsters have been victimized by cyber-

crimes. And nally he concluded that it all happens just because of not 

understanding the terms and conditions properly.
Zeitel-Bank N [12] in her paper summarized that social platforms 
provide us the opportunity to share a local news and make it a global 

one. But if one business rm spread a fake news about his rival, it hardly 

take minutes to damage the companys reputation and shares. And same 

is the scene in politics, one viral video can have a huge impact on you 

individually and distributed as well.
Mingle J and Adams M [13] came to the conclusion that besides the 
positive impact of social media on students academics, it is dangerous 
in many ways. It distracts students from their studies and students get 
addicted to social networks unconsciously because of its extremely 
user- friendly interface. And it later on aects the grades of students.
Wang Q, et al. [14] conducted a survey in which they found out that 
Facebook is the most popular social platform. Students spent vast hours 

checking social media posts. Students use social platforms during their 
classes, using their cell phones. 90% of the students use social networks 
for entertainment purposes only. It aects the eciencies and their 

grades as well. Social platforms distract students from their academics 
because of its attractive and user-friendly interface.
Abdulahi A, et al. [15] did a research on the negative eects of social 
networking platforms. ey found out that students also use social 
platforms on their cell phones during the lectures. e fact that students 
miss their lectures leads them to low grades in academics. Slowly, social 
networking sites also lead them to get addicted to it, and then users 
use it day and night. is aects the students health. In hope of getting 
more friends, users usually share their own personal data, which later 
on can be misused and they compromise their privacy and security.
Li Y [16] in his research concluded that social networking platform 
itself is a mass-oriented, social and value-neutral. eir value depends 
upon the users application. ere is much messy, mixed, false and 
untrue information on social platforms, which cannot fully meet the 
demands of students in China. It also adds risks for students with 

regards to their privacy and security.
Chanchalor S, et al. [17] had done a detailed quantitative research 
regarding the eects of online gaming on the health eects of online 
gamers. e symptom of drowsiness and dizziness was the rst 

disorder, then insomnia, annoyance, pain in the occipital bone or back. 
e least symptom was avoidance of their social contacts. ey also 
concluded that students who played violent games decreased on EQ, 
sympathy, responsibility, decision making and good relationship. And 
those students who play less violent games has no positive eect on EQ.
Fu-Yun Y and Yu-Hsin L [18] worked on using social media 
(Facebook) as a teaching and learning tool for in-class Q&A activities.
ey performed a survey and aer data analysis, four features were 
identied: enhanced social interaction, more focused learning, more 
amiable classroom environment and deepened thinking about the 
awareness of the complexity of the study topic
Leelathakul N and Chaioah K [19] studied the eects of using 
Facebook as an educational tool for students to acquire knowledge. 
ey explored the correlation between dierent Facebook activities 
and students performances in classes. ey concluded that Facebook is 

good for students as a learning tool and they also said that an individual 
activity aspect is not linearly correlated with students result.
 De Silva E and De Silva EC [20] in their research discussed the 
importance of multidisciplinary approach in online classrooms, and 
to create a learner-centered environment for the students for their 
clear focus on what they are going to learn, why they are learning 
and what competencies are expected. ey also concluded that with 

multidisciplinary approach student will be able to identify pragmatic 
solutions to common problems through interaction, interconnection 
and integration with other elds.
Purpose of the Study
e purpose of the study is to understand the dierence between 
social media and social networking sites. en we will discuss 
its impact on students academics, physical health, psychological 

health and security and privacy issues. Finally we will present our 

recommendations aer discussing the results and conclusions.
Results and Discussion 
We have arranged a survey, in which we asked the respondents 
dierent questions regarding social networking sites and its upshots. 

Our targeted audience was mostly students aged between 16-26. Our 

total responses were 100.
Aer getting the data from survey, we statistically analyzed all the 
questions with respect to their variance, standard deviation, mean, 

standard error and satisfaction rate.
Lastly, we took the average of all the values for referencing in our 
paper. e values are given below:
Variance: 6.18
Standard deviation= : 1.50
Mean=  
 or : 3.06
Standard error: 0.15

Satisfaction rate: 56.38
Page 3 of 6Popular social platform
When we ask the students that which platforms they use? ey told 
us that, in the present world of technology everyone uses some kind 
of social network for some purpose. Few of us use social networks for 

acquiring knowledge, some people use for making new friends around 
the globe, few of us use to make themselves updated regarding the events 
occurred in the classes like assignment or presentations etc. Some of 

the students use social platforms for entertainment purpose. 33% of 
the students use Facebook, 23% use WhatsApp, 21% use YouTube, 12% 
Instagram, 9% Twitter and 2% other social platforms i.e., snapchat and 
Imo (Figure 1).
Purpose of using social networks
Students use social networks for many reasons, like online gaming, 
chatting with their friends, watching movies/videos, making new 
friends, learning something new, doing assignment etc. According to 
our survey, 40% of the student use the social networking platforms 

for entertainment purposes, which further includes online gaming, 

chatting, watching online videos etc. 35% use social networking 
platforms for educative purposes. It includes watching online tutorials 
regarding their courses, reading online material etc. 20% students use 
social platforms to keep themselves updated regarding the current 

activities in their classes. Like presentation updates, assignment and its 

deadlines etc. Only 5% students are interested in making new friends 

around the globe. e purpose of these students is to learn their culture 

and if they have any scope in dierent countries for jobs or further 

education (Figure 2). 
Awareness about the terms and policies
When the students were asked that are you aware of the terms and 
policies of the network/platform you are using, 71% of the students 

refused that they are not aware of the terms and policies of the social 

networks they are using. 29% students agreed that they understand the 

terms and policies of the social networking platforms they are using 

(Figure 3).
Impact of social networking platforms on academics
73% of the responses quoted that social networking platforms play 
a positive role in our academics. Social platforms have a positive impact 

on our studies during the exams as well. Just 27% have the negative 

opinion. ey think that social platforms usually distracts us from 
 Figure 1: Popular social network. Figure 2: Purpose of using social networks.Figure 3: Awareness regarding the terms and policies of social platforms they 

are using.our studies, especially during exams (Figure 4). 60% think that SNS 

play a negative role, when we talk about the psychological health of 

students. ey think that online gamers are the people who are aected 

negatively by the extreme use of SNS. For other users, whenever any 

platform becomes an addiction, it will aect their psychology in a 

negative way (Figure 5).
Impact of social networking platforms on physical health
According to 70% of our respondents, SNS has a negative impact on 
their physical health. ey think that once you get addicted to any social 
platform, then it will aect your time and physical health negatively. 
30% said that it has a positive impact on their physical health. ey 
think that if we use social platforms in a balanced way, then it will not 

aect your physical health negatively (Figure 6).
How to spread awareness regarding social networks
When the students were asked about the ways or techniques to 
spread awareness regarding dierent aspects/ dimensions of Social 
Media and Social Networking Platforms. 24% of the students thought 

that it should be taught as a subject in schools or colleges. 33% suggested 

that in a workshop, the students should taught dierent dimensions of 

social media and also make them understand the security and privacy 

policies. 43% advised that during a seminar at dierent schools, colleges 
Page 4 of 6 Figure 4: Impact of social networking platforms on academics. Figure 5: Impact of social networking platforms on physical health. Figure 6: Impact of social networking platforms on psychological health.and universities, the positive and negative aspects of using SNS should 
be discussed. By doing this, we can also spread awareness regarding 

dierent issues related to social networks.
Impact of social networking platforms on psychological 

health
According to 40% of our respondents, SNS have a positive impact 
on students psychological health (Figure 7).
Cybercrimes
During the survey, we asked the students about the cybercrimes. 
ey told us that we have heard people doing dierent illegal 
activities, like misuse of someones personal data, using fake-ids and 

impersonation. 22% agrees that they have seen fake-ids on dierent 

social networks. 11% agreed that sometimes data has been misused for 
dierent purposes as well as for illegal activities. Only 2% mentioned 

the crime impersonation. 64% respondents told us that all the crimes 
are present in todays world (Figure 8).
Awareness to cybercrimes
Regarding the above question about the cybercrimes, we asked the 
students about any kind of exposure to any cybercrime. 89% answered 
the question negatively. Just 11% responded positively (Figure 9). Aer 
understanding the analyzed data from the survey, we understand the 
purpose of the students to use SNS. Secondly, we also got to know 

about the unawareness regarding dierent dimensions of using social 

platforms. Lastly, we concluded that the new generation is indierent 

to cybercrimes. Students who spend more time on social platforms are 
trying to get more socialized and because of this, they get addicted to a 
social platform, which slowly aects their health.
Future Work
In my opinion, the current research work lays down reasonably 
 Figure 7: Way to spread awareness among students regarding social media.
Figure 8: Common aspects of cybercrimes related to the students.Page 5 of 6good foundations for the futuristic research work particularly in the 
dimensions as shown in the gure given below (Figure 10).
Conclusion
In the end we will discuss the concluded facts, which we discovered 
from reviewing the past literature and by doing the survey through 
a questionnaire. At the end we will give certain suggestions to be 
followed to make the students health and life safer. First thing we 

discovered was that in Pakistan, every student is using some kind 
of platform for socializing as well as for academic and educational 
purposes. Most of the students have accounts on Facebook and use 

YouTube for dierent purposes. If the students use these platforms 
in a balanced way, it will not harm their health. Secondly, we found 
out that there is a huge number of users who are unaware of the terms 

and policies of the social networks they are using. Most of the time, 
the users download the applications on their mobile phones and they 
without reading the terms and conditions press OK/Allow button to 

download their desired applications. Understanding the terms and 
policies of any platform is very much important. It can aect someones 
security and privacy negatively. irdly, we came to the conclusion that 

social networks plays a positive role in students academic activities. 
Especially using Facebook, classmates make a group and share the 
lectures, presentations, assignments on that group. During exams, 

they share important questions etc. it helps the whole class to know 

and to understand the issues or problems in detail. e next thing we 

discovered was that using social platforms for entertainment purposes 

has a negative impact on students physical health. ose users who 

plays online games and those who are addict to any social platform 

become physically sick. Dierent kinds of symptoms have been 

observed like dizziness, drowsiness, insomnia, annoyance and pain in 

the occipital bone of back. In some cases, they become so unsocial in 

their real lives that they ignore meeting face to face with their friends 

and family. We also got to know that excessive use of social platforms or 

playing online games for a longer duration also aect your psychological 
health negatively. We found out that students who spent more than ve 
hours surng on dierent social platforms and playing online games 

are aected by psychological health issues like stress, mood disorder, 

anxiety disorder etc. We gured out that there is a large number of social 
networks users who are actually not aware of the terms and policies of 
the platform, they are using. It can further lead them being victimized 
by cyber experts, as well as they also face problems with their personal 

data being misused in dierent ways. We concluded that this generation 

is a quite indierent to cybercrimes. ey are not interested in what 

kind of crimes are going on around them. eir posts contain violent 

material and they feel happy when other users react to their posts. 

Everyone is aware of dierent kinds of cybercrimes like fake Ids, fake 

news, libel, misuse of personal stu and impersonation. But most of the 

time students do not take such things seriously. Lastly, we discovered 

that Social Media is as deep as an ocean. erefore it should be taught 

properly whether as a subject or through seminars or workshops. It is 

imperative to spread awareness regarding dierent dimensions of social 

media. Some of them are given below:
Pros and cons of using social platforms. 
Proper and legal use of social networks.
Understanding the terms and policies of dierent social 
networks.
How to keep your account secure and safe?
 Recommendations After discussing the results and giving the conclusion, we recommend the 

following things to be noted and acted upon to have a better and safer 

future: Age limit: There should be age limit to the users of different social platforms.
 Parental guidance and check: Parents should have a little check on their 

kids as well as guide them, so that they use such platforms in a proper 

manner to avoid the imbalance in their physical and psychological health. General awareness: The awareness should be spread around, to 
understand the social networks properly.
 Teaching social media: Social Media should be taught as a subject or during 

a workshop or at a seminar.
 Awareness regarding cybercrimes: students should learn how to secure 

their accounts from different cyber-attacks and what to do if they get 
victimized. Awareness regarding terms and policies: Students should also learn and 
understand the terms and policies of all the social platforms they have been 
signed up.References1. O'Keefe GS, Clarke-Pearson K (2011) The Impact of Social Media on Children, 
Adolescents, and Families. Pediatrics 127: 800-804.2. Ahn J (2011) The Effect of Social Network Sites on Adolescents Social and 
 Figure 9: Exposed to any dimension of cybercrimes.Figure 10: Prospects of future work.Page 6 of 6Academic Development: Current Theories and Controversies. J Am Soc Inf Sci 
Technol 62: 1435-1445.
3.Rideout V (2012) Social Media, Social Life: How Teens View Their Digital Lives. 
Common Sense Media.4.Pardo A (2013) Social Learning Graphs: Combining Social Network Graphs
and Analytics to Represent Learning Experiences. Int J Social Media and

Interactive Learning Environments 1: 43-58.5.Cain J, Chretien K (2013) Exploring Social Media's Potential in Interprofessional 
Education. Journal of Research in Interprofessional Practice and Education.6.El-Badawy TA, Hashem Y (2015) The Impact of Social Media on Academic
Development of School Students. International Journal of Business

Administration 6: 46-52.7.Tariq W, Mehboob M, Khan M, FaseeUllah (2012) The Impact of Social Media
and Social Networks on Education and Students on Pakistan. International
Journal of Computer Science Issues 9:  407-411.8. Kalia G (2013) A Research Paper on Social Media: An Innovative Educational
Tool. Issues and Ideas in Education 1: 43-50.
9. Shahjahan ATM, Chisty KU (2014) Social Media Research and its Effects on
Our Society. International Journal of Social, Behavioral, Educational, Economic, 

Business and Industrial Engineering 8: 2009-2013.10. Siddiqui SS, Singh T (2016) Social Media its Impact with Positive and Negative 
Aspects. International Journal of Computer Applications Technology and

Research 5: 71-75.11.
 Khurana N (2015) The Impact of Social Networking Sites on the Youth. JMCL
5: 1-4.12. Zeitel-Bank N, Tat U (2014) Social Media and its Effects on Individuals and
Social Systems. Portoroz, Slovenia: Human Capital without Borders, pp: 1183-

1190.
13. Mingle J, Adams DM (2015) Social Media Network Participation and Academic 
Performance in Senior High Schools in Ghana. Library Philosophy and Practice 

(EJournal), pp: 1-51.14. Wang Q, Chen W, Liang Y (2011) The Effects of Social Media on College
Students. 15. Abdulahi A, Samadi B, Gharleghi B (2014) A Study on the Negative Effects

Scholars in Malaysia. Int J Bus Soc Sci 5: 133-145.16. Li Y (2011) Survey on Situation of Chinese College Students Choosing To Use 
Social Networking. IEEE, Computer Research and Development (ICCRD),

2011 3rd International Conference. pp: 344-348.
17. Chanchalor S, Konsue S, Chanchalor O (2012) Health Effects of Playing
Online Game: Vocational and Technical Students on Thailand. International

Conference on Biomedical and Health Informatics, pp: 120-122.18. Yu FY, Liu YH (2015) Social Media as a Teaching and Learning Tool for In-
Class Q&A Activities to Promote Learning and Transform College Engineering

Classroom Dynamics: The Case of Facebook. International Conference on

Advanced Learning Technologies.
19. Leelathakul N, Chaipah K (2013) Quantitative Effects of using Facebook as
a Learning Tool on Students' Performance. International Joint Conference on

Computer Science and Software Engineering (JCSSE).20. DeSilva E, DeSilva EC (2014) Online Teaching: A Comparison of On-Ground,
Online & Facebook-linked Teaching. International Conference on Interactive

Mobile Communication Technologies and Learning (IMCL) 338-341.
ROUGE: A Package for Automatic Evaluation of Summaries Chin-Yew Lin Information Sciences Institute University of Southern California 4676 Admiralty Way Marina del Rey, CA  90292 cyl@isi.edu  Abstract ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It includes measures to auto-matically determine the quality of a summary by comparing it to other (ideal) summaries created by humans. The measures count the number of over-lapping units such as n-gram, word sequences, and word pairs between the computer-generated sum-mary to be evaluated and the ideal summaries cre-ated by humans. This paper introduces four different ROUGE measures: ROUGE-N, ROUGE-L, ROUGE-W, and ROUGE-S included in the ROUGE summariza-tion evaluation package and their evaluations. Three of them have been used in the Document Under-standing Conference (DUC) 2004, a large-scale summarization evaluation sponsored by NIST. 1 Introduction Traditionally evaluation of summarization involves human judgments of different quality metrics, for example, coherence, conciseness, grammaticality, readability, and content (Mani, 2001). However, even simple manual evaluation of summaries on a large scale over a few linguistic quality questions and content coverage as in the Document Under-standing Conference (DUC) (Over and Yen, 2003) would require over 3,000 hours of human efforts. This is very expensive and difficult to conduct in a frequent basis. Therefore, how to evaluate summa-ries automatically has drawn a lot of attention in the summarization research community in recent years. For example, Saggion et al. (2002) proposed three content-based evaluation methods that measure similarity between summaries. These methods are: cosine similarity, unit overlap (i.e. unigram or bi-gram), and longest common subsequence. However, they did not show how the results of these automatic evaluation methods correlate to human judgments. Following the successful application of automatic evaluation methods, such as BLEU (Papineni et al., 2001), in machine translation evaluation, Lin and Hovy (2003) showed that methods similar to BLEU, i.e. n-gram co-occurrence statistics, could be applied to evaluate summaries. In this paper, we introduce a package, ROUGE, for automatic evaluation of sum-maries and its evaluations. ROUGE stands for Re-call-Oriented Understudy for Gisting Evaluation. It includes several automatic evaluation methods that measure the similarity between summaries. We de-scribe ROUGE-N in Section 2, ROUGE-L in Section 3, ROUGE-W in Section 4, and ROUGE-S in Section 5. Section 6 shows how these measures correlate with human judgments using DUC 2001, 2002, and 2003 data. Section 7 concludes this paper and dis-cusses future directions. 2 ROUGE-N: N-gram Co-Occurrence Statistics Formally, ROUGE-N is an n-gram recall between a candidate summary and a set of reference summa-ries. ROUGE-N is computed as follows:  ROUGE-N =}{}{)()(SummariesReferenceSSgramSummariesReferemceSSgrammatchnnnngramCountgramCount (1)  Where n stands for the length of the n-gram, gramn, and Countmatch(gramn) is the maximum num-ber of n-grams co-occurring in a candidate summary and a set of reference summaries.  It is clear that ROUGE-N is a recall-related meas-ure because the denominator of the equation is the total sum of the number of n-grams occurring at the reference summary side. A closely related measure, BLEU, used in automatic evaluation of machine translation, is a precision-based measure. BLEU measures how well a candidate translation matches a set of reference translations by counting the per-centage of n-grams in the candidate translation over-lapping with the references. Please see Papineni et al. (2001) for details about BLEU. Note that the number of n-grams in the denomina-tor of the ROUGE-N formula increases as we add more references. This is intuitive and reasonable because there might exist multiple good summaries. Every time we add a reference into the pool, we ex-pand the space of alternative summaries. By con-trolling what types of references we add to the reference pool, we can design evaluations that focus on different aspects of summarization. Also note that the numerator sums over all reference summa-ries. This effectively gives more weight to matching n-grams occurring in multiple references. Therefore a candidate summary that contains words shared by more references is favored by the ROUGE-N meas-ure. This is again very intuitive and reasonable be-cause we normally prefer a candidate summary that is more similar to consensus among reference sum-maries. 2.1 Multiple References So far, we only demonstrated how to compute ROUGE-N using a single reference. When multiple references are used, we compute pairwise summary-level ROUGE-N between a candidate summary s and every reference, ri, in the reference set. We then take the maximum of pairwise summary-level ROUGE-N scores as the final multiple reference ROUGE-N score. This can be written as follows:  ROUGE-Nmulti  = argmaxi ROUGE-N(ri,s)   This procedure is also applied to computation of ROUGE-L (Section 3), ROUGE-W (Section 4), and ROUGE-S (Section 5). In the implementation, we use a Jackknifing procedure. Given M references, we compute the best score over M sets of M-1 refer-ences. The final ROUGE-N score is the average of the M ROUGE-N scores using different M-1 refer-ences.  The Jackknifing procedure is adopted since we often need to compare system and human per-formance and the reference summaries are usually the only human summaries available. Using this procedure, we are able to estimate average human performance by averaging M ROUGE-N scores of one reference vs. the rest M-1 references. Although the Jackknifing procedure is not necessary when we just want to compute ROUGE scores using multiple references, it is applied in all ROUGE score compu-tations in the ROUGE evaluation package. In the next section, we describe a ROUGE measure based on longest common subsequences between two summaries. 3 ROUGE-L: Longest Common Subsequence  A sequence Z = [z1, z2, ..., zn] is a subsequence of another sequence X = [x1, x2, ..., xm], if there exists a strict increasing sequence [i1, i2, ..., ik] of indices of X such that for all j = 1, 2, ..., k, we have xij = zj  (Cormen et al., 1989). Given two sequences X and Y, the longest common subsequence (LCS) of X and Y is a common subsequence with maximum length. LCS has been used in identifying cognate candi-dates during construction of N-best translation lexi-con from parallel text. Melamed (1995) used the ratio (LCSR) between the length of the LCS of two 
words and the length of the longer word of the two 
words to measure the cognateness between them. 
He used LCS as an approximate string matching algorithm. Saggion et al. (2002) used normalized pairwise LCS to compare similarity between two texts in automatic summarization evaluation.  3.1 Sentence-Level LCS To apply LCS in summarization evaluation, we view a summary sentence as a sequence of words. The intuition is that the longer the LCS of two summary sentences is, the more similar the two summaries are. We propose using LCS-based F-measure to estimate the similarity between two summaries X of length m and Y of length n, assum-ing X is a reference summary sentence and Y is a candidate summary sentence, as follows:  Rlcs mYXLCS),(=      (2) Plcs nYXLCS),(=      (3) Flcs  lcslcslcslcsPRPR22)1(bb++=(4)  Where LCS(X,Y) is the length of a longest com-mon subsequence of X and Y, and  = Plcs/Rlcs when ?Flcs/?Rlcs_=_?Flcs/?Plcs.  In DUC,  is set to a very big number (? 8). Therefore, only Rlcs is consid-ered. We call the LCS-based F-measure, i.e. Equa-tion 4, ROUGE-L. Notice that ROUGE-L is 1 when X = Y; while ROUGE-L is zero when LCS(X,Y) = 0, i.e. there is nothing in common between X and Y. F-measure or its equivalents has been shown to have met several theoretical criteria in measuring accu-racy involving more than one factor (Van Rijsber-gen, 1979). The composite factors are LCS-based recall and precision in this case. Melamed et al. (2003) used unigram F-measure to estimate machine translation quality and showed that unigram F-measure was as good as BLEU.  One advantage of using LCS is that it does not re-quire consecutive matches but in-sequence matches that reflect sentence level word order as n-grams. The other advantage is that it automatically includes longest in-sequence common n-grams, therefore no predefined n-gram length is necessary.  ROUGE-L as defined in Equation 4 has the prop-erty that its value is less than or equal to the mini-mum of unigram F-measure of X and Y. Unigram recall reflects the proportion of words in X (refer-ence summary sentence) that are also present in Y (candidate summary sentence); while unigram pre-cision is the proportion of words in Y that are also in X. Unigram recall and precision count all co-occurring words regardless their orders; while ROUGE-L counts only in-sequence co-occurrences.  By only awarding credit to in-sequence unigram matches, ROUGE-L also captures sentence level structure in a natural way. Consider the following example:  S1. police killed the gunman S2. police kill the gunman S3. the gunman kill police  We only consider ROUGE-2, i.e. N=2, for the pur-pose of explanation. Using S1 as the reference and S2 and S3 as the candidate summary sentences, S2 and S3 would have the same ROUGE-2 score, since they both have one bigram, i.e. the gunman. How-ever, S2 and S3 have very different meanings. In the case of ROUGE-L, S2 has a score of 3/4 = 0.75 and S3 has a score of 2/4 = 0.5, with  = 1. Therefore S2 is better than S3 according to ROUGE-L. This exam-ple also illustrated that ROUGE-L can work reliably at sentence level. However, LCS suffers one disadvantage that it only counts the main in-sequence words; therefore, other alternative LCSes and shorter sequences are not reflected in the final score. For example, given the following candidate sentence: S4. the gunman police killed Using S1 as its reference, LCS counts either the gunman or police killed, but not both; therefore, S4 has the same ROUGE-L score as S3. ROUGE-2 would prefer S4 than S3. 3.2 Summary-Level LCS Previous section described how to compute sen-tence-level LCS-based F-measure score. When ap-plying to summary-level, we take the union LCS matches between a reference summary sentence, ri, and every candidate summary sentence, cj. Given a reference summary of u sentences containing a total of m words and a candidate summary of v sentences containing a total of n words, the summary-level LCS-based F-measure can be computed as follows: Rlcs mCrLCSuii==1),(      (5) Plcs nCrLCSuii==1),(      (6) Flcs  lcslcslcslcsPRPR22)1(bb++=   (7)  Again  is set to a very big number (? 8) in DUC, i.e. only Rlcs is considered. ),(CrLCSiis the LCS score of the union longest common subse-quence between reference sentence ri and candidate summary C. For example, if ri = w1 w2 w3 w4 w5, and C contains two sentences: c1 = w1 w2 w6 w7 w8 and c2 = w1 w3 w8 w9 w5, then the longest common subse-quence of ri and c1 is w1 w2 and the longest com-mon subsequence of ri and c2 is w1 w3 w5. The union longest common subsequence of ri, c1, and c2 is w1 w2 w3 w5 and ),(CrLCSi= 4/5. 3.3 ROUGE-L vs. Normalized Pairwise LCS The normalized pairwise LCS proposed by Radev et al. (page 51, 2002) between two summaries S1 and S2, LCS(S1 ,S2)MEAD , is written as follows:  ++121212)()(),(max),(maxSsSsjiSsSsjiSsjiSsijijijslengthslengthssLCSssLCS (8)  Assuming S1 has m words and S2 has n words, Equation 8 can be rewritten as Equation 9 due to symmetry:  nmssLCSSsjiSsij+12),(max*2                       (9)  We then define MEAD LCS recall (Rlcs-MEAD) and MEAD LCS precision (Plcs-MEAD) as follows:   Rlcs-MEAD mssLCSSsjiSsij=12),(max      (10) Plcs-MEAD nssLCSSsjiSsij=12),(max       (11)  We can rewrite Equation (9) in terms of Rlcs-MEAD and Plcs-MEAD with a constant parameter  = 1 as fol-lows: LCS(S1 ,S2)MEAD  MEADlcsMEADlcsMEADlcsMEADlcsPRPR----++=22)1(bb (12) Equation 12 shows that normalized pairwise LCS as defined in Radev et al. (2002) and implemented in MEAD is also a F-measure with  = 1. Sentence-level normalized pairwise LCS is the same as ROUGE-L with  = 1. Besides setting  = 1, sum-mary-level normalized pairwise LCS is different from ROUGE-L in how a sentence gets its LCS score from its references. Normalized pairwise LCS takes the best LCS score while ROUGE-L takes the union LCS score. 4 ROUGE-W: Weighted Longest Common Sub-sequence LCS has many nice properties as we have described in the previous sections. Unfortunately, the basic LCS also has a problem that it does not differentiate LCSes of different spatial relations within their em-bedding sequences. For example, given a reference sequence X and two candidate sequences Y1 and Y2 as follows:  X:  [A B C D E F G] Y1: [A B C D H I K] Y2:  [A H B K C I D]  Y1 and Y2 have the same ROUGE-L score. How-ever, in this case, Y1 should be the better choice than Y2 because Y1 has consecutive matches. To improve the basic LCS method, we can simply remember the length of consecutive matches encountered so far to a regular two dimensional dynamic program table computing LCS. We call this weighted LCS (WLCS) and use k to indicate the length of the cur-rent consecutive matches ending at words xi and yj. Given two sentences X and Y, the WLCS score of X and Y can be computed using the following dynamic programming procedure:  (1) For (i = 0; i <=m; i++)         c(i,j) = 0  // initialize c-table         w(i,j) = 0 // initialize w-table (2) For (i = 1; i <= m; i++)         For (j = 1; j <= n; j++)           If xi = yj Then      // the length of consecutive matches at      // position i-1 and j-1      k = w(i-1,j-1)      c(i,j) = c(i-1,j-1) + f(k+1)  f(k)      // remember the length of consecutive      // matches at position i, j      w(i,j) = k+1           Otherwise      If c(i-1,j) > c(i,j-1) Then     c(i,j) = c(i-1,j)     w(i,j) = 0           // no match at i, j      Else c(i,j) = c(i,j-1)      w(i,j) = 0           // no match at i, j (3) WLCS(X,Y) = c(m,n)  Where c is the dynamic programming table, c(i,j) stores the WLCS score ending at word xi of X and yj of Y, w is the table storing the length of consecutive matches ended at c table position i and j, and f is a function of consecutive matches at the table posi-tion, c(i,j). Notice that by providing different weighting function f, we can parameterize the WLCS algorithm to assign different credit to con-secutive in-sequence matches.  The weighting function f must have the property that f(x+y) > f(x) + f(y) for any positive integers x and y. In other words, consecutive matches are awarded more scores than non-consecutive matches. For example, f(k)-=-ak  b when k >= 0, and a, b > 0. This function charges a gap penalty of b for each non-consecutive n-gram sequences. Another possible function family is the polynomial family of the form ka where -a > 1. However, in order to normalize the final ROUGE-W score, we also prefer to have a function that has a close form inverse function. For example, f(k)-=-k2 has a close form inverse function f -1(k)-=-k1/2. F-measure based on WLCS can be computed as follows, given two se-quences X of length m and Y of length n: Rwlcs =-)(),(1mfYXWLCSf      (13) Pwlcs =-)(),(1nfYXWLCSf      (14) Fwlcs  wlcswlcswlcswlcsPRPR22)1(bb++=          (15)  Where f -1 is the inverse function of f. In DUC,  is set to a very big number (? 8). Therefore, only Rwlcs is considered. We call the WLCS-based F-measure, i.e. Equation 15, ROUGE-W. Using Equa-tion 15 and f(k)-=-k2 as the weighting function, the ROUGE-W scores for sequences Y1 and Y2 are 0.571 and 0.286 respectively. Therefore, Y1 would be ranked higher than Y2 using WLCS. We use the polynomial function of the form ka in the ROUGE evaluation package. In the next section, we intro-duce the skip-bigram co-occurrence statistics. 5 ROUGE-S: Skip-Bigram Co-Occurrence Sta-tistics Skip-bigram is any pair of words in their sentence order, allowing for arbitrary gaps. Skip-bigram co-occurrence statistics measure the overlap of skip-bigrams between a candidate translation and a set of reference translations. Using the example given in Section 3.1:  S1. police killed the gunman S2. police kill the gunman S3. the gunman kill police S4. the gunman police killed each sentence has C(4,2)1 = 6 skip-bigrams. For ex-ample, S1 has the following skip-bigrams: (police killed, police the, police gunman, killed the, killed gunman, the gunman)  S2 has three skip-bigram matches with S1 (po-lice the, police gunman, the gunman), S3 has one skip-bigram match with S1 (the gunman), and S4 has two skip-bigram matches with S1 (police killed, the gunman).  Given translations X of length m and Y of length n, assuming X is a refer-ence translation and Y is a candidate translation, we compute skip-bigram-based F-measure as follows: Rskip2 )2,(),(2mCYXSKIP=          (16) Pskip2 )2,(),(2nCYXSKIP=          (17) Fskip2 222222)1(skipskipskipskipPRPRbb++=  (18)  Where SKIP2(X,Y) is the number of skip-bigram matches between X and Y,  controlling the relative importance of Pskip2 and Rskip2, and  C is the combi-nation function. We call the skip-bigram-based F-measure, i.e. Equation 18, ROUGE-S. Using Equation 18 with  = 1 and S1 as the refer-ence, S2s ROUGE-S score is 0.5, S3 is 0.167, and S4 is 0.333. Therefore, S2 is better than S3 and S4, and S4 is better than S3. This result is more intuitive than using BLEU-2 and ROUGE-L. One advantage of skip-bigram vs. BLEU is that it does not require con-secutive matches but is still sensitive to word order. Comparing skip-bigram with LCS, skip-bigram counts all in-order matching word pairs while LCS only counts one longest common subsequence. Applying skip-bigram without any constraint on the distance between the words, spurious matches such as the the or of in might be counted as valid matches. To reduce these spurious matches, we can limit the maximum skip distance, dskip, be-tween two in-order words that is allowed to form a skip-bigram. For example, if we set dskip to 0 then ROUGE-S is equivalent to bigram overlap F-measure. If we set dskip to 4 then only word pairs of at most 4 words apart can form skip-bigrams. Adjusting Equations 16, 17, and 18 to use maxi-mum skip distance limit is straightforward: we only count the skip-bigram matches, SKIP2(X,Y), within the maximum skip distance and replace denomina-tors of Equations 16, C(m,2), and 17, C(n,2), with the actual numbers of within distance skip-bigrams from the reference and the candidate respectively.                                                                    1 C(4,2) = 4!/(2!*2!) = 6. 5.1 ROUGE-SU: Extension of ROUGE-S One potential problem for ROUGE-S is that it does not give any credit to a candidate sentence if the sentence does not have any word pair co-occurring with its references. For example, the following sen-tence has a ROUGE-S score of zero:  S5. gunman the killed police  S5 is the exact reverse of S1 and there is no skip bigram match between them. However, we would like to differentiate sentences similar to S5 from sentences that do not have single word co-occurrence with S1. To achieve this, we extend ROUGE-S with the addition of unigram as counting unit. The extended version is called ROUGE-SU. We can also obtain ROUGE-SU from ROUGE-S by add-ing a begin-of-sentence marker at the beginning of candidate and reference sentences. 6 Evaluations of ROUGE To assess the effectiveness of ROUGE measures, we compute the correlation between ROUGE assigned summary scores and human assigned summary scores. The intuition is that a good evaluation meas-ure should assign a good score to a good summary and a bad score to a bad summary. The ground truth is based on human assigned scores. Acquiring hu-man judgments are usually very expensive; fortu-nately, we have DUC 2001, 2002, and 2003 evaluation data that include human judgments for the following:  Single document summaries of about 100 words: 12 systems2 for DUC 2001 and 14 sys-tems for 2002. 149 single document summaries were judged per system in DUC 2001 and 295 were judged in DUC 2002.  Single document very short summaries of about 10 words (headline-like, keywords, or phrases): 14 systems for DUC 2003. 624 very short sum-maries were judged per system in DUC 2003.  Multi-document summaries of about 10 words: 6 systems for DUC 2002; 50 words: 14 systems for DUC 2001 and 10 systems for DUC 2002; 100 words: 14 systems for DUC 2001, 10 sys-tems for DUC 2002, and 18 systems for DUC 2003; 200 words: 14 systems for DUC 2001 and 10 systems for DUC 2002; 400 words: 14 sys-tems for DUC 2001. 29 summaries were judged per system per summary size in DUC 2001, 59 were judged in DUC 2002, and 30 were judged in DUC 2003.                                                                  2 All systems include 1 or 2 baselines. Please see DUC website for details. Besides these human judgments, we also have 3 sets of manual summaries for DUC 2001, 2 sets for DUC 2002, and 4 sets for DUC 2003. Human judges assigned content coverage scores to a candi-date summary by examining the percentage of con-tent overlap between a manual summary unit, i.e. elementary discourse unit or sentence, and the can-didate summary using Summary Evaluation Envi-ronment3 (SEE) developed by  the University of Southern Californias Information Sciences Institute (ISI). The overall candidate summary score is the average of the content coverage scores of all the units in the manual summary. Note that human judges used only one manual summary in all the evaluations although multiple alternative summaries were available. With the DUC data, we computed Pearsons product moment correlation coefficients, Spear-mans rank order correlation coefficients, and Kendalls correlation coefficients between systems average ROUGE scores and their human assigned average coverage scores using single reference and multiple references. To investigate the effect of stemming and inclusion or exclusion of stopwords, we also ran experiments over original automatic and                                                                  3 SEE is available online at http://www.isi.edu/~cyl. manual summaries (CASE set), stemmed4 version of the summaries (STEM set), and stopped version of the summaries (STOP set). For example, we com-puted ROUGE scores for the 12 systems participated in the DUC 2001 single document summarization evaluation using the CASE set with single reference and then calculated the three correlation scores for these 12 systems ROUGE scores vs. human assigned average coverage scores. After that we repeated the process using multiple references and then using STEM and STOP sets. Therefore, 2 (multi or single) x 3 (CASE, STEM, or STOP) x 3 (Pearson, Spear-man, or Kendall) = 18 data points were collected for each ROUGE measure and each DUC task. To assess the significance of the results, we applied bootstrap resampling technique (Davison and Hinkley, 1997) to estimate 95% confidence intervals for every cor-relation computation. 17 ROUGE measures were tested for each run us-ing ROUGE evaluation package v1.2.1: ROUGE-N  with N = 1 to 9, ROUGE-L, ROUGE-W with weighting factor a  = 1.2, ROUGE-S and ROUGE-SU with maximum skip distance dskip = 1, 4, and 9. Due to limitation of space, we only report correlation analysis results based on Pearsons correlation coef-ficient. Correlation analyses based on Spearmans and Kendalls correlation coefficients are tracking Pearsons very closely and will be posted later at the ROUGE website5 for reference. The critical value6 for Pearsons correlation is 0.632 at 95% confidence with 8 degrees of freedom. Table 1 shows the Pearsons correlation coeffi-cients of the 17 ROUGE measures vs. human judg-ments on DUC 2001 and 2002 100 words single document summarization data. The best values in each column are marked with dark (green) color and statistically equivalent values to the best values are marked with gray. We found that correlations were not affected by stemming or removal of stopwords in this data set, ROUGE-2 performed better among the ROUGE-N variants, ROUGE-L, ROUGE-W, and ROUGE-S were all performing well, and using mul-tiple references improved performance though not much. All ROUGE measures achieved very good correlation with human judgments in the DUC 2002 data. This might due to the double sample size in DUC 2002 (295 vs. 149 in DUC 2001) for each sys-tem. Table 2 shows the correlation analysis results on the DUC 2003 single document very short summary data. We found that ROUGE-1, ROUGE-L, ROUGE-                                                                 4 Porters stemmer was used. 5 ROUGE website: http://www.isi.edu/~cyl/ROUGE. 6 The critical values for Pearsons correlation at 95% confidence with 10, 12, 14, and 16 degrees of freedom are 0.576, 0.532, 0.497, and 0.468 respectively. MethodCASESTEMSTOPCASESTEMSTOPCASESTEMSTOPCASESTEMSTOPR-10.760.760.840.800.780.840.980.980.990.980.980.99R-20.840.840.830.870.870.860.990.990.990.990.990.99R-30.820.830.800.860.860.850.990.990.990.990.990.99R-40.810.810.770.840.840.830.990.990.980.990.990.99R-50.790.790.750.830.830.810.990.990.980.990.990.98R-60.760.770.710.810.810.790.980.990.970.990.990.98R-70.730.740.650.790.800.760.980.980.970.990.990.97R-80.690.710.610.780.780.720.980.980.960.990.990.97R-90.650.670.590.760.760.690.970.970.950.980.980.96R-L0.830.830.830.860.860.860.990.990.990.990.990.99R-S*0.740.740.800.780.770.820.980.980.980.980.970.98R-S40.840.850.840.870.880.870.990.990.990.990.990.99R-S90.840.850.840.870.880.870.990.990.990.990.990.99R-SU*0.740.740.810.780.770.830.980.980.980.980.980.98R-SU40.840.840.850.870.870.870.990.990.990.990.990.99R-SU90.840.840.850.870.870.870.990.990.990.990.990.99R-W-1.20.850.850.850.870.870.870.990.990.990.990.990.99DUC 2001 100 WORDS SINGLE DOCDUC 2002 100 WORDS SINGLE DOC1 REF3 REFS1 REF2 REFSTable 1: Pearsons correlations of 17 ROUGEmeasure scores vs. human judgments for the DUC 2001 and 2002 100 words single document sum-marization tasks 1 REF4REFS1 REF4 REFS1 REF4 REFSMethodR-10.960.950.950.950.900.90R-20.750.760.750.750.760.77R-30.710.700.700.680.730.70R-40.640.650.620.630.690.66R-50.620.640.600.630.630.60R-60.570.620.550.610.460.54R-70.560.560.580.600.460.44R-80.550.530.540.550.000.24R-90.510.470.510.490.000.14R-L0.970.960.970.960.970.96R-S*0.890.870.880.850.950.92R-S40.880.890.880.880.950.96R-S90.920.920.920.910.970.95R-SU*0.930.900.910.890.960.94R-SU40.970.960.960.950.980.97R-SU90.970.950.960.940.970.95R-W-1.20.960.960.960.960.960.96DUC 2003 10 WORDS SINGLE DOCCASESTEMSTOPTable 2: Pearsons correlations of 17 ROUGEmeasure scores vs. human judgments for the DUC 2003 very short summary task SU4 and 9, and ROUGE-W were very good measures in this category, ROUGE-N with N > 1 performed significantly worse than all other measures, and ex-clusion of stopwords improved performance in gen-eral except for ROUGE-1. Due to the large number of samples (624) in this data set, using multiple ref-erences did not improve correlations. In Table 3 A1, A2, and A3, we show correlation analysis results on DUC 2001, 2002, and 2003 100 words multi-document summarization data. The results indicated that using multiple references im-proved correlation and exclusion of stopwords usu-ally improved performance. ROUGE-1, 2, and 3 performed fine but were not consistent. ROUGE-1, ROUGE-S4, ROUGE-SU4, ROUGE-S9, and ROUGE-SU9 with stopword removal had correlation above 0.70. ROUGE-L and ROUGE-W did not work well in this set of data. Table 3 C, D1, D2, E1, E2, and F show the corre-lation analyses using multiple references on the rest of DUC data. These results again suggested that exclusion of stopwords achieved better performance especially in multi-document summaries of 50 words. Better correlations (> 0.70) were observed on long summary tasks, i.e. 200 and 400 words summaries. The relative performance of ROUGE measures followed the pattern of the 100 words multi-document summarization task. Comparing the results in Table 3 with Tables 1 and 2, we found that correlation values in the multi-document tasks rarely reached high 90% except in long summary tasks. One possible explanation of this outcome is that we did not have large amount of samples for the multi-document tasks. In the single document summarization tasks we had over 100 samples; while we only had about 30 samples in the multi-document tasks. The only tasks that had over 30 samples was from DUC 2002 and the correla-tions of ROUGE measures with human judgments on the 100 words summary task were much better and more stable than similar tasks in DUC 2001 and 2003. Statistically stable human judgments of sys-tem performance might not be obtained due to lack of samples and this in turn caused instability of cor-relation analyses. 7 Conclusions In this paper, we introduced ROUGE, an automatic evaluation package for summarization, and con-ducted comprehensive evaluations of the automatic measures included in the ROUGE package using three years of DUC data. To check the significance of the results, we estimated confidence intervals of correlations using bootstrap resampling. We found that (1) ROUGE-2, ROUGE-L, ROUGE-W, and ROUGE-S worked well in single document summa-rization tasks, (2) ROUGE-1, ROUGE-L, ROUGE-W, ROUGE-SU4, and ROUGE-SU9 performed great in evaluating very short summaries (or headline-like summaries), (3) correlation of high 90% was hard to achieve for multi-document summarization tasks but ROUGE-1, ROUGE-2, ROUGE-S4, ROUGE-S9, ROUGE-SU4, and ROUGE-SU9 worked reasonably well when stopwords were excluded from matching, (4) exclusion of  stopwords usually improved corre-lation, and (5) correlations to human judgments were increased by using multiple references. In summary, we showed that the ROUGE package could be used effectively in automatic evaluation of summaries. In a separate study (Lin and Och, 2004), MethodCASESTEMSTOPCASESTEMSTOPCASESTEMSTOPCASESTEMSTOPCASESTEMSTOPCASESTEMSTOPR-10.480.560.860.530.570.870.660.660.770.710.710.780.580.570.710.580.570.71R-20.550.570.640.590.610.710.830.830.800.880.870.850.690.670.710.790.790.81R-30.460.450.470.530.530.550.850.840.760.890.880.830.540.510.480.760.750.74R-40.390.390.430.480.490.470.800.800.630.830.820.750.370.360.360.620.610.52R-50.380.390.330.470.480.430.730.730.450.730.730.620.250.250.270.450.440.38R-60.390.390.200.450.460.390.710.720.380.660.640.460.210.210.260.340.310.29R-70.310.310.170.440.440.360.630.650.330.560.530.440.200.200.230.290.270.25R-80.180.190.090.400.400.310.550.550.520.500.460.520.180.180.210.230.220.23R-90.110.120.060.380.380.280.540.540.520.450.420.520.160.160.190.210.210.21R-L0.490.490.490.560.560.560.620.620.620.650.650.650.500.500.500.530.530.53R-S*0.450.520.840.510.540.860.690.690.770.730.730.790.600.600.670.610.600.70R-S40.460.500.710.540.570.780.790.800.790.840.850.820.630.640.700.730.730.78R-S90.420.490.770.530.560.810.790.800.780.830.840.810.650.650.700.700.700.76R-SU*0.450.520.840.510.540.870.690.690.770.730.730.790.600.590.670.600.600.70R-SU40.470.530.800.550.580.830.760.760.790.800.810.810.640.640.740.680.680.76R-SU90.440.500.800.530.570.840.770.780.780.810.820.810.650.650.720.680.680.75R-W-1.20.520.520.520.600.600.600.670.670.670.690.690.690.530.530.530.580.580.58MethodCASESTEMSTOPCASESTEMSTOPCASESTEMSTOPCASESTEMSTOPCASESTEMSTOPCASESTEMSTOPR-10.710.680.490.490.490.730.440.480.800.810.810.900.840.840.910.740.730.90R-20.820.850.800.430.450.590.470.490.620.840.850.860.930.930.940.880.880.87R-30.590.740.750.320.330.390.360.360.450.800.800.810.900.910.910.840.840.82R-40.250.360.160.280.260.360.280.280.390.770.780.780.870.880.880.800.800.75R-5-0.25-0.25-0.240.300.290.310.280.300.490.770.760.720.820.830.840.770.770.70R-60.000.000.000.220.230.410.180.21-0.170.750.750.670.780.790.770.740.740.63R-70.000.000.000.260.230.500.110.160.000.720.720.620.720.730.740.700.700.58R-80.000.000.000.320.320.34-0.11-0.110.000.680.680.540.710.710.700.660.660.52R-90.000.000.000.300.300.34-0.14-0.140.000.640.640.480.700.690.590.630.620.46R-L0.780.780.780.560.560.560.500.500.500.810.810.810.880.880.880.820.820.82R-S*0.830.820.690.460.450.740.460.490.800.800.800.900.840.850.930.750.740.89R-S40.850.860.760.400.410.690.420.440.730.820.820.870.910.910.930.850.850.85R-S90.820.810.690.420.410.720.400.430.780.810.820.860.900.900.920.830.830.84R-SU*0.750.740.560.460.460.740.460.490.800.800.800.900.840.850.930.750.740.89R-SU40.760.750.580.450.450.720.440.460.780.820.830.890.900.900.930.840.840.88R-SU90.740.730.560.440.440.730.410.450.790.820.820.880.890.890.920.830.820.87R-W-1.20.780.780.780.560.560.560.510.510.510.840.840.840.900.900.900.860.860.86(A1) DUC 2001 100 WORDS MULTI(A2) DUC 2002 100 WORDS MULTI(A3) DUC 2003 100 WORDS MULTI1 RFF3 REFS1 REF2 REFS1 REF4 REFS(E2) DUC02 200(F) DUC01 400(C) DUC02 10(D1) DUC01 50(D2) DUC02 50(E1) DUC01 200Table 3: Pearsons correlations of 17 ROUGE measure scores vs. human judgments for the DUC 2001, 2002, and 2003 multi-document summarization tasks ROUGE-L, W, and S were also shown to be very effective in automatic evaluation of machine translation. The stability and reliability of ROUGE at different sample sizes was reported by the author in (Lin, 2004). However, how to achieve high correla-tion with human judgments in multi-document summarization tasks as ROUGE already did in single document summarization tasks is still an open re-search topic. 8  Acknowledgements The author would like to thank the anonymous re-viewers for their constructive comments, Paul Over at NIST, U.S.A, and ROUGE users around the world for testing and providing useful feedback on earlier versions of the ROUGE evaluation package, and the DARPA TIDES project for supporting this research. References  Cormen, T. R., C. E. Leiserson, and R. L. Rivest. 1989. Introduction to Algorithms. The MIT Press. Davison, A. C. and D. V. Hinkley. 1997. Bootstrap Methods and Their Application. Cambridge Uni-versity Press. Lin, C.-Y. and E. H. Hovy. 2003. Automatic evalua-tion of summaries using n-gram co-occurrence statistics. In Proceedings of 2003 Language Technology Conference (HLT-NAACL 2003), Edmonton, Canada. Lin, C.-Y. 2004. Looking for a few good metrics: ROUGE and its evaluation. In Proceedings of NTCIR Workshop 2004, Tokyo, Japan. Lin, C.-Y. and F. J. Och. 2004. Automatic evalua-tion of machine translation quality using longest common subsequence and skip-bigram statistics. In Proceedings of 42nd Annual Meeting of ACL (ACL 2004), Barcelona, Spain. Mani, I. 2001. Automatic Summarization. John Ben-jamins Publishing Co. Melamed, I. D. 1995. Automatic evaluation and uni-form filter cascades for inducing n-best transla-tion lexicons. In Proceedings of the 3rd Workshop on Very Large Corpora (WVLC3). Boston, U.S.A. Melamed, I. D., R. Green and J. P. Turian (2003). Precision and recall of machine translation. In Proceedings of 2003 Language Technology Con-ference (HLT-NAACL 2003), Edmonton, Can-ada. Over, P. and J. Yen. 2003. An introduction to DUC 2003  Intrinsic evaluation of generic news text summarization systems. AAAAAAAAAA                               http://www-nlpir.nist.gov/projects/duc/pubs/ 2003slides/duc2003intro.pdf Papineni, K., S. Roukos, T. Ward, and W.-J. Zhu. 2001. BLEU: A method for automatic evaluation of machine translation. IBM Research Report RC22176 (W0109-022). Saggion H., D. Radev, S. Teufel, and W. Lam. 2002. Meta-evaluation of summaries in a cross-lingual environment using content-based metrics. In Proceedings of COLING-2002, Taipei, Tai-wan. Radev, D.  S. Teufel, H. Saggion, W. Lam, J. Blit-zer, A. Gelebi, H. Qi, E. Drabek, and D. Liu. 2002. Evaluation of Text Summarization in a Cross-Lingual Information Retrieval Framework. Technical report, Center for Language and Speech Processing, Johns Hopkins University, Baltimore, MD, USA. Van Rijsbergen, C. J. 1979. Information Retrieval. Butterworths. London. 